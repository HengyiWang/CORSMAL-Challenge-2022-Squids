{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioFeatures.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XZzyqi941ZX9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ed8742defd048188d5c12cd18fbc800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bec0d10affe541ff8e64c91d2bdb5ac0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_40b2bcbc586c41919bf0f6a4e853178a",
              "IPY_MODEL_05666dca53f1440ab7b089c07ad9a3d4",
              "IPY_MODEL_991eedd0420a4684a99d06159e632534"
            ]
          }
        },
        "bec0d10affe541ff8e64c91d2bdb5ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40b2bcbc586c41919bf0f6a4e853178a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_29f244d469db4f25a5754659d251e684",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e36aada47dd44b919fda2ca9a10ea557"
          }
        },
        "05666dca53f1440ab7b089c07ad9a3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e5974246df8e4463a4b61910e941afcf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 684,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 684,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99fba8a3919f4d6180fe7630cc983dc2"
          }
        },
        "991eedd0420a4684a99d06159e632534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f8a7bd01426444fc82f193985970c19b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 684/684 [03:53&lt;00:00,  2.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d6d71f1edb7469ab022d44c8043a0cb"
          }
        },
        "29f244d469db4f25a5754659d251e684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e36aada47dd44b919fda2ca9a10ea557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5974246df8e4463a4b61910e941afcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99fba8a3919f4d6180fe7630cc983dc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8a7bd01426444fc82f193985970c19b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d6d71f1edb7469ab022d44c8043a0cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb2fad5e11374ad0bb7eb320a8b2d5c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_11a208b41d174483bbcfd1c3c3df485c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_92bfb75199624426878f39eedc11900c",
              "IPY_MODEL_8902fd83c450426dbd84a51cda66934d",
              "IPY_MODEL_1f1225f36df84132a49309fc1e26748d"
            ]
          }
        },
        "11a208b41d174483bbcfd1c3c3df485c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92bfb75199624426878f39eedc11900c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5c55f61b8f474bfe988e13533746b797",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_26e0e0bae0044855bc88c935a800f2ca"
          }
        },
        "8902fd83c450426dbd84a51cda66934d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8b56af3dd98241f09053826775d9e46b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 684,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 684,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5633d0db4604b988ae66c6a701a9c1d"
          }
        },
        "1f1225f36df84132a49309fc1e26748d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ccac89fe71b24f65afb4065c885ae64a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 684/684 [10:40&lt;00:00,  2.24it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f77302b52ed84e25916a1493a6e69055"
          }
        },
        "5c55f61b8f474bfe988e13533746b797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "26e0e0bae0044855bc88c935a800f2ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b56af3dd98241f09053826775d9e46b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5633d0db4604b988ae66c6a701a9c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ccac89fe71b24f65afb4065c885ae64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f77302b52ed84e25916a1493a6e69055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6fd45eda7c604273b0e6d65f9563a8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_31a36b5780284e6aabd44a29292af12b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a1bd32c213744acd999c6bf89827a93a",
              "IPY_MODEL_59a214a8b28041fe8e241df30384757c",
              "IPY_MODEL_580d86db47dd41e9a620cb241c90a9b9"
            ]
          }
        },
        "31a36b5780284e6aabd44a29292af12b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1bd32c213744acd999c6bf89827a93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_14836bf44b6a4fd4863a1e0b1ac735dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f5cc8a24e80c4b8cbe1fc51a4019a3ae"
          }
        },
        "59a214a8b28041fe8e241df30384757c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3fe752d058104c338b03c55b27d8d35e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 684,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 684,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c3ba7d5d3d194156b402b316411888ae"
          }
        },
        "580d86db47dd41e9a620cb241c90a9b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ff5f13deb3df40e39470885b38bb2106",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 684/684 [48:43&lt;00:00,  5.05s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_965209a96f2e4b61990a02e516f47b71"
          }
        },
        "14836bf44b6a4fd4863a1e0b1ac735dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f5cc8a24e80c4b8cbe1fc51a4019a3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3fe752d058104c338b03c55b27d8d35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c3ba7d5d3d194156b402b316411888ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff5f13deb3df40e39470885b38bb2106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "965209a96f2e4b61990a02e516f47b71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmbNwcAdmThI",
        "outputId": "c1ad469a-c895-4854-d7f1-6f487209b57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan  2 20:02:24 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "_uy1XaYeYMs4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQfv4bkWeyrX"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.io.wavfile\n",
        "import time\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gt = pd.read_csv('/content/drive/MyDrive/COSRMAL_CHALLENGE/train.csv')\n",
        "gt.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK345_nrpxEU",
        "outputId": "07ba20b7-08fb-4227-bc36-48b950e4cd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-55275628-1601-4610-9e7f-d09d62093ebb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>container id</th>\n",
              "      <th>scenario</th>\n",
              "      <th>background</th>\n",
              "      <th>illumination</th>\n",
              "      <th>width at the top</th>\n",
              "      <th>width at the bottom</th>\n",
              "      <th>height</th>\n",
              "      <th>depth</th>\n",
              "      <th>container capacity</th>\n",
              "      <th>container mass</th>\n",
              "      <th>filling type</th>\n",
              "      <th>filling level</th>\n",
              "      <th>filling density</th>\n",
              "      <th>filling mass</th>\n",
              "      <th>object mass</th>\n",
              "      <th>handover starting frame</th>\n",
              "      <th>handover start timestamp</th>\n",
              "      <th>handover hand</th>\n",
              "      <th>action</th>\n",
              "      <th>nframes</th>\n",
              "      <th>folder_num</th>\n",
              "      <th>file_name</th>\n",
              "      <th>num</th>\n",
              "      <th>subject</th>\n",
              "      <th>filling_type</th>\n",
              "      <th>filling_level</th>\n",
              "      <th>back</th>\n",
              "      <th>light</th>\n",
              "      <th>camera_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.82</td>\n",
              "      <td>76.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>291576</td>\n",
              "      <td>2</td>\n",
              "      <td>s2_fi2_fu1_b1_l0</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>3209.397</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118483</td>\n",
              "      <td>7</td>\n",
              "      <td>s0_fi0_fu0_b0_l0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>93.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>572008</td>\n",
              "      <td>2</td>\n",
              "      <td>s0_fi3_fu1_b1_l0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.40</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1239.840</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141680</td>\n",
              "      <td>8</td>\n",
              "      <td>s0_fi0_fu0_b1_l0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>296.000</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34</td>\n",
              "      <td>45.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138681</td>\n",
              "      <td>4</td>\n",
              "      <td>s1_fi1_fu1_b1_l0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55275628-1601-4610-9e7f-d09d62093ebb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-55275628-1601-4610-9e7f-d09d62093ebb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-55275628-1601-4610-9e7f-d09d62093ebb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id  container id  scenario  background  ...  light  camera_id  start  end\n",
              "0   0             2         2           1  ...      0          2   0.75  3.5\n",
              "1   1             7         0           0  ...      0          2  -1.00 -1.0\n",
              "2   2             2         0           1  ...      0          2   3.40  6.5\n",
              "3   3             8         0           1  ...      0          2  -1.00 -1.0\n",
              "4   4             4         1           1  ...      0          2   0.75  1.8\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioProcessing():\n",
        "    def __init__(self,sample_rate,signal,frame_length_t=0.025,frame_stride_t=0.01,nfilt =64):\n",
        "        \n",
        "        self.sample_rate=sample_rate\n",
        "        self.signal = signal\n",
        "        self.frame_length_t=frame_length_t\n",
        "        self.frame_stride_t=frame_stride_t\n",
        "        self.signal_length_t=float(signal.shape[0]/sample_rate)\n",
        "        self.frame_length=int(round(frame_length_t * sample_rate)) #number of samples\n",
        "        self.frame_step=int(round(frame_stride_t * sample_rate))\n",
        "        self.signal_length = signal.shape[0]\n",
        "        self.nfilt=nfilt\n",
        "        self.num_frames = int(np.ceil(float(np.abs(self.signal_length - self.frame_length)) / self.frame_step))\n",
        "        self.pad_signal_length=self.num_frames * self.frame_step + self.frame_length\n",
        "        self.NFFT=512\n",
        "        \n",
        "    def cal_frames(self):\n",
        "        z = np.zeros([self.pad_signal_length - self.signal_length,8])\n",
        "        pad_signal = np.concatenate([self.signal, z], 0)\n",
        "        indices = np.tile(np.arange(0, self.frame_length), (self.num_frames, 1)) + np.tile(np.arange(0, self.num_frames * self.frame_step, self.frame_step), (self.frame_length, 1)).T\n",
        "        frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
        "        return frames\n",
        "        \n",
        "    def calc_MFCC(self):\n",
        "        # 291576\n",
        "        pre_emphasis=0.97\n",
        "\n",
        "        # (n,8)\n",
        "        emphasized_signal=np.concatenate([self.signal[0,:].reshape([1,-1]),  self.signal[1:,:] - pre_emphasis * self.signal[:-1,:]], 0)\n",
        "        z = np.zeros([self.pad_signal_length - self.signal_length,8])\n",
        "        pad_signal = np.concatenate([emphasized_signal, z], 0)\n",
        "        indices = np.tile(np.arange(0, self.frame_length), (self.num_frames, 1)) + np.tile(np.arange(0, self.num_frames * self.frame_step, self.frame_step), (self.frame_length, 1)).T\n",
        "        frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
        "        frames=frames*np.hamming(self.frame_length).reshape(1,-1,1)\n",
        "        frames=frames.transpose(0,2,1)\n",
        "        mag_frames = np.absolute(np.fft.rfft(frames,self.NFFT))\n",
        "        pow_frames = ((1.0 / self.NFFT) * ((mag_frames) ** 2))\n",
        "        filter_banks = np.dot(pow_frames, self.cal_fbank().T)\n",
        "        filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
        "        filter_banks = 20 * np.log10(filter_banks)  # dB\n",
        "        filter_banks =filter_banks.transpose(0,2,1)\n",
        "        \n",
        "        return filter_banks\n",
        "           \n",
        "    def cal_fbank(self):\n",
        "        \n",
        "        low_freq_mel = 0\n",
        "        high_freq_mel = (2595 * np.log10(1 + (self.sample_rate / 2) / 700))  \n",
        "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.nfilt + 2)  \n",
        "        hz_points = (700 * (10**(mel_points / 2595) - 1)) \n",
        "        bin = np.floor((self.NFFT + 1) * hz_points / self.sample_rate)\n",
        "        fbank = np.zeros((self.nfilt, int(np.floor(self.NFFT / 2 + 1))))\n",
        "        for m in range(1, self.nfilt + 1):\n",
        "            f_m_minus = int(bin[m - 1])   # left\n",
        "            f_m = int(bin[m])             # center\n",
        "            f_m_plus = int(bin[m + 1])    # right\n",
        "\n",
        "            for k in range(f_m_minus, f_m):\n",
        "                fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
        "            for k in range(f_m, f_m_plus):\n",
        "                fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
        "        return fbank"
      ],
      "metadata": {
        "id": "E4dlxplIpfCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "rx68_zB5xtK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "HmGrl1TbYSJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# base_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE/'\n",
        "# audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio/'\n",
        "# os.makedirs(os.path.join(base_path, 'audios'), exist_ok=True)\n",
        "# mfcc_path = (os.path.join(base_path, 'audios', 'mfcc'))\n",
        "# raw_path = (os.path.join(base_path, 'audios', 'raw'))\n",
        "# os.makedirs(mfcc_path,exist_ok=True)\n",
        "# os.makedirs(raw_path,exist_ok=True)\n",
        "\n",
        "# audio_paths = [os.path.join(audio_folder, path) for path in sorted(os.listdir(audio_folder))]\n",
        "# save_size=64\n",
        "# ratio_step = 0.25\n",
        "# count = 0\n",
        "# pouring_or_shaking_list = []\n",
        "# file_idx_list = []\n",
        "# filling_type_list = []\n",
        "# pbar = tqdm(total=len(audio_paths))\n",
        "\n",
        "# for i, path in enumerate(audio_paths):\n",
        "#   id = i\n",
        "#   start_time = gt[gt.id==id]['start'].item()\n",
        "#   end_time = gt[gt.id==id]['end'].item()\n",
        "#   filling_type = gt[gt.id==id]['filling_type'].item()\n",
        "#   sample_rate, signal = scipy.io.wavfile.read(path)\n",
        "#   ap = AudioProcessing(sample_rate,signal,nfilt=save_size)\n",
        "#   mfcc = ap.calc_MFCC()\n",
        "#   raw_frames = ap.cal_frames()\n",
        "#   mfcc_length=mfcc.shape[0]\n",
        "\n",
        "#   if mfcc_length < save_size:\n",
        "#     print(\"file {} is too short\".format(id))\n",
        "#   else:\n",
        "#     f_step=int(mfcc.shape[1]*ratio_step)\n",
        "#     f_length=mfcc.shape[1]\n",
        "#     save_mfcc_num=int(np.ceil(float(np.abs(mfcc_length - save_size)) / f_step))\n",
        "\n",
        "#     for i in range(save_mfcc_num):\n",
        "#       count += 1\n",
        "#       tmp_mfcc = mfcc[i*f_step:save_size+i*f_step,: ,:]\n",
        "#       if start_time == -1:\n",
        "#           pouring_or_shaking_list.append(0)\n",
        "#       elif start_time/ap.signal_length_t*mfcc_length<i*f_step+f_length*0.75 and end_time/ap.signal_length_t*mfcc_length>i*f_step+f_length*0.25:\n",
        "#           pouring_or_shaking_list.append(1) \n",
        "#       else:\n",
        "#           pouring_or_shaking_list.append(0)\n",
        "      \n",
        "#       filling_type_list.append(filling_type)\n",
        "#       file_idx_list.append(id)\n",
        "\n",
        "#       np.save(os.path.join(mfcc_path, \"{0:06d}\".format(count)), tmp_mfcc)\n",
        "#   pbar.update()\n",
        "\n",
        "\n",
        "# np.save(os.path.join(base_path, 'audios', 'pouring_or_shaking'), np.array(pouring_or_shaking_list) )\n",
        "# np.save(os.path.join(base_path, 'audios', 'filling_type'), np.array(filling_type_list))\n"
      ],
      "metadata": {
        "id": "Nb-WRGUQp1kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/'\n",
        "audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio/'\n",
        "os.makedirs(os.path.join(base_path, 'audios'), exist_ok=True)\n",
        "mfcc_path = (os.path.join(base_path, 'audios', 'mfcc'))\n",
        "raw_path = (os.path.join(base_path, 'audios', 'raw'))\n",
        "os.makedirs(mfcc_path,exist_ok=True)\n",
        "os.makedirs(raw_path,exist_ok=True)\n",
        "\n",
        "audio_paths = [os.path.join(audio_folder, path) for path in sorted(os.listdir(audio_folder))]\n",
        "save_size=64\n",
        "ratio_step = 0.25\n",
        "count = 0\n",
        "pouring_or_shaking_list = []\n",
        "file_idx_list = []\n",
        "filling_type_list = []\n",
        "pbar = tqdm(total=len(audio_paths))\n",
        "\n",
        "for i, path in enumerate(audio_paths):\n",
        "  id = i\n",
        "  start_time = gt[gt.id==id]['start'].item()\n",
        "  end_time = gt[gt.id==id]['end'].item()\n",
        "  filling_type = gt[gt.id==id]['filling_type'].item()\n",
        "  sample_rate, signal = scipy.io.wavfile.read(path)\n",
        "  ap = AudioProcessing(sample_rate,signal,nfilt=save_size)\n",
        "  mfcc = ap.calc_MFCC()\n",
        "  mfcc_length=mfcc.shape[0]\n",
        "\n",
        "  if mfcc_length < save_size:\n",
        "    print(\"file {} is too short\".format(id))\n",
        "  else:\n",
        "    f_step=int(mfcc.shape[1]*ratio_step)\n",
        "    f_length=mfcc.shape[1]\n",
        "    save_mfcc_num=int(np.ceil(float(np.abs(mfcc_length - save_size)) / f_step))\n",
        "\n",
        "    for i in range(save_mfcc_num):\n",
        "      count += 1\n",
        "      tmp_mfcc = mfcc[i*f_step:save_size+i*f_step,: ,:]\n",
        "      if start_time == -1:\n",
        "          pouring_or_shaking_list.append(0)\n",
        "      elif start_time/ap.signal_length_t*mfcc_length<i*f_step+f_length*0.75 and end_time/ap.signal_length_t*mfcc_length>i*f_step+f_length*0.25:\n",
        "          pouring_or_shaking_list.append(1) \n",
        "      else:\n",
        "          pouring_or_shaking_list.append(0)\n",
        "      \n",
        "      filling_type_list.append(filling_type)\n",
        "      file_idx_list.append(id)\n",
        "\n",
        "      np.save(os.path.join(mfcc_path, \"{0:06d}\".format(count)), tmp_mfcc)\n",
        "  pbar.update()\n",
        "\n",
        "\n",
        "np.save(os.path.join(base_path, 'audios', 'pouring_or_shaking'), np.array(pouring_or_shaking_list) )\n",
        "np.save(os.path.join(base_path, 'audios', 'filling_type'), np.array(filling_type_list))\n"
      ],
      "metadata": {
        "id": "jQcRO_VGiHRK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "cb2fad5e11374ad0bb7eb320a8b2d5c1",
            "11a208b41d174483bbcfd1c3c3df485c",
            "92bfb75199624426878f39eedc11900c",
            "8902fd83c450426dbd84a51cda66934d",
            "1f1225f36df84132a49309fc1e26748d",
            "5c55f61b8f474bfe988e13533746b797",
            "26e0e0bae0044855bc88c935a800f2ca",
            "8b56af3dd98241f09053826775d9e46b",
            "a5633d0db4604b988ae66c6a701a9c1d",
            "ccac89fe71b24f65afb4065c885ae64a",
            "f77302b52ed84e25916a1493a6e69055"
          ]
        },
        "outputId": "8fee2b5b-eced-45b6-d97b-3162db76bda7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb2fad5e11374ad0bb7eb320a8b2d5c1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filling_type = np.load(os.path.join(base_path, 'audios', 'filling_type.npy'))\n",
        "pouring_or_shaking = np.load(os.path.join(base_path,  'audios', 'pouring_or_shaking.npy'))\n",
        "\n",
        "label = filling_type * pouring_or_shaking"
      ],
      "metadata": {
        "id": "dNahj5LNbU4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torchinfo import summary\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "#model = Net(8, 4).to(device)\n",
        "#summary(model, input_size=(32, 8, 64, 64))"
      ],
      "metadata": {
        "id": "vdYjOKUXw4uA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dffc3dc-ff16-4c05-8b80-044c80768010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "XZzyqi941ZX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv01 = nn.Conv2d(8, 32, 3,padding=1)#64\n",
        "        self.conv02 = nn.Conv2d(32, 32, 3,padding=1)#64\n",
        "        self.bn1=nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)#32\n",
        "\n",
        "        self.conv03 = nn.Conv2d(32, 64, 3,padding=1)#32\n",
        "        self.conv04 = nn.Conv2d(64, 64, 3,padding=1)#32\n",
        "        self.bn2=nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)#16\n",
        "\n",
        "        self.conv05 = nn.Conv2d(64, 128, 3,padding=1)#16\n",
        "        self.conv06 = nn.Conv2d(128, 128, 3,padding=1)#16\n",
        "        self.conv07 = nn.Conv2d(128, 128, 3,padding=1)#16\n",
        "        self.bn3=nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)#8\n",
        "\n",
        "        self.conv08 = nn.Conv2d(128, 256, 3,padding=1)#8\n",
        "        self.conv09 = nn.Conv2d(256, 256, 3,padding=1)#8\n",
        "        self.conv10 = nn.Conv2d(256, 256, 3,padding=1)#8\n",
        "        self.bn4=nn.BatchNorm2d(256)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)#4\n",
        "\n",
        "        self.conv11 = nn.Conv2d(256, 256, 3,padding=1)#4\n",
        "        self.conv12 = nn.Conv2d(256, 256, 3,padding=1)#4\n",
        "        self.conv13 = nn.Conv2d(256, 256, 3,padding=1)#4\n",
        "        self.bn5=nn.BatchNorm2d(256)\n",
        "        self.pool5 = nn.MaxPool2d(2, 2)#2\n",
        "\n",
        "       \n",
        "        self.fc1 = nn.Linear(256 * 2 * 2, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 4)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv01(x))\n",
        "        x = F.relu(self.conv02(x))\n",
        "        x = self.pool1(self.bn1(x))\n",
        "\n",
        "        x = F.relu(self.conv03(x))\n",
        "        x = F.relu(self.conv04(x))\n",
        "        x = self.pool2(self.bn2(x))\n",
        "\n",
        "        x = F.relu(self.conv05(x))\n",
        "        x = F.relu(self.conv06(x))\n",
        "        x = F.relu(self.conv07(x))\n",
        "        x = self.pool3(self.bn3(x))\n",
        "\n",
        "        x = F.relu(self.conv08(x))\n",
        "        x = F.relu(self.conv09(x))\n",
        "        x = F.relu(self.conv10(x))\n",
        "        x = self.pool4(self.bn4(x))\n",
        "\n",
        "        x = F.relu(self.conv11(x))\n",
        "        x = F.relu(self.conv12(x))\n",
        "        x = F.relu(self.conv13(x))\n",
        "        x = self.pool5(self.bn5(x))\n",
        "\n",
        "\n",
        "        x = x.view(-1, 256 * 2 * 2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return self.softmax(x)   "
      ],
      "metadata": {
        "id": "UB-Rc5Sd_VA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets"
      ],
      "metadata": {
        "id": "_48nYpmL1dsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class audioDataSet(Dataset):\n",
        "  def __init__(self,root_pth,test=False,transform = None):\n",
        "    print(\"Dataset initializing...\")\n",
        "    class_num=4\n",
        "    self.audio_pth = os.path.join(root_pth, 'audios', 'mfcc')\n",
        "    filling_type = np.load(os.path.join(root_pth, 'audios', 'filling_type.npy'))\n",
        "    pouring_or_shaking = np.load(os.path.join(root_pth,  'audios', 'pouring_or_shaking.npy'))\n",
        "    self.label = filling_type * pouring_or_shaking\n",
        "    self.is_test=test\n",
        "    self.each_class_size = []\n",
        "    for i in range(class_num):\n",
        "        self.each_class_size.append(np.count_nonzero(self.label==i))\n",
        "    # mx=0\n",
        "    # mn=1000\n",
        "    # for idx in tqdm(range(self.label.shape[0])):\n",
        "    #   data=np.load(os.path.join(self.audio_pth, \"{0:06d}\".format(idx+1) + '.npy'), allow_pickle=True)\n",
        "    #   tmp_max=np.max(data)\n",
        "    #   tmp_min=np.min(data)\n",
        "    #   if mx<tmp_max:\n",
        "    #       mx=tmp_max\n",
        "    #   if mn>tmp_min:\n",
        "    #       mn=tmp_min\n",
        "    mx=194.19187653405487\n",
        "    mn=-313.07119549054045\n",
        "    \n",
        "    self.mn=mn\n",
        "    self.mx=mx\n",
        "  def __len__(self):\n",
        "    return self.label.shape[0]\n",
        "  def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        lbl = -1\n",
        "\n",
        "        if self.is_test is False:\n",
        "            lbl = self.label[idx]\n",
        "        data=np.load(os.path.join(self.audio_pth, \"{0:06d}\".format(idx+1) + '.npy'), allow_pickle=True)\n",
        "        data= (data-self.mn)/(self.mx-self.mn)\n",
        "        data=data.transpose(2,0,1)\n",
        "        data=torch.from_numpy(data.astype(np.float32))\n",
        "        return data , lbl\n",
        "            \n",
        "  def get_each_class_size(self):\n",
        "    return np.array(self.each_class_size)"
      ],
      "metadata": {
        "id": "Cofl-UhojZwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydataset = audioDataSet(base_path)"
      ],
      "metadata": {
        "id": "hTAZbi9Es7KO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce76802d-bd57-47b6-bb49-216847f7bf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset initializing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(mydataset.mn)\n",
        "print(mydataset.mx)\n",
        "print(len(mydataset))"
      ],
      "metadata": {
        "id": "SORcf_X_5GQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2c94021-da3d-4b3b-e6a9-422556db302d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-313.07119549054045\n",
            "194.19187653405487\n",
            "31812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing"
      ],
      "metadata": {
        "id": "77VG-vce1gJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion = nn.CrossEntropyLoss()):\n",
        "  model.train()\n",
        "  loss_train = 0.0\n",
        "  correct_train = 0.0\n",
        "  num_train = len(train_loader)\n",
        "  for batch_idx, (audio, target) in enumerate(train_loader):\n",
        "    audio = audio.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(audio)\n",
        "\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_train += loss.item() / num_train\n",
        "    _, preds=torch.max(outputs,1)\n",
        "    correct_train+=torch.sum(preds==target).item()\n",
        "  \n",
        "  return loss_train, correct_train\n",
        "\n",
        "\n",
        "def evaluate(model, testloader, criterion = nn.CrossEntropyLoss()):\n",
        "  model.eval()\n",
        "  loss_test = 0\n",
        "  correct_test=0\n",
        "  num_val = len(testloader)\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (audio, target) in enumerate(testloader):\n",
        "      audio = audio.to(device)\n",
        "      target = target.to(device)\n",
        "      outputs = model.forward(audio)\n",
        "      loss = criterion(outputs, target)\n",
        "      loss_test += loss.item() / num_val\n",
        "      _, preds=torch.max(outputs,1)\n",
        "      correct_test+=torch.sum(preds==target).item()\n",
        "  \n",
        "  return loss_test, correct_test\n"
      ],
      "metadata": {
        "id": "kUH-0jkOqI2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-5\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch {}/{} train loss:{:.4f} train acc:{:.2f}% \".format(epoch+1,epochs, loss_train, 100 * correct_train/num_train))\n",
        "  print(\"Epoch {}/{} val loss:{:.4f} val acc:{:.2f}% \".format(epoch+1,epochs, loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  if loss_val < best_loss:\n",
        "    best_loss = loss_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_loss.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_val.pth\"))\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "Oad1iOtNl01a",
        "outputId": "b782ab67-6b89-4b71-e4ef-7073d7aeb6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/drive/MyDrive/COSRMAL_CHALLENGE/audios/mfcc/024382.npy'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ffcc9f37a2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m#start_time = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m#elapsed_time = time.time() - start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-079ebda9c01d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mcorrect_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnum_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-7148af745c50>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mlbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_pth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{0:06d}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mobile import MobileNetV3_Large\n",
        "\n",
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-3\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = MobileNetV3_Large().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  if loss_val < best_loss:\n",
        "    best_loss = loss_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"bl-mobile.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"bv-mobile.pth\"))\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPonh4SL1_M9",
        "outputId": "c686f3c5-efeb-4f61-9346-6dcd969391f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/200 train loss:0.5775 train acc:79.35% val loss:0.5985 val acc:79.21%\n",
            "2/200 train loss:0.3572 train acc:87.87% val loss:0.3697 val acc:86.89%\n",
            "3/200 train loss:0.2895 train acc:89.89% val loss:0.3728 val acc:87.68%\n",
            "4/200 train loss:0.2644 train acc:90.48% val loss:0.3346 val acc:88.31%\n",
            "5/200 train loss:0.2458 train acc:91.26% val loss:0.5997 val acc:73.06%\n",
            "6/200 train loss:0.2228 train acc:91.91% val loss:0.4331 val acc:88.59%\n",
            "7/200 train loss:0.2042 train acc:92.81% val loss:0.2907 val acc:89.39%\n",
            "8/200 train loss:0.1795 train acc:93.65% val loss:0.6188 val acc:78.47%\n",
            "9/200 train loss:0.1554 train acc:94.29% val loss:0.2454 val acc:91.56%\n",
            "10/200 train loss:0.1468 train acc:94.63% val loss:0.5624 val acc:80.92%\n",
            "11/200 train loss:0.1265 train acc:95.43% val loss:0.3431 val acc:89.66%\n",
            "12/200 train loss:0.1140 train acc:95.92% val loss:0.4471 val acc:88.10%\n",
            "13/200 train loss:0.1017 train acc:96.37% val loss:0.3115 val acc:91.62%\n",
            "14/200 train loss:0.0938 train acc:96.56% val loss:0.3609 val acc:89.55%\n",
            "15/200 train loss:0.0892 train acc:96.89% val loss:0.4583 val acc:87.60%\n",
            "16/200 train loss:0.0742 train acc:97.27% val loss:0.3772 val acc:89.75%\n",
            "17/200 train loss:0.0689 train acc:97.53% val loss:0.4517 val acc:84.72%\n",
            "18/200 train loss:0.0684 train acc:97.60% val loss:0.2592 val acc:91.65%\n",
            "19/200 train loss:0.0611 train acc:97.82% val loss:0.4175 val acc:89.83%\n",
            "20/200 train loss:0.0544 train acc:98.07% val loss:0.2997 val acc:90.98%\n",
            "21/200 train loss:0.0514 train acc:98.08% val loss:0.2955 val acc:91.83%\n",
            "22/200 train loss:0.0430 train acc:98.43% val loss:0.3018 val acc:92.30%\n",
            "23/200 train loss:0.0441 train acc:98.32% val loss:0.2883 val acc:92.46%\n",
            "24/200 train loss:0.0397 train acc:98.64% val loss:0.5684 val acc:84.68%\n",
            "25/200 train loss:0.0415 train acc:98.52% val loss:0.3164 val acc:92.33%\n",
            "26/200 train loss:0.0478 train acc:98.38% val loss:0.2758 val acc:91.84%\n",
            "27/200 train loss:0.0386 train acc:98.66% val loss:0.6551 val acc:86.61%\n",
            "28/200 train loss:0.0324 train acc:98.87% val loss:0.3178 val acc:91.48%\n",
            "29/200 train loss:0.0365 train acc:98.79% val loss:0.3633 val acc:90.40%\n",
            "30/200 train loss:0.0350 train acc:98.80% val loss:0.3538 val acc:90.15%\n",
            "31/200 train loss:0.0308 train acc:98.96% val loss:0.5222 val acc:88.37%\n",
            "32/200 train loss:0.0387 train acc:98.65% val loss:0.2714 val acc:92.03%\n",
            "33/200 train loss:0.0312 train acc:98.90% val loss:0.3591 val acc:91.62%\n",
            "34/200 train loss:0.0347 train acc:98.79% val loss:0.4019 val acc:91.26%\n",
            "35/200 train loss:0.0298 train acc:98.99% val loss:0.4210 val acc:91.64%\n",
            "36/200 train loss:0.0310 train acc:99.04% val loss:0.2825 val acc:93.86%\n",
            "37/200 train loss:0.0350 train acc:98.81% val loss:0.3438 val acc:92.36%\n",
            "38/200 train loss:0.0243 train acc:99.12% val loss:0.6150 val acc:89.91%\n",
            "39/200 train loss:0.0208 train acc:99.29% val loss:0.2779 val acc:93.37%\n",
            "40/200 train loss:0.0307 train acc:99.01% val loss:0.2294 val acc:93.92%\n",
            "41/200 train loss:0.0232 train acc:99.19% val loss:0.2789 val acc:93.10%\n",
            "42/200 train loss:0.0298 train acc:99.02% val loss:0.2499 val acc:93.51%\n",
            "43/200 train loss:0.0231 train acc:99.19% val loss:0.2469 val acc:94.03%\n",
            "44/200 train loss:0.0403 train acc:98.66% val loss:0.4367 val acc:91.70%\n",
            "45/200 train loss:0.0264 train acc:99.09% val loss:0.3652 val acc:89.83%\n",
            "46/200 train loss:0.0197 train acc:99.36% val loss:0.3089 val acc:91.51%\n",
            "47/200 train loss:0.0178 train acc:99.39% val loss:0.2455 val acc:93.81%\n",
            "48/200 train loss:0.0238 train acc:99.17% val loss:0.2965 val acc:92.99%\n",
            "49/200 train loss:0.0285 train acc:99.12% val loss:0.2769 val acc:93.23%\n",
            "50/200 train loss:0.0239 train acc:99.14% val loss:0.2322 val acc:94.56%\n",
            "51/200 train loss:0.0171 train acc:99.41% val loss:0.2272 val acc:94.92%\n",
            "52/200 train loss:0.0181 train acc:99.37% val loss:1.0437 val acc:77.37%\n",
            "53/200 train loss:0.0255 train acc:99.13% val loss:0.2448 val acc:94.48%\n",
            "54/200 train loss:0.0253 train acc:99.10% val loss:0.2403 val acc:93.82%\n",
            "55/200 train loss:0.0176 train acc:99.45% val loss:0.3003 val acc:93.42%\n",
            "56/200 train loss:0.0187 train acc:99.38% val loss:0.2772 val acc:94.11%\n",
            "57/200 train loss:0.0181 train acc:99.37% val loss:0.2650 val acc:93.40%\n",
            "58/200 train loss:0.0199 train acc:99.32% val loss:0.4116 val acc:90.96%\n",
            "59/200 train loss:0.0222 train acc:99.28% val loss:0.2709 val acc:93.98%\n",
            "60/200 train loss:0.0190 train acc:99.37% val loss:0.2379 val acc:94.89%\n",
            "61/200 train loss:0.0219 train acc:99.27% val loss:0.2398 val acc:93.87%\n",
            "62/200 train loss:0.0157 train acc:99.47% val loss:0.2284 val acc:95.16%\n",
            "63/200 train loss:0.0144 train acc:99.53% val loss:0.5360 val acc:90.52%\n",
            "64/200 train loss:0.0173 train acc:99.44% val loss:0.2653 val acc:93.48%\n",
            "65/200 train loss:0.0201 train acc:99.36% val loss:0.2694 val acc:93.68%\n",
            "66/200 train loss:0.0246 train acc:99.16% val loss:0.2235 val acc:94.31%\n",
            "67/200 train loss:0.0148 train acc:99.56% val loss:0.2176 val acc:95.41%\n",
            "68/200 train loss:0.0120 train acc:99.60% val loss:0.3185 val acc:94.14%\n",
            "69/200 train loss:0.0179 train acc:99.49% val loss:0.3083 val acc:93.38%\n",
            "70/200 train loss:0.0214 train acc:99.33% val loss:0.2914 val acc:94.03%\n",
            "71/200 train loss:0.0133 train acc:99.56% val loss:0.3163 val acc:93.21%\n",
            "72/200 train loss:0.0210 train acc:99.32% val loss:0.1915 val acc:95.16%\n",
            "73/200 train loss:0.0131 train acc:99.54% val loss:0.2856 val acc:93.67%\n",
            "74/200 train loss:0.0180 train acc:99.42% val loss:0.2475 val acc:94.69%\n",
            "75/200 train loss:0.0158 train acc:99.46% val loss:0.2917 val acc:93.38%\n",
            "76/200 train loss:0.0143 train acc:99.54% val loss:0.3382 val acc:93.48%\n",
            "77/200 train loss:0.0138 train acc:99.56% val loss:0.2649 val acc:94.47%\n",
            "78/200 train loss:0.0111 train acc:99.64% val loss:0.5711 val acc:89.94%\n",
            "79/200 train loss:0.0179 train acc:99.41% val loss:0.3225 val acc:92.13%\n",
            "80/200 train loss:0.0144 train acc:99.52% val loss:0.2604 val acc:93.82%\n",
            "81/200 train loss:0.0133 train acc:99.65% val loss:0.2429 val acc:94.88%\n",
            "82/200 train loss:0.0134 train acc:99.54% val loss:0.4548 val acc:92.93%\n",
            "83/200 train loss:0.0176 train acc:99.38% val loss:0.3554 val acc:89.96%\n",
            "84/200 train loss:0.0154 train acc:99.55% val loss:0.2632 val acc:93.68%\n",
            "85/200 train loss:0.0137 train acc:99.54% val loss:0.2350 val acc:94.55%\n",
            "86/200 train loss:0.0146 train acc:99.54% val loss:0.2901 val acc:94.19%\n",
            "87/200 train loss:0.0192 train acc:99.30% val loss:0.2598 val acc:93.10%\n",
            "88/200 train loss:0.0106 train acc:99.68% val loss:0.3471 val acc:92.96%\n",
            "89/200 train loss:0.0089 train acc:99.71% val loss:0.2740 val acc:93.86%\n",
            "90/200 train loss:0.0155 train acc:99.50% val loss:0.3772 val acc:90.79%\n",
            "91/200 train loss:0.0124 train acc:99.59% val loss:0.2805 val acc:93.75%\n",
            "92/200 train loss:0.0127 train acc:99.61% val loss:0.2308 val acc:94.58%\n",
            "93/200 train loss:0.0112 train acc:99.60% val loss:0.2739 val acc:94.17%\n",
            "94/200 train loss:0.0174 train acc:99.43% val loss:0.2136 val acc:95.10%\n",
            "95/200 train loss:0.0115 train acc:99.65% val loss:0.2027 val acc:95.35%\n",
            "96/200 train loss:0.0092 train acc:99.69% val loss:0.2078 val acc:95.38%\n",
            "97/200 train loss:0.0134 train acc:99.59% val loss:0.2264 val acc:94.94%\n",
            "98/200 train loss:0.0108 train acc:99.66% val loss:0.6957 val acc:84.57%\n",
            "99/200 train loss:0.0146 train acc:99.52% val loss:0.4127 val acc:91.42%\n",
            "100/200 train loss:0.0136 train acc:99.56% val loss:0.2106 val acc:94.78%\n",
            "101/200 train loss:0.0155 train acc:99.47% val loss:0.3544 val acc:91.78%\n",
            "102/200 train loss:0.0153 train acc:99.43% val loss:0.3698 val acc:93.26%\n",
            "103/200 train loss:0.0087 train acc:99.71% val loss:0.3259 val acc:93.32%\n",
            "104/200 train loss:0.0092 train acc:99.67% val loss:0.2786 val acc:94.19%\n",
            "105/200 train loss:0.0125 train acc:99.58% val loss:0.2382 val acc:94.64%\n",
            "106/200 train loss:0.0088 train acc:99.71% val loss:0.2443 val acc:94.94%\n",
            "107/200 train loss:0.0103 train acc:99.65% val loss:0.2687 val acc:94.81%\n",
            "108/200 train loss:0.0149 train acc:99.56% val loss:0.2416 val acc:94.74%\n",
            "109/200 train loss:0.0087 train acc:99.73% val loss:0.2256 val acc:95.41%\n",
            "110/200 train loss:0.0137 train acc:99.57% val loss:0.2531 val acc:94.66%\n",
            "111/200 train loss:0.0119 train acc:99.56% val loss:0.2312 val acc:95.05%\n",
            "112/200 train loss:0.0127 train acc:99.57% val loss:0.3020 val acc:93.26%\n",
            "113/200 train loss:0.0086 train acc:99.71% val loss:0.2358 val acc:95.16%\n",
            "114/200 train loss:0.0098 train acc:99.67% val loss:0.2695 val acc:93.81%\n",
            "115/200 train loss:0.0134 train acc:99.56% val loss:0.3655 val acc:92.19%\n",
            "116/200 train loss:0.0112 train acc:99.63% val loss:0.5550 val acc:91.76%\n",
            "117/200 train loss:0.0081 train acc:99.77% val loss:0.2669 val acc:93.84%\n",
            "118/200 train loss:0.0100 train acc:99.68% val loss:0.2577 val acc:94.86%\n",
            "119/200 train loss:0.0093 train acc:99.68% val loss:0.3864 val acc:91.20%\n",
            "120/200 train loss:0.0138 train acc:99.54% val loss:0.2975 val acc:94.06%\n",
            "121/200 train loss:0.0091 train acc:99.72% val loss:0.4174 val acc:91.73%\n",
            "122/200 train loss:0.0102 train acc:99.71% val loss:0.3208 val acc:93.84%\n",
            "123/200 train loss:0.0276 train acc:99.09% val loss:0.2678 val acc:94.81%\n",
            "124/200 train loss:0.0126 train acc:99.54% val loss:0.3209 val acc:93.31%\n",
            "125/200 train loss:0.0068 train acc:99.80% val loss:0.2452 val acc:94.85%\n",
            "126/200 train loss:0.0057 train acc:99.82% val loss:0.3176 val acc:94.59%\n",
            "127/200 train loss:0.0071 train acc:99.80% val loss:0.3067 val acc:94.41%\n",
            "128/200 train loss:0.0139 train acc:99.57% val loss:0.3065 val acc:94.00%\n",
            "129/200 train loss:0.0092 train acc:99.67% val loss:0.2361 val acc:95.30%\n",
            "130/200 train loss:0.0101 train acc:99.67% val loss:0.2508 val acc:94.72%\n",
            "131/200 train loss:0.0116 train acc:99.63% val loss:0.5624 val acc:89.36%\n",
            "132/200 train loss:0.0093 train acc:99.72% val loss:0.3998 val acc:93.04%\n",
            "133/200 train loss:0.0099 train acc:99.69% val loss:0.2870 val acc:94.78%\n",
            "134/200 train loss:0.0074 train acc:99.77% val loss:0.2369 val acc:95.38%\n",
            "135/200 train loss:0.0088 train acc:99.71% val loss:0.2733 val acc:94.56%\n",
            "136/200 train loss:0.0091 train acc:99.66% val loss:0.4408 val acc:92.08%\n",
            "137/200 train loss:0.0099 train acc:99.65% val loss:0.2336 val acc:95.16%\n",
            "138/200 train loss:0.0135 train acc:99.54% val loss:0.2719 val acc:94.92%\n",
            "139/200 train loss:0.0111 train acc:99.63% val loss:0.2011 val acc:95.69%\n",
            "140/200 train loss:0.0067 train acc:99.76% val loss:0.2214 val acc:95.79%\n",
            "141/200 train loss:0.0059 train acc:99.81% val loss:0.3569 val acc:93.07%\n",
            "142/200 train loss:0.0136 train acc:99.54% val loss:0.2362 val acc:94.89%\n",
            "143/200 train loss:0.0061 train acc:99.77% val loss:0.2964 val acc:94.69%\n",
            "144/200 train loss:0.0117 train acc:99.59% val loss:0.5341 val acc:86.89%\n",
            "145/200 train loss:0.0122 train acc:99.57% val loss:0.2566 val acc:94.12%\n",
            "146/200 train loss:0.0061 train acc:99.81% val loss:0.3185 val acc:94.64%\n",
            "147/200 train loss:0.0113 train acc:99.66% val loss:0.2691 val acc:94.63%\n",
            "148/200 train loss:0.0036 train acc:99.89% val loss:0.2530 val acc:95.55%\n",
            "149/200 train loss:0.0078 train acc:99.76% val loss:0.2800 val acc:93.71%\n",
            "150/200 train loss:0.0073 train acc:99.74% val loss:0.2721 val acc:94.69%\n",
            "151/200 train loss:0.0092 train acc:99.66% val loss:0.2368 val acc:95.14%\n",
            "152/200 train loss:0.0140 train acc:99.54% val loss:0.2292 val acc:94.92%\n",
            "153/200 train loss:0.0102 train acc:99.63% val loss:0.2492 val acc:94.69%\n",
            "154/200 train loss:0.0042 train acc:99.87% val loss:0.4118 val acc:93.04%\n",
            "155/200 train loss:0.0077 train acc:99.70% val loss:0.2394 val acc:94.75%\n",
            "156/200 train loss:0.0144 train acc:99.49% val loss:0.2552 val acc:94.92%\n",
            "157/200 train loss:0.0094 train acc:99.70% val loss:0.2574 val acc:94.14%\n",
            "158/200 train loss:0.0061 train acc:99.80% val loss:0.1973 val acc:96.31%\n",
            "159/200 train loss:0.0094 train acc:99.72% val loss:0.2338 val acc:95.13%\n",
            "160/200 train loss:0.0078 train acc:99.76% val loss:0.2910 val acc:94.11%\n",
            "161/200 train loss:0.0100 train acc:99.70% val loss:0.2471 val acc:94.86%\n",
            "162/200 train loss:0.0107 train acc:99.70% val loss:0.2020 val acc:95.76%\n",
            "163/200 train loss:0.0062 train acc:99.83% val loss:0.2410 val acc:95.29%\n",
            "164/200 train loss:0.0045 train acc:99.84% val loss:0.2173 val acc:95.87%\n",
            "165/200 train loss:0.0057 train acc:99.82% val loss:0.2877 val acc:93.92%\n",
            "166/200 train loss:0.0150 train acc:99.47% val loss:0.3225 val acc:92.13%\n",
            "167/200 train loss:0.0176 train acc:99.46% val loss:0.3484 val acc:93.92%\n",
            "168/200 train loss:0.0093 train acc:99.72% val loss:0.2582 val acc:95.38%\n",
            "169/200 train loss:0.0078 train acc:99.77% val loss:0.2317 val acc:95.38%\n",
            "170/200 train loss:0.0071 train acc:99.77% val loss:0.2382 val acc:95.13%\n",
            "171/200 train loss:0.0094 train acc:99.71% val loss:0.2422 val acc:94.64%\n",
            "172/200 train loss:0.0054 train acc:99.83% val loss:0.2422 val acc:95.51%\n",
            "173/200 train loss:0.0093 train acc:99.67% val loss:0.3035 val acc:94.37%\n",
            "174/200 train loss:0.0073 train acc:99.74% val loss:0.2112 val acc:95.91%\n",
            "175/200 train loss:0.0071 train acc:99.77% val loss:0.2260 val acc:95.33%\n",
            "176/200 train loss:0.0041 train acc:99.87% val loss:0.2160 val acc:96.02%\n",
            "177/200 train loss:0.0054 train acc:99.84% val loss:0.2876 val acc:94.59%\n",
            "178/200 train loss:0.0080 train acc:99.73% val loss:0.3012 val acc:93.87%\n",
            "179/200 train loss:0.0115 train acc:99.62% val loss:0.3200 val acc:92.35%\n",
            "180/200 train loss:0.0086 train acc:99.72% val loss:0.2879 val acc:93.97%\n",
            "181/200 train loss:0.0085 train acc:99.72% val loss:0.2327 val acc:95.16%\n",
            "182/200 train loss:0.0106 train acc:99.64% val loss:0.2163 val acc:95.58%\n",
            "183/200 train loss:0.0079 train acc:99.71% val loss:0.3225 val acc:94.47%\n",
            "184/200 train loss:0.0073 train acc:99.76% val loss:0.2316 val acc:95.76%\n",
            "185/200 train loss:0.0057 train acc:99.82% val loss:0.3127 val acc:94.17%\n",
            "186/200 train loss:0.0126 train acc:99.58% val loss:0.2705 val acc:94.91%\n",
            "187/200 train loss:0.0072 train acc:99.79% val loss:0.1868 val acc:95.95%\n",
            "188/200 train loss:0.0030 train acc:99.93% val loss:0.2295 val acc:95.93%\n",
            "189/200 train loss:0.0083 train acc:99.72% val loss:0.2336 val acc:95.47%\n",
            "190/200 train loss:0.0075 train acc:99.74% val loss:0.3197 val acc:93.34%\n",
            "191/200 train loss:0.0056 train acc:99.83% val loss:0.2351 val acc:95.30%\n",
            "192/200 train loss:0.0072 train acc:99.76% val loss:0.3483 val acc:94.11%\n",
            "193/200 train loss:0.0097 train acc:99.70% val loss:0.2406 val acc:94.70%\n",
            "194/200 train loss:0.0100 train acc:99.69% val loss:0.2827 val acc:94.70%\n",
            "195/200 train loss:0.0040 train acc:99.89% val loss:0.2139 val acc:95.63%\n",
            "196/200 train loss:0.0053 train acc:99.82% val loss:0.2404 val acc:94.81%\n",
            "197/200 train loss:0.0105 train acc:99.66% val loss:0.3483 val acc:93.64%\n",
            "198/200 train loss:0.0098 train acc:99.65% val loss:0.2214 val acc:95.46%\n",
            "199/200 train loss:0.0075 train acc:99.78% val loss:0.2074 val acc:95.99%\n",
            "200/200 train loss:0.0074 train acc:99.82% val loss:0.2289 val acc:95.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pretrained = torch.load(os.path.join(base_path, 'audios', \"bv-mobile.pth\"))\n",
        "model_pretrained.to(device)\n",
        "model_pretrained.eval()\n",
        "\n",
        "voting_dir = '/content/drive/MyDrive/COSRMAL_CHALLENGE/audios'\n",
        "\n",
        "mfcc_MAX_VALUE=194.19187653405487\n",
        "mfcc_MIN_VALUE=-313.07119549054045\n",
        "\n",
        "t2_MAX_VALUE = 57.464638\n",
        "t2_MIN_VALUE = -1.1948369\n",
        "save_size=64\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "audio_paths = [os.path.join(audio_folder, path) for path in sorted(os.listdir(audio_folder))]\n",
        "save_data = {}\n",
        "data_num = 0\n",
        "for i, path in enumerate(audio_paths):\n",
        "  count_pred = [0] * 4\n",
        "  pred_list = []\n",
        "  sample_rate, signal = scipy.io.wavfile.read(path)\n",
        "  ap = AudioProcessing(sample_rate,signal)\n",
        "  mfcc = ap.calc_MFCC()\n",
        "  mfcc_length=mfcc.shape[0]\n",
        "  f_step=int(mfcc.shape[1]*0.25)\n",
        "  f_length=mfcc.shape[1]\n",
        "  save_mfcc_num=int(np.ceil(float(np.abs(mfcc_length - save_size)) /f_step))\n",
        "  for i in range(save_mfcc_num):\n",
        "      tmp_mfcc = mfcc[i*f_step:save_size+i*f_step,: ,:]\n",
        "      tmp_mfcc= (tmp_mfcc-mfcc_MIN_VALUE)/(mfcc_MAX_VALUE-mfcc_MIN_VALUE)\n",
        "      tmp_mfcc=tmp_mfcc.transpose(2,0,1)\n",
        "      audio=torch.from_numpy(tmp_mfcc.astype(np.float32))\n",
        "      audio=torch.unsqueeze(audio, 0)\n",
        "      audio = audio.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        pred_T2 = model_pretrained.forward(audio)\n",
        "        _,pred_T2=torch.max(pred_T2,1)\n",
        "        count_pred[pred_T2.item()]+=1\n",
        "        pred_list.append(pred_T2.item())\n",
        "  if  count_pred[1]>5 or count_pred[2]>5 or count_pred[3]>5:\n",
        "    final_pred_T2=count_pred[1:4].index(max(count_pred[1:4]))+1\n",
        "  else:\n",
        "    final_pred_T2=0\n",
        "  \n",
        "  file_name = path.split(os.path.sep)[-1].replace('.wav', '')\n",
        "  #print(\"sequence:{}, frequency:{}\".format(file_name, count_pred))\n",
        "  to_save_data = {\"data_num\":data_num,\n",
        "                  \"file\":file_name,\n",
        "                  \"count_pred\":count_pred,\n",
        "                  \"final_pred\":final_pred_T2,\n",
        "                  'pred':pred_list}\n",
        "  save_data[\"{}\".format(file_name)] = to_save_data\n",
        "  data_num+=1\n",
        "\n",
        "with open (os.path.join(voting_dir, \"voting.json\"), 'w') as f:\n",
        "  json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
        "  elapsed_time = time.time() - start\n",
        "  print(\"elapsed_time:{}\".format(elapsed_time) + \"sec\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IRmiIp0hrkk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(os.path.join(voting_dir, \"voting.json\"))\n",
        "vote_js = json.load(f)\n",
        "\n",
        "vote = pd.DataFrame(vote_js).T\n",
        "vote.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "nv4DQKgjsHjV",
        "outputId": "cbe01d09-1d3b-4028-f234-460557dc8782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0e627883-9ca2-4b44-8dc7-9829f548bcef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_num</th>\n",
              "      <th>file</th>\n",
              "      <th>count_pred</th>\n",
              "      <th>final_pred</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>000000</th>\n",
              "      <td>0</td>\n",
              "      <td>000000</td>\n",
              "      <td>[19, 0, 19, 0]</td>\n",
              "      <td>2</td>\n",
              "      <td>[0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000001</th>\n",
              "      <td>1</td>\n",
              "      <td>000001</td>\n",
              "      <td>[13, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000002</th>\n",
              "      <td>2</td>\n",
              "      <td>000002</td>\n",
              "      <td>[56, 0, 0, 21]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000003</th>\n",
              "      <td>3</td>\n",
              "      <td>000003</td>\n",
              "      <td>[16, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000004</th>\n",
              "      <td>4</td>\n",
              "      <td>000004</td>\n",
              "      <td>[7, 9, 0, 0]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e627883-9ca2-4b44-8dc7-9829f548bcef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e627883-9ca2-4b44-8dc7-9829f548bcef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e627883-9ca2-4b44-8dc7-9829f548bcef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       data_num  ...                                               pred\n",
              "000000        0  ...  [0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...\n",
              "000001        1  ...            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "000002        2  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "000003        3  ...   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "000004        4  ...   [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gt = pd.read_csv('/content/drive/MyDrive/COSRMAL_CHALLENGE/train.csv')\n",
        "acc = np.sum(gt['filling_type'].to_numpy() == vote['final_pred'].to_numpy()) / len(gt['filling_type'])\n",
        "print('Acc: {:.2f}%'.format(100 * acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG3xQQqJoQpv",
        "outputId": "4c29b1c8-5005-4e50-8883-9b5659c4d5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient"
      ],
      "metadata": {
        "id": "VbMtniWYTnBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a EfficientNetV2 Model as defined in:\n",
        "Mingxing Tan, Quoc V. Le. (2021). \n",
        "EfficientNetV2: Smaller Models and Faster Training\n",
        "arXiv preprint arXiv:2104.00298.\n",
        "import from https://github.com/d-li14/mobilenetv2.pytorch\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "__all__ = ['effnetv2_s', 'effnetv2_m', 'effnetv2_l', 'effnetv2_xl']\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "# SiLU (Swish) activation function\n",
        "if hasattr(nn, 'SiLU'):\n",
        "    SiLU = nn.SiLU\n",
        "else:\n",
        "    # For compatibility with old PyTorch versions\n",
        "    class SiLU(nn.Module):\n",
        "        def forward(self, x):\n",
        "            return x * torch.sigmoid(x)\n",
        "\n",
        " \n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, inp, oup, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
        "                SiLU(),\n",
        "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "def conv_3x3_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
        "        super(MBConv, self).__init__()\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = round(inp * expand_ratio)\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "        if use_se:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                SELayer(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # fused\n",
        "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.identity:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class EffNetV2(nn.Module):\n",
        "    def __init__(self, cfgs, num_classes=1000, width_mult=1.):\n",
        "        super(EffNetV2, self).__init__()\n",
        "        self.cfgs = cfgs\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(24 * width_mult, 8)\n",
        "        layers = [conv_3x3_bn(8, input_channel, 2)]\n",
        "        # building inverted residual blocks\n",
        "        block = MBConv\n",
        "        for t, c, n, s, use_se in self.cfgs:\n",
        "            output_channel = _make_divisible(c * width_mult, 8)\n",
        "            for i in range(n):\n",
        "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n",
        "                input_channel = output_channel\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        # building last several layers\n",
        "        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n",
        "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(output_channel, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    def extract(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        feature = x.view(x.size(0), -1)\n",
        "        x = self.classifier(feature)\n",
        "\n",
        "        return feature, x\n",
        "\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.001)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def effnetv2_s(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-S model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  24,  2, 1, 0],\n",
        "        [4,  48,  4, 2, 0],\n",
        "        [4,  64,  4, 2, 0],\n",
        "        [4, 128,  6, 2, 1],\n",
        "        [6, 160,  9, 1, 1],\n",
        "        [6, 256, 15, 2, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)\n",
        "\n",
        "\n",
        "def effnetv2_m(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-M model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  24,  3, 1, 0],\n",
        "        [4,  48,  5, 2, 0],\n",
        "        [4,  80,  5, 2, 0],\n",
        "        [4, 160,  7, 2, 1],\n",
        "        [6, 176, 14, 1, 1],\n",
        "        [6, 304, 18, 2, 1],\n",
        "        [6, 512,  5, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)\n",
        "\n",
        "\n",
        "def effnetv2_l(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-L model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  32,  4, 1, 0],\n",
        "        [4,  64,  7, 2, 0],\n",
        "        [4,  96,  7, 2, 0],\n",
        "        [4, 192, 10, 2, 1],\n",
        "        [6, 224, 19, 1, 1],\n",
        "        [6, 384, 25, 2, 1],\n",
        "        [6, 640,  7, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)\n",
        "\n",
        "\n",
        "def effnetv2_xl(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-XL model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  32,  4, 1, 0],\n",
        "        [4,  64,  8, 2, 0],\n",
        "        [4,  96,  8, 2, 0],\n",
        "        [4, 192, 16, 2, 1],\n",
        "        [6, 256, 24, 1, 1],\n",
        "        [6, 512, 32, 2, 1],\n",
        "        [6, 640,  8, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)"
      ],
      "metadata": {
        "id": "FNCNhMeqTq4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient train"
      ],
      "metadata": {
        "id": "cPtaKaY1ToDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_save_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE'\n",
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = effnetv2_xl().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True,\n",
        "                            num_workers=1)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "  \n",
        "\n",
        "  # if loss_val < best_loss:\n",
        "  #   best_loss = loss_val\n",
        "  #   torch.save(model, os.path.join(base_path, 'audios', \"bl-efficient-xl.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(my_save_path, \n",
        "                                              'audios', \n",
        "                                              'efficient',\n",
        "                                              \"XL-{:.2f}.pth\".format(100 * correct_val/num_val)))\n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fNmPh7WnklwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a3e36a-0060-47cb-ed62-76dcd1065bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/200 train loss:1.8104 train acc:69.01% val loss:0.8256 val acc:77.60%\n",
            "2/200 train loss:0.6068 train acc:79.74% val loss:0.6122 val acc:77.26%\n",
            "3/200 train loss:0.4565 train acc:84.31% val loss:0.3969 val acc:86.47%\n",
            "4/200 train loss:0.3449 train acc:88.14% val loss:0.4365 val acc:84.02%\n",
            "5/200 train loss:0.2854 train acc:89.94% val loss:0.3182 val acc:89.44%\n",
            "6/200 train loss:0.2235 train acc:92.07% val loss:0.3550 val acc:89.74%\n",
            "7/200 train loss:0.1681 train acc:94.10% val loss:0.4066 val acc:85.31%\n",
            "8/200 train loss:0.1265 train acc:95.56% val loss:0.2367 val acc:92.22%\n",
            "9/200 train loss:0.0840 train acc:97.08% val loss:0.2572 val acc:92.42%\n",
            "10/200 train loss:0.0688 train acc:97.67% val loss:0.2259 val acc:93.59%\n",
            "11/200 train loss:0.0497 train acc:98.29% val loss:0.2650 val acc:91.42%\n",
            "12/200 train loss:0.0429 train acc:98.55% val loss:0.2763 val acc:92.44%\n",
            "13/200 train loss:0.0411 train acc:98.69% val loss:0.7908 val acc:77.48%\n",
            "14/200 train loss:0.0360 train acc:98.81% val loss:0.3213 val acc:92.71%\n",
            "15/200 train loss:0.0304 train acc:99.01% val loss:0.2228 val acc:94.06%\n",
            "16/200 train loss:0.0257 train acc:99.24% val loss:0.2295 val acc:93.95%\n",
            "17/200 train loss:0.0245 train acc:99.24% val loss:0.3219 val acc:91.94%\n",
            "18/200 train loss:0.0234 train acc:99.28% val loss:0.2185 val acc:93.89%\n",
            "19/200 train loss:0.0232 train acc:99.32% val loss:0.2437 val acc:94.61%\n",
            "20/200 train loss:0.0370 train acc:98.88% val loss:0.2241 val acc:93.89%\n",
            "21/200 train loss:0.0243 train acc:99.27% val loss:0.2161 val acc:94.20%\n",
            "22/200 train loss:0.0218 train acc:99.33% val loss:0.2597 val acc:93.67%\n",
            "23/200 train loss:0.0181 train acc:99.43% val loss:0.3131 val acc:92.74%\n",
            "24/200 train loss:0.0196 train acc:99.45% val loss:0.2677 val acc:94.30%\n",
            "25/200 train loss:0.0266 train acc:99.23% val loss:0.2206 val acc:94.34%\n",
            "26/200 train loss:0.0204 train acc:99.37% val loss:0.2592 val acc:94.39%\n",
            "27/200 train loss:0.0131 train acc:99.64% val loss:0.2494 val acc:94.77%\n",
            "28/200 train loss:0.0210 train acc:99.32% val loss:0.1787 val acc:95.49%\n",
            "29/200 train loss:0.0091 train acc:99.74% val loss:0.2403 val acc:95.35%\n",
            "30/200 train loss:0.0244 train acc:99.17% val loss:0.2102 val acc:94.96%\n",
            "31/200 train loss:0.0202 train acc:99.41% val loss:0.1729 val acc:95.38%\n",
            "32/200 train loss:0.0117 train acc:99.63% val loss:0.2126 val acc:94.17%\n",
            "33/200 train loss:0.0140 train acc:99.57% val loss:0.1876 val acc:95.18%\n",
            "34/200 train loss:0.0141 train acc:99.61% val loss:0.2166 val acc:95.65%\n",
            "35/200 train loss:0.0115 train acc:99.65% val loss:0.2587 val acc:93.95%\n",
            "36/200 train loss:0.0160 train acc:99.51% val loss:0.3267 val acc:92.52%\n",
            "37/200 train loss:0.0170 train acc:99.45% val loss:0.2739 val acc:94.69%\n",
            "38/200 train loss:0.0133 train acc:99.60% val loss:0.2174 val acc:95.05%\n",
            "39/200 train loss:0.0120 train acc:99.63% val loss:0.3444 val acc:91.67%\n",
            "40/200 train loss:0.0136 train acc:99.65% val loss:0.3216 val acc:93.05%\n",
            "41/200 train loss:0.0070 train acc:99.82% val loss:0.2433 val acc:95.47%\n",
            "42/200 train loss:0.0116 train acc:99.68% val loss:0.7310 val acc:93.90%\n",
            "43/200 train loss:0.0146 train acc:99.55% val loss:0.1961 val acc:95.47%\n",
            "44/200 train loss:0.0164 train acc:99.52% val loss:0.2855 val acc:93.78%\n",
            "45/200 train loss:0.0151 train acc:99.49% val loss:0.1912 val acc:95.62%\n",
            "46/200 train loss:0.0069 train acc:99.82% val loss:0.1814 val acc:95.99%\n",
            "47/200 train loss:0.0150 train acc:99.53% val loss:0.3210 val acc:94.86%\n",
            "48/200 train loss:0.0098 train acc:99.69% val loss:0.2086 val acc:95.49%\n",
            "49/200 train loss:0.0124 train acc:99.62% val loss:0.1794 val acc:96.01%\n",
            "50/200 train loss:0.0085 train acc:99.76% val loss:0.2000 val acc:95.82%\n",
            "51/200 train loss:0.0077 train acc:99.74% val loss:0.1811 val acc:96.12%\n",
            "52/200 train loss:0.0062 train acc:99.81% val loss:0.2157 val acc:95.08%\n",
            "53/200 train loss:0.0102 train acc:99.70% val loss:0.2973 val acc:94.25%\n",
            "54/200 train loss:0.0143 train acc:99.55% val loss:0.2780 val acc:95.74%\n",
            "55/200 train loss:0.0085 train acc:99.76% val loss:0.2158 val acc:96.09%\n",
            "56/200 train loss:0.0097 train acc:99.69% val loss:0.4127 val acc:93.01%\n",
            "57/200 train loss:0.0115 train acc:99.63% val loss:0.1918 val acc:95.93%\n",
            "58/200 train loss:0.0055 train acc:99.81% val loss:0.2464 val acc:94.86%\n",
            "59/200 train loss:0.0083 train acc:99.76% val loss:0.2182 val acc:95.77%\n",
            "60/200 train loss:0.0073 train acc:99.78% val loss:0.2302 val acc:95.66%\n",
            "61/200 train loss:0.0118 train acc:99.65% val loss:0.2131 val acc:95.22%\n",
            "62/200 train loss:0.0045 train acc:99.87% val loss:0.2700 val acc:94.77%\n",
            "63/200 train loss:0.0101 train acc:99.72% val loss:0.2046 val acc:95.71%\n",
            "64/200 train loss:0.0104 train acc:99.75% val loss:0.5222 val acc:89.11%\n",
            "65/200 train loss:0.0080 train acc:99.78% val loss:0.1946 val acc:96.13%\n",
            "66/200 train loss:0.0038 train acc:99.89% val loss:0.3264 val acc:94.50%\n",
            "67/200 train loss:0.0112 train acc:99.61% val loss:0.2290 val acc:95.74%\n",
            "68/200 train loss:0.0098 train acc:99.70% val loss:0.2073 val acc:95.40%\n",
            "69/200 train loss:0.0136 train acc:99.61% val loss:0.1940 val acc:95.21%\n",
            "70/200 train loss:0.0069 train acc:99.78% val loss:0.2130 val acc:95.84%\n",
            "71/200 train loss:0.0032 train acc:99.92% val loss:0.1781 val acc:96.40%\n",
            "72/200 train loss:0.0022 train acc:99.92% val loss:0.2034 val acc:96.31%\n",
            "73/200 train loss:0.0127 train acc:99.60% val loss:0.2283 val acc:94.92%\n",
            "74/200 train loss:0.0076 train acc:99.78% val loss:0.1885 val acc:96.20%\n",
            "75/200 train loss:0.0042 train acc:99.89% val loss:0.1943 val acc:96.26%\n",
            "76/200 train loss:0.0017 train acc:99.97% val loss:0.2253 val acc:95.71%\n",
            "77/200 train loss:0.0082 train acc:99.77% val loss:0.2667 val acc:94.63%\n",
            "78/200 train loss:0.0133 train acc:99.59% val loss:0.1818 val acc:95.85%\n",
            "79/200 train loss:0.0035 train acc:99.92% val loss:0.2667 val acc:95.00%\n",
            "80/200 train loss:0.0109 train acc:99.65% val loss:0.2616 val acc:95.05%\n",
            "81/200 train loss:0.0075 train acc:99.76% val loss:0.2334 val acc:96.01%\n",
            "82/200 train loss:0.0048 train acc:99.86% val loss:0.2772 val acc:93.40%\n",
            "83/200 train loss:0.0150 train acc:99.58% val loss:0.2562 val acc:95.08%\n",
            "84/200 train loss:0.0054 train acc:99.81% val loss:0.1760 val acc:96.67%\n",
            "85/200 train loss:0.0026 train acc:99.95% val loss:0.2693 val acc:95.62%\n",
            "86/200 train loss:0.0062 train acc:99.80% val loss:0.3063 val acc:93.35%\n",
            "87/200 train loss:0.0080 train acc:99.74% val loss:0.1700 val acc:96.42%\n",
            "88/200 train loss:0.0013 train acc:99.96% val loss:0.2395 val acc:95.85%\n",
            "89/200 train loss:0.0109 train acc:99.65% val loss:0.1697 val acc:96.51%\n",
            "90/200 train loss:0.0101 train acc:99.71% val loss:0.2099 val acc:95.73%\n",
            "91/200 train loss:0.0051 train acc:99.86% val loss:0.1908 val acc:96.29%\n",
            "92/200 train loss:0.0020 train acc:99.94% val loss:0.2014 val acc:96.45%\n",
            "93/200 train loss:0.0008 train acc:99.98% val loss:0.2405 val acc:95.90%\n",
            "94/200 train loss:0.0076 train acc:99.77% val loss:0.3364 val acc:93.81%\n",
            "95/200 train loss:0.0107 train acc:99.68% val loss:0.1743 val acc:96.20%\n",
            "96/200 train loss:0.0038 train acc:99.89% val loss:0.2179 val acc:95.84%\n",
            "97/200 train loss:0.0060 train acc:99.85% val loss:0.1826 val acc:96.50%\n",
            "98/200 train loss:0.0058 train acc:99.82% val loss:0.1917 val acc:96.15%\n",
            "99/200 train loss:0.0094 train acc:99.74% val loss:0.1834 val acc:96.17%\n",
            "100/200 train loss:0.0041 train acc:99.86% val loss:0.2614 val acc:95.29%\n",
            "101/200 train loss:0.0084 train acc:99.78% val loss:0.2158 val acc:96.01%\n",
            "102/200 train loss:0.0019 train acc:99.94% val loss:0.1952 val acc:96.18%\n",
            "103/200 train loss:0.0046 train acc:99.86% val loss:0.2479 val acc:95.82%\n",
            "104/200 train loss:0.0088 train acc:99.72% val loss:0.1889 val acc:96.32%\n",
            "105/200 train loss:0.0029 train acc:99.92% val loss:0.1722 val acc:96.87%\n",
            "106/200 train loss:0.0081 train acc:99.73% val loss:0.1906 val acc:95.76%\n",
            "107/200 train loss:0.0049 train acc:99.86% val loss:0.1862 val acc:96.13%\n",
            "108/200 train loss:0.0034 train acc:99.87% val loss:0.3698 val acc:93.60%\n",
            "109/200 train loss:0.0056 train acc:99.85% val loss:0.1675 val acc:97.00%\n",
            "110/200 train loss:0.0021 train acc:99.96% val loss:0.1638 val acc:96.79%\n",
            "111/200 train loss:0.0002 train acc:100.00% val loss:0.1770 val acc:97.01%\n",
            "112/200 train loss:0.0003 train acc:100.00% val loss:0.1854 val acc:96.89%\n",
            "113/200 train loss:0.0001 train acc:100.00% val loss:0.1895 val acc:96.86%\n",
            "114/200 train loss:0.0001 train acc:100.00% val loss:0.1914 val acc:96.97%\n",
            "115/200 train loss:0.0001 train acc:100.00% val loss:0.2018 val acc:97.00%\n",
            "116/200 train loss:0.0000 train acc:100.00% val loss:0.1946 val acc:96.95%\n",
            "117/200 train loss:0.0000 train acc:100.00% val loss:0.1745 val acc:97.00%\n",
            "118/200 train loss:0.0000 train acc:100.00% val loss:0.2231 val acc:96.94%\n",
            "119/200 train loss:0.0000 train acc:100.00% val loss:0.2456 val acc:96.95%\n",
            "120/200 train loss:0.0000 train acc:100.00% val loss:0.2123 val acc:97.06%\n",
            "121/200 train loss:0.0000 train acc:100.00% val loss:0.2038 val acc:97.00%\n",
            "122/200 train loss:0.0000 train acc:100.00% val loss:0.2347 val acc:97.06%\n",
            "123/200 train loss:0.0000 train acc:100.00% val loss:0.3015 val acc:97.03%\n",
            "124/200 train loss:0.0000 train acc:100.00% val loss:0.2562 val acc:97.01%\n",
            "125/200 train loss:0.0000 train acc:100.00% val loss:0.1836 val acc:96.75%\n",
            "126/200 train loss:0.0000 train acc:100.00% val loss:0.2538 val acc:97.06%\n",
            "127/200 train loss:0.0616 train acc:98.40% val loss:0.1697 val acc:95.54%\n",
            "128/200 train loss:0.0105 train acc:99.75% val loss:0.1891 val acc:96.07%\n",
            "129/200 train loss:0.0048 train acc:99.86% val loss:0.1907 val acc:96.31%\n",
            "130/200 train loss:0.0037 train acc:99.90% val loss:0.2359 val acc:95.58%\n",
            "131/200 train loss:0.0114 train acc:99.66% val loss:0.1737 val acc:96.59%\n",
            "132/200 train loss:0.0018 train acc:99.94% val loss:0.2439 val acc:95.91%\n",
            "133/200 train loss:0.0043 train acc:99.89% val loss:0.2931 val acc:95.43%\n",
            "134/200 train loss:0.0082 train acc:99.74% val loss:0.2411 val acc:95.52%\n",
            "135/200 train loss:0.0101 train acc:99.74% val loss:0.1792 val acc:96.53%\n",
            "136/200 train loss:0.0019 train acc:99.93% val loss:0.1915 val acc:96.65%\n",
            "137/200 train loss:0.0002 train acc:100.00% val loss:0.1830 val acc:96.92%\n",
            "138/200 train loss:0.0001 train acc:100.00% val loss:0.1844 val acc:97.01%\n",
            "139/200 train loss:0.0001 train acc:100.00% val loss:0.1921 val acc:96.98%\n",
            "140/200 train loss:0.0001 train acc:100.00% val loss:0.1919 val acc:96.94%\n",
            "141/200 train loss:0.0001 train acc:100.00% val loss:0.1974 val acc:96.98%\n",
            "142/200 train loss:0.0000 train acc:100.00% val loss:0.1993 val acc:96.97%\n",
            "143/200 train loss:0.0000 train acc:100.00% val loss:0.1956 val acc:97.00%\n",
            "144/200 train loss:0.0001 train acc:100.00% val loss:0.2400 val acc:96.54%\n",
            "145/200 train loss:0.0370 train acc:98.90% val loss:0.2718 val acc:94.22%\n",
            "146/200 train loss:0.0124 train acc:99.67% val loss:0.1864 val acc:96.24%\n",
            "147/200 train loss:0.0016 train acc:99.96% val loss:0.1807 val acc:96.83%\n",
            "148/200 train loss:0.0004 train acc:99.98% val loss:0.1717 val acc:96.97%\n",
            "149/200 train loss:0.0013 train acc:99.97% val loss:0.2001 val acc:95.41%\n",
            "150/200 train loss:0.0062 train acc:99.80% val loss:0.2289 val acc:95.41%\n",
            "151/200 train loss:0.0121 train acc:99.66% val loss:0.1928 val acc:96.13%\n",
            "152/200 train loss:0.0026 train acc:99.92% val loss:0.1966 val acc:96.54%\n",
            "153/200 train loss:0.0027 train acc:99.92% val loss:0.2055 val acc:96.50%\n",
            "154/200 train loss:0.0063 train acc:99.83% val loss:0.1735 val acc:96.70%\n",
            "155/200 train loss:0.0036 train acc:99.89% val loss:0.2476 val acc:95.44%\n",
            "156/200 train loss:0.0057 train acc:99.81% val loss:0.2937 val acc:95.19%\n",
            "157/200 train loss:0.0086 train acc:99.71% val loss:0.2015 val acc:95.95%\n",
            "158/200 train loss:0.0040 train acc:99.91% val loss:0.1993 val acc:96.57%\n",
            "159/200 train loss:0.0010 train acc:99.96% val loss:0.1716 val acc:97.14%\n",
            "160/200 train loss:0.0015 train acc:99.96% val loss:0.1757 val acc:97.11%\n",
            "161/200 train loss:0.0023 train acc:99.92% val loss:0.2705 val acc:95.87%\n",
            "162/200 train loss:0.0112 train acc:99.65% val loss:0.1797 val acc:96.62%\n",
            "163/200 train loss:0.0045 train acc:99.87% val loss:0.2136 val acc:96.10%\n",
            "164/200 train loss:0.0069 train acc:99.78% val loss:0.1825 val acc:96.62%\n",
            "165/200 train loss:0.0019 train acc:99.95% val loss:0.1920 val acc:96.68%\n",
            "166/200 train loss:0.0005 train acc:99.98% val loss:0.1947 val acc:96.67%\n",
            "167/200 train loss:0.0001 train acc:100.00% val loss:0.1877 val acc:96.86%\n",
            "168/200 train loss:0.0001 train acc:100.00% val loss:0.1945 val acc:96.92%\n",
            "169/200 train loss:0.0001 train acc:100.00% val loss:0.1973 val acc:96.94%\n",
            "170/200 train loss:0.0000 train acc:100.00% val loss:0.1975 val acc:96.95%\n",
            "171/200 train loss:0.0000 train acc:100.00% val loss:0.1988 val acc:96.95%\n",
            "172/200 train loss:0.0000 train acc:100.00% val loss:0.1973 val acc:96.94%\n",
            "173/200 train loss:0.0000 train acc:100.00% val loss:0.2019 val acc:96.98%\n",
            "174/200 train loss:0.0000 train acc:100.00% val loss:0.2051 val acc:96.90%\n",
            "175/200 train loss:0.0000 train acc:100.00% val loss:0.2043 val acc:96.89%\n",
            "176/200 train loss:0.0000 train acc:100.00% val loss:0.2096 val acc:96.95%\n",
            "177/200 train loss:0.0000 train acc:100.00% val loss:0.2114 val acc:96.98%\n",
            "178/200 train loss:0.0000 train acc:100.00% val loss:0.2153 val acc:97.05%\n",
            "179/200 train loss:0.0000 train acc:100.00% val loss:0.2183 val acc:97.00%\n",
            "180/200 train loss:0.0000 train acc:100.00% val loss:0.2235 val acc:97.06%\n",
            "181/200 train loss:0.0000 train acc:100.00% val loss:0.2222 val acc:97.00%\n",
            "182/200 train loss:0.0000 train acc:100.00% val loss:0.2259 val acc:96.97%\n",
            "183/200 train loss:0.0000 train acc:100.00% val loss:0.2387 val acc:97.08%\n",
            "184/200 train loss:0.0000 train acc:100.00% val loss:0.2390 val acc:96.95%\n",
            "185/200 train loss:0.0558 train acc:98.40% val loss:0.2252 val acc:94.92%\n",
            "186/200 train loss:0.0064 train acc:99.80% val loss:0.1903 val acc:96.20%\n",
            "187/200 train loss:0.0020 train acc:99.95% val loss:0.1962 val acc:96.48%\n",
            "188/200 train loss:0.0050 train acc:99.83% val loss:0.2663 val acc:95.32%\n",
            "189/200 train loss:0.0096 train acc:99.72% val loss:0.2046 val acc:96.21%\n",
            "190/200 train loss:0.0063 train acc:99.80% val loss:0.2893 val acc:95.13%\n",
            "191/200 train loss:0.0039 train acc:99.89% val loss:0.2087 val acc:96.39%\n",
            "192/200 train loss:0.0024 train acc:99.94% val loss:0.2103 val acc:96.64%\n",
            "193/200 train loss:0.0011 train acc:99.96% val loss:0.2204 val acc:96.32%\n",
            "194/200 train loss:0.0042 train acc:99.89% val loss:0.2639 val acc:95.22%\n",
            "195/200 train loss:0.0059 train acc:99.79% val loss:0.2963 val acc:94.50%\n",
            "196/200 train loss:0.0070 train acc:99.76% val loss:0.2531 val acc:95.29%\n",
            "197/200 train loss:0.0067 train acc:99.77% val loss:0.1819 val acc:96.75%\n",
            "198/200 train loss:0.0022 train acc:99.95% val loss:0.1838 val acc:96.92%\n",
            "199/200 train loss:0.0024 train acc:99.92% val loss:0.2326 val acc:96.23%\n",
            "200/200 train loss:0.0056 train acc:99.80% val loss:0.2702 val acc:95.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task1 filling level"
      ],
      "metadata": {
        "id": "CktpEYVM-s9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0WoGDyU2d0A",
        "outputId": "b81bfc3d-1f46-40da-f145-bcc8f336e05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE/'\n",
        "audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio/'\n",
        "os.makedirs(os.path.join(base_path, 'audios'), exist_ok=True)\n",
        "mfcc_path_ = (os.path.join(base_path, 'audios', 'mfcc_t1'))\n",
        "os.makedirs(mfcc_path_,exist_ok=True)\n",
        "\n",
        "audio_paths = [os.path.join(audio_folder, path) for path in sorted(os.listdir(audio_folder))]\n",
        "save_size=64\n",
        "ratio_step = 0.25\n",
        "count = 0\n",
        "\n",
        "pbar = tqdm(total=len(audio_paths))\n",
        "\n",
        "# for i, path in enumerate(audio_paths):\n",
        "#   mfcc_list = []\n",
        "#   id = i\n",
        "#   start_time = gt[gt.id==id]['start'].item()\n",
        "#   end_time = gt[gt.id==id]['end'].item()\n",
        "#   filling_level= gt[gt.id==id]['filling_level'].item()\n",
        "#   sample_rate, signal = scipy.io.wavfile.read(path)\n",
        "\n",
        "\n",
        "#   # TODO: Check whether it is trimed from the start\n",
        "#   if id == 377:\n",
        "#     signal = signal[:int(sample_rate * 13), :]\n",
        "#   ap = AudioProcessing(sample_rate,signal,nfilt=save_size)\n",
        "#   mfcc = ap.calc_MFCC()\n",
        "#   mfcc_length=mfcc.shape[0]\n",
        "\n",
        "#   if mfcc_length < save_size:\n",
        "#     print(\"file {} is too short\".format(id))\n",
        "#   else:\n",
        "#     f_step=int(mfcc.shape[1]*ratio_step)\n",
        "#     f_length=mfcc.shape[1]\n",
        "#     save_mfcc_num=int(np.ceil(float(np.abs(mfcc_length - save_size)) / f_step))\n",
        "\n",
        "#     for i in range(save_mfcc_num):\n",
        "#       count += 1\n",
        "#       tmp_mfcc = mfcc[i*f_step:save_size+i*f_step,: ,:]\n",
        "\n",
        "#       mfcc_list.append(tmp_mfcc)\n",
        "    \n",
        "#     mfcc_video = np.array(mfcc_list)\n",
        "#     assert len(mfcc_video.shape) == 4, \"4 channel\"\n",
        "#     assert mfcc_video.shape[1] +  mfcc_video.shape[2] + mfcc_video.shape[3]== 136, \"136 dims\"\n",
        "#     np.save(os.path.join(mfcc_path_, \"{0:06d}\".format(id)), mfcc_video)\n",
        "#   pbar.update()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1ed8742defd048188d5c12cd18fbc800",
            "bec0d10affe541ff8e64c91d2bdb5ac0",
            "40b2bcbc586c41919bf0f6a4e853178a",
            "05666dca53f1440ab7b089c07ad9a3d4",
            "991eedd0420a4684a99d06159e632534",
            "29f244d469db4f25a5754659d251e684",
            "e36aada47dd44b919fda2ca9a10ea557",
            "e5974246df8e4463a4b61910e941afcf",
            "99fba8a3919f4d6180fe7630cc983dc2",
            "f8a7bd01426444fc82f193985970c19b",
            "4d6d71f1edb7469ab022d44c8043a0cb"
          ]
        },
        "id": "JCGuIY8au4eH",
        "outputId": "00602068-35e1-4d19-977c-f78e54ecaaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ed8742defd048188d5c12cd18fbc800",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Noa56xWaY6of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gt['filling_type'].to_numpy().shape)\n",
        "print(gt['id'].to_numpy().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8ZJr4MszDOn",
        "outputId": "b712e51c-8bb8-4306-ae31-4192dd71cdf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(684,)\n",
            "(684,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class audioDataSet_t1(Dataset):\n",
        "  def __init__(self,root_pth,test=False,transform = None,padding_size = 100):\n",
        "    print(\"Dataset initializing...\")\n",
        "    class_num=3\n",
        "    self.audio_pth = os.path.join(root_pth, 'audios', 'mfcc_t1')\n",
        "    self.label = gt['filling_type'].to_numpy()\n",
        "    self.is_test=test\n",
        "    self.each_class_size = []\n",
        "    for i in range(class_num):\n",
        "        self.each_class_size.append(np.count_nonzero(self.label==i))\n",
        " \n",
        "    mx=194.19187653405487\n",
        "    mn=-313.07119549054045\n",
        "    \n",
        "    self.mn=mn\n",
        "    self.mx=mx\n",
        "  def __len__(self):\n",
        "    return self.label.shape[0]\n",
        "  def __getitem__(self, idx):\n",
        "       \n",
        "        lbl = self.label[idx]\n",
        "        data=np.load(os.path.join(self.audio_pth, \"{0:06d}\".format(idx) + '.npy'), allow_pickle=True)\n",
        "        data= (data-self.mn)/(self.mx-self.mn)\n",
        "        data=data.transpose(0, 3, 1, 2)\n",
        "        data=torch.from_numpy(data.astype(np.float32))\n",
        "        return data , lbl\n",
        "            \n",
        "  def get_each_class_size(self):\n",
        "    return np.array(self.each_class_size)"
      ],
      "metadata": {
        "id": "rIkcz0WUt3tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "  def __init__(self, feature):\n",
        "    super(CNN_LSTM, self).__init__()\n",
        "    self.feature = feature\n",
        "    self.proj = nn.Linear(1792, 512)\n",
        "    self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=3, bidirectional=True)\n",
        "    self.fc1 = nn.Linear(512, 128)\n",
        "    self.fc2 = nn.Linear(128, 3)\n",
        "  def forward(self, x_3d):\n",
        "    # bs, t, c, h, w\n",
        "    hidden = None\n",
        "    for t in range(x_3d.size(1)):\n",
        "      with torch.no_grad():\n",
        "        x = self.feature.extract(x_3d[:, t, :, :, :])\n",
        "      x = self.proj(x)\n",
        "      out, hidden = self.lstm(x.unsqueeze(0), hidden) \n",
        "\n",
        "    x = self.fc1(out[-1, :, :])\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jgq8j8I2VWhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mobile"
      ],
      "metadata": {
        "id": "yeAhRAMh78N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "\n",
        "\n",
        "class hswish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        out = x * F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class hsigmoid(nn.Module):\n",
        "    def forward(self, x):\n",
        "        out = F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class SeModule(nn.Module):\n",
        "    def __init__(self, in_size, reduction=4):\n",
        "        super(SeModule, self).__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_size, in_size // reduction, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(in_size // reduction),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_size // reduction, in_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(in_size),\n",
        "            hsigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.se(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise'''\n",
        "    def __init__(self, kernel_size, in_size, expand_size, out_size, nolinear, semodule, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.se = semodule\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(expand_size)\n",
        "        self.nolinear1 = nolinear\n",
        "        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=expand_size, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(expand_size)\n",
        "        self.nolinear2 = nolinear\n",
        "        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_size)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_size != out_size:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_size),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.nolinear1(self.bn1(self.conv1(x)))\n",
        "        out = self.nolinear2(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        if self.se != None:\n",
        "            out = self.se(out)\n",
        "        out = out + self.shortcut(x) if self.stride==1 else out\n",
        "        return out\n",
        "\n",
        "\n",
        "class MobileNetV3_Large(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(MobileNetV3_Large, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.hs1 = hswish()\n",
        "\n",
        "        self.bneck = nn.Sequential(\n",
        "            Block(3, 16, 16, 16, nn.ReLU(inplace=True), None, 1),\n",
        "            Block(3, 16, 64, 24, nn.ReLU(inplace=True), None, 2),\n",
        "            Block(3, 24, 72, 24, nn.ReLU(inplace=True), None, 1),\n",
        "            Block(5, 24, 72, 40, nn.ReLU(inplace=True), SeModule(40), 2),\n",
        "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
        "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
        "            Block(3, 40, 240, 80, hswish(), None, 2),\n",
        "            Block(3, 80, 200, 80, hswish(), None, 1),\n",
        "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
        "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
        "            Block(3, 80, 480, 112, hswish(), SeModule(112), 1),\n",
        "            Block(3, 112, 672, 112, hswish(), SeModule(112), 1),\n",
        "            Block(5, 112, 672, 160, hswish(), SeModule(160), 1),\n",
        "            Block(5, 160, 672, 160, hswish(), SeModule(160), 2),\n",
        "            Block(5, 160, 960, 160, hswish(), SeModule(160), 1),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.conv2 = nn.Conv2d(160, 960, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(960)\n",
        "        self.hs2 = hswish()\n",
        "        self.linear3 = nn.Linear(960, 1280)\n",
        "        self.bn3 = nn.BatchNorm1d(1280)\n",
        "        self.hs3 = hswish()\n",
        "        self.linear4 = nn.Linear(1280, num_classes)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.hs3(self.bn3(self.linear3(out)))\n",
        "        out = self.linear4(out)\n",
        "        return out\n",
        "    def extract(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MobileNetV3_Small(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(MobileNetV3_Small, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.hs1 = hswish()\n",
        "\n",
        "        self.bneck = nn.Sequential(\n",
        "            Block(3, 16, 16, 16, nn.ReLU(inplace=True), SeModule(16), 2),\n",
        "            Block(3, 16, 72, 24, nn.ReLU(inplace=True), None, 2),\n",
        "            Block(3, 24, 88, 24, nn.ReLU(inplace=True), None, 1),\n",
        "            Block(5, 24, 96, 40, hswish(), SeModule(40), 2),\n",
        "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
        "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
        "            Block(5, 40, 120, 48, hswish(), SeModule(48), 1),\n",
        "            Block(5, 48, 144, 48, hswish(), SeModule(48), 1),\n",
        "            Block(5, 48, 288, 96, hswish(), SeModule(96), 2),\n",
        "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
        "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(576)\n",
        "        self.hs2 = hswish()\n",
        "        self.linear3 = nn.Linear(576, 1280)\n",
        "        self.bn3 = nn.BatchNorm1d(1280)\n",
        "        self.hs3 = hswish()\n",
        "        self.linear4 = nn.Linear(1280, num_classes)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.hs3(self.bn3(self.linear3(out)))\n",
        "        out = self.linear4(out)\n",
        "        return out\n",
        "        \n",
        "    def extract(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = MobileNetV3_Small()\n",
        "    x = torch.randn(2,3,224,224)\n",
        "    y = net(x)\n",
        "    print(y.size())"
      ],
      "metadata": {
        "id": "G7JoD2RS76n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model test"
      ],
      "metadata": {
        "id": "Z3jQ0Hnd8BmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "gBQGUbD47uqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "efficient = '/content/drive/MyDrive/COSRMAL_CHALLENGE/audios/efficient/XL-97.14.pth'\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "feature = effnetv2_xl()\n",
        "feature.load_state_dict(torch.load(efficient))\n",
        "model = CNN_LSTM(feature).to(device)\n",
        "summary(model, input_size=(1, 2, 8, 64, 64))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I3skps84W5b",
        "outputId": "08b148ee-4895-4b71-fa62-19f9d1927143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=========================================================================================================\n",
              "Layer (type:depth-idx)                                  Output Shape              Param #\n",
              "=========================================================================================================\n",
              "CNN_LSTM                                                --                        --\n",
              "├─EffNetV2: 1                                           --                        --\n",
              "│    └─Sequential: 2-1                                  [1, 640, 2, 2]            --\n",
              "│    │    └─Sequential: 3-1                             [1, 24, 32, 32]           1,776\n",
              "│    │    └─MBConv: 3-2                                 [1, 32, 32, 32]           6,064\n",
              "│    │    └─MBConv: 3-3                                 [1, 32, 32, 32]           10,368\n",
              "│    │    └─MBConv: 3-4                                 [1, 32, 32, 32]           10,368\n",
              "│    │    └─MBConv: 3-5                                 [1, 32, 32, 32]           10,368\n",
              "│    │    └─MBConv: 3-6                                 [1, 64, 16, 16]           45,440\n",
              "│    │    └─MBConv: 3-7                                 [1, 64, 16, 16]           164,480\n",
              "│    │    └─MBConv: 3-8                                 [1, 64, 16, 16]           164,480\n",
              "│    │    └─MBConv: 3-9                                 [1, 64, 16, 16]           164,480\n",
              "│    │    └─MBConv: 3-10                                [1, 64, 16, 16]           164,480\n",
              "│    │    └─MBConv: 3-11                                [1, 64, 16, 16]           164,480\n",
              "│    │    └─MBConv: 3-12                                [1, 64, 16, 16]           164,480\n",
              "│    │    └─MBConv: 3-13                                [1, 64, 16, 16]           164,480\n",
              "│    │    └─MBConv: 3-14                                [1, 96, 8, 8]             172,736\n",
              "│    │    └─MBConv: 3-15                                [1, 96, 8, 8]             369,600\n",
              "│    │    └─MBConv: 3-16                                [1, 96, 8, 8]             369,600\n",
              "│    │    └─MBConv: 3-17                                [1, 96, 8, 8]             369,600\n",
              "│    │    └─MBConv: 3-18                                [1, 96, 8, 8]             369,600\n",
              "│    │    └─MBConv: 3-19                                [1, 96, 8, 8]             369,600\n",
              "│    │    └─MBConv: 3-20                                [1, 96, 8, 8]             369,600\n",
              "│    │    └─MBConv: 3-21                                [1, 96, 8, 8]             369,600\n",
              "│    │    └─MBConv: 3-22                                [1, 192, 4, 4]            134,808\n",
              "│    │    └─MBConv: 3-23                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-24                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-25                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-26                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-27                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-28                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-29                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-30                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-31                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-32                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-33                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-34                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-35                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-36                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-37                                [1, 192, 4, 4]            379,824\n",
              "│    │    └─MBConv: 3-38                                [1, 256, 4, 4]            643,376\n",
              "│    │    └─MBConv: 3-39                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-40                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-41                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-42                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-43                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-44                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-45                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-46                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-47                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-48                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-49                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-50                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-51                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-52                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-53                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-54                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-55                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-56                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-57                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-58                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-59                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-60                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-61                                [1, 256, 4, 4]            1,005,120\n",
              "│    │    └─MBConv: 3-62                                [1, 512, 2, 2]            1,398,848\n",
              "│    │    └─MBConv: 3-63                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-64                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-65                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-66                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-67                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-68                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-69                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-70                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-71                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-72                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-73                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-74                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-75                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-76                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-77                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-78                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-79                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-80                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-81                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-82                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-83                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-84                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-85                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-86                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-87                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-88                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-89                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-90                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-91                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-92                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-93                                [1, 512, 2, 2]            3,976,320\n",
              "│    │    └─MBConv: 3-94                                [1, 640, 2, 2]            4,369,792\n",
              "│    │    └─MBConv: 3-95                                [1, 640, 2, 2]            6,199,200\n",
              "│    │    └─MBConv: 3-96                                [1, 640, 2, 2]            6,199,200\n",
              "│    │    └─MBConv: 3-97                                [1, 640, 2, 2]            6,199,200\n",
              "│    │    └─MBConv: 3-98                                [1, 640, 2, 2]            6,199,200\n",
              "│    │    └─MBConv: 3-99                                [1, 640, 2, 2]            6,199,200\n",
              "│    │    └─MBConv: 3-100                               [1, 640, 2, 2]            6,199,200\n",
              "│    │    └─MBConv: 3-101                               [1, 640, 2, 2]            6,199,200\n",
              "│    └─Sequential: 2-2                                  [1, 1792, 2, 2]           --\n",
              "│    │    └─Conv2d: 3-102                               [1, 1792, 2, 2]           1,146,880\n",
              "│    │    └─BatchNorm2d: 3-103                          [1, 1792, 2, 2]           3,584\n",
              "│    │    └─SiLU: 3-104                                 [1, 1792, 2, 2]           --\n",
              "│    └─AdaptiveAvgPool2d: 2-3                           [1, 1792, 1, 1]           --\n",
              "├─Linear: 1-1                                           [1, 512]                  918,016\n",
              "├─LSTM: 1-2                                             [1, 1, 512]               4,730,880\n",
              "├─EffNetV2: 1                                           --                        --\n",
              "│    └─Sequential: 2-4                                  [1, 640, 2, 2]            (recursive)\n",
              "│    │    └─Sequential: 3-105                           [1, 24, 32, 32]           (recursive)\n",
              "│    │    └─MBConv: 3-106                               [1, 32, 32, 32]           (recursive)\n",
              "│    │    └─MBConv: 3-107                               [1, 32, 32, 32]           (recursive)\n",
              "│    │    └─MBConv: 3-108                               [1, 32, 32, 32]           (recursive)\n",
              "│    │    └─MBConv: 3-109                               [1, 32, 32, 32]           (recursive)\n",
              "│    │    └─MBConv: 3-110                               [1, 64, 16, 16]           (recursive)\n",
              "│    │    └─MBConv: 3-111                               [1, 64, 16, 16]           (recursive)\n",
              "│    │    └─MBConv: 3-112                               [1, 64, 16, 16]           (recursive)\n",
              "│    │    └─MBConv: 3-113                               [1, 64, 16, 16]           (recursive)\n",
              "│    │    └─MBConv: 3-114                               [1, 64, 16, 16]           (recursive)\n",
              "│    │    └─MBConv: 3-115                               [1, 64, 16, 16]           (recursive)\n",
              "│    │    └─MBConv: 3-116                               [1, 64, 16, 16]           (recursive)\n",
              "│    │    └─MBConv: 3-117                               [1, 64, 16, 16]           (recursive)\n",
              "│    │    └─MBConv: 3-118                               [1, 96, 8, 8]             (recursive)\n",
              "│    │    └─MBConv: 3-119                               [1, 96, 8, 8]             (recursive)\n",
              "│    │    └─MBConv: 3-120                               [1, 96, 8, 8]             (recursive)\n",
              "│    │    └─MBConv: 3-121                               [1, 96, 8, 8]             (recursive)\n",
              "│    │    └─MBConv: 3-122                               [1, 96, 8, 8]             (recursive)\n",
              "│    │    └─MBConv: 3-123                               [1, 96, 8, 8]             (recursive)\n",
              "│    │    └─MBConv: 3-124                               [1, 96, 8, 8]             (recursive)\n",
              "│    │    └─MBConv: 3-125                               [1, 96, 8, 8]             (recursive)\n",
              "│    │    └─MBConv: 3-126                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-127                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-128                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-129                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-130                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-131                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-132                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-133                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-134                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-135                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-136                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-137                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-138                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-139                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-140                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-141                               [1, 192, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-142                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-143                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-144                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-145                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-146                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-147                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-148                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-149                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-150                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-151                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-152                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-153                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-154                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-155                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-156                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-157                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-158                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-159                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-160                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-161                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-162                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-163                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-164                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-165                               [1, 256, 4, 4]            (recursive)\n",
              "│    │    └─MBConv: 3-166                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-167                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-168                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-169                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-170                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-171                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-172                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-173                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-174                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-175                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-176                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-177                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-178                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-179                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-180                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-181                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-182                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-183                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-184                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-185                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-186                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-187                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-188                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-189                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-190                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-191                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-192                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-193                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-194                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-195                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-196                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-197                               [1, 512, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-198                               [1, 640, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-199                               [1, 640, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-200                               [1, 640, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-201                               [1, 640, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-202                               [1, 640, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-203                               [1, 640, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-204                               [1, 640, 2, 2]            (recursive)\n",
              "│    │    └─MBConv: 3-205                               [1, 640, 2, 2]            (recursive)\n",
              "│    └─Sequential: 2-5                                  [1, 1792, 2, 2]           (recursive)\n",
              "│    │    └─Conv2d: 3-206                               [1, 1792, 2, 2]           (recursive)\n",
              "│    │    └─BatchNorm2d: 3-207                          [1, 1792, 2, 2]           (recursive)\n",
              "│    │    └─SiLU: 3-208                                 [1, 1792, 2, 2]           --\n",
              "│    └─AdaptiveAvgPool2d: 2-6                           [1, 1792, 1, 1]           --\n",
              "├─Linear: 1-3                                           [1, 512]                  (recursive)\n",
              "├─LSTM: 1-4                                             [1, 1, 512]               (recursive)\n",
              "├─Linear: 1-5                                           [1, 128]                  65,664\n",
              "├─Linear: 1-6                                           [1, 3]                    387\n",
              "=========================================================================================================\n",
              "Total params: 212,883,355\n",
              "Trainable params: 212,883,355\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 3.01\n",
              "=========================================================================================================\n",
              "Input size (MB): 0.26\n",
              "Forward/backward pass size (MB): 65.12\n",
              "Params size (MB): 851.53\n",
              "Estimated Total Size (MB): 916.92\n",
              "========================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion = nn.CrossEntropyLoss()):\n",
        "  model.train()\n",
        "  loss_train = 0.0\n",
        "  correct_train = 0.0\n",
        "  num_train = len(train_loader)\n",
        "  for batch_idx, (audio, target) in enumerate(train_loader):\n",
        "    audio = audio.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(audio)\n",
        "\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_train += loss.item() / num_train\n",
        "    _, preds=torch.max(outputs,1)\n",
        "    correct_train+=torch.sum(preds==target).item()\n",
        "  \n",
        "  return loss_train, correct_train\n",
        "\n",
        "\n",
        "def evaluate(model, testloader, criterion = nn.CrossEntropyLoss()):\n",
        "  model.eval()\n",
        "  loss_test = 0\n",
        "  correct_test=0\n",
        "  num_val = len(testloader)\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (audio, target) in enumerate(testloader):\n",
        "      audio = audio.to(device)\n",
        "      target = target.to(device)\n",
        "      outputs = model.forward(audio)\n",
        "      loss = criterion(outputs, target)\n",
        "      loss_test += loss.item() / num_val\n",
        "      _, preds=torch.max(outputs,1)\n",
        "      correct_test+=torch.sum(preds==target).item()\n",
        "  \n",
        "  return loss_test, correct_test\n"
      ],
      "metadata": {
        "id": "xSW_s5Ox2ptm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 2\n",
        "train_split = 0.8\n",
        "lr = 1e-5\n",
        "epochs = 200\n",
        "mydataset_t1 = audioDataSet_t1(base_path)\n",
        "n_samples = len(mydataset_t1)\n",
        "assert n_samples == 684, \"684\"\n",
        "feature_ = MobileNetV3_Large()\n",
        "\n",
        "model = CNN_LSTM(feature_).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset_t1, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch {}/{} train loss:{:.4f} train acc:{:.2f}% \".format(epoch+1,epochs, loss_train, 100 * correct_train/num_train))\n",
        "  print(\"Epoch {}/{} val loss:{:.4f} val acc:{:.2f}% \".format(epoch+1,epochs, loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  if loss_val < best_loss:\n",
        "    best_loss = loss_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_loss.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_val.pth\"))\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tRbf8S5o1oBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aKc2SC2q2EEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task1 filling level audio\n",
        "\n"
      ],
      "metadata": {
        "id": "YVJtnrAkZLQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "efficient = '/content/drive/MyDrive/COSRMAL_CHALLENGE/audios/efficient/XL-97.14.pth'\n",
        "base_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE/'\n",
        "audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio'\n",
        "T2_mid_dir = os.path.join(base_path, 'T2_mid')\n",
        "T2_pred_dir = os.path.join(base_path, 'T2_pred')\n",
        "os.makedirs(T2_mid_dir,exist_ok=True)\n",
        "os.makedirs(T2_pred_dir,exist_ok=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "model = effnetv2_xl()\n",
        "model.load_state_dict(torch.load(efficient))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "audio_paths = [os.path.join(audio_folder, path) for path in sorted(os.listdir(audio_folder))]\n",
        "save_size=64\n",
        "ratio_step = 0.25\n",
        "count = 0\n",
        "MAX_VALUE=194.19187653405487\n",
        "MIN_VALUE=-313.07119549054045\n",
        "\n",
        "# pbar = tqdm(total=len(audio_paths))\n",
        "\n",
        "# for i, path in enumerate(audio_paths):\n",
        "#   id = i\n",
        "#   start_time = gt[gt.id==id]['start'].item()\n",
        "#   end_time = gt[gt.id==id]['end'].item()\n",
        "#   filling_type = gt[gt.id==id]['filling_type'].item()\n",
        "#   datalist = []\n",
        "#   predlist = []\n",
        "#   sample_rate, signal = scipy.io.wavfile.read(path)\n",
        "#   ap = AudioProcessing(sample_rate,signal,nfilt=save_size)\n",
        "#   mfcc = ap.calc_MFCC()\n",
        "#   mfcc_length=mfcc.shape[0]\n",
        "\n",
        "#   if mfcc_length < save_size:\n",
        "#     print(\"file {} is too short\".format(id))\n",
        "#   else:\n",
        "#     f_step=int(mfcc.shape[1]*ratio_step)\n",
        "#     f_length=mfcc.shape[1]\n",
        "#     save_mfcc_num=int(np.ceil(float(np.abs(mfcc_length - save_size)) / f_step))\n",
        "\n",
        "#     for i in range(save_mfcc_num):\n",
        "#       tmp_mfcc = mfcc[i*f_step:save_size+i*f_step,: ,:]\n",
        "#       tmp_mfcc= (tmp_mfcc-MIN_VALUE)/(MAX_VALUE-MIN_VALUE)\n",
        "#       tmp_mfcc=tmp_mfcc.transpose(2,0,1)\n",
        "#       audio=torch.from_numpy(tmp_mfcc.astype(np.float32))\n",
        "#       audio=torch.unsqueeze(audio, 0)\n",
        "#       audio = audio.to(device) \n",
        "#       feature, pred=model.extract(audio)\n",
        "#       _,pred=torch.max(pred,1)\n",
        "#       datalist.append(feature.to('cpu').detach().numpy().copy())\n",
        "#       predlist.append(pred.item())\n",
        "#     datalist = np.squeeze(np.array(datalist))\n",
        "#     predlist = np.squeeze(np.array(predlist))\n",
        "#     np.save(os.path.join(T2_mid_dir, \"{0:06d}\".format(id)), datalist)\n",
        "#     np.save(os.path.join(T2_pred_dir, \"{0:06d}\".format(id)), predlist)\n",
        "\n",
        "#   pbar.update()\n",
        "\n"
      ],
      "metadata": {
        "id": "rQysZ884ZNMv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "6fd45eda7c604273b0e6d65f9563a8cb",
            "31a36b5780284e6aabd44a29292af12b",
            "a1bd32c213744acd999c6bf89827a93a",
            "59a214a8b28041fe8e241df30384757c",
            "580d86db47dd41e9a620cb241c90a9b9",
            "14836bf44b6a4fd4863a1e0b1ac735dd",
            "f5cc8a24e80c4b8cbe1fc51a4019a3ae",
            "3fe752d058104c338b03c55b27d8d35e",
            "c3ba7d5d3d194156b402b316411888ae",
            "ff5f13deb3df40e39470885b38bb2106",
            "965209a96f2e4b61990a02e516f47b71"
          ]
        },
        "outputId": "8bdf9a32-9bcf-4002-893d-f3bd21f67cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fd45eda7c604273b0e6d65f9563a8cb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Padding(object):\n",
        "    def __init__(self, seq_len):\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __call__(self, sample, pred):\n",
        "        #np.clip(pred, 0,1,out=pred)\n",
        "        sample_len, input_dim = sample.shape\n",
        "        #for i in range(sample_len):\n",
        "        #    sample[i, :] *= pred[i]\n",
        "\n",
        "        if sample_len >= self.seq_len:\n",
        "            features = sample[:self.seq_len, :]\n",
        "            return features\n",
        "        else:\n",
        "            start_seq = np.random.randint(0, self.seq_len - sample_len+1)\n",
        "            #ini=[1]+[0]*(input_dim-1)\n",
        "            ini=[0]*(input_dim)\n",
        "            features = np.full((self.seq_len, input_dim),ini, dtype = float)\n",
        "            features[start_seq:start_seq+sample_len, :] = sample\n",
        "            return features\n",
        "\n",
        "class MyLSTMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,root_pth,test=False,transform = None, padding_size = 100):\n",
        "        class_num=3\n",
        "        self.mid_pth = os.path.join(root_pth, 'T2_mid')\n",
        "        self.pred_pth = os.path.join(root_pth, 'T2_pred')\n",
        "        self.label = gt['filling_level'].to_numpy()\n",
        "        self.is_test=test\n",
        "        self.each_class_size = []\n",
        "        self.each_class_sum = [0]*class_num\n",
        "        for i in range(class_num):\n",
        "            self.each_class_size.append(np.count_nonzero(self.label==i))\n",
        "        mx=0\n",
        "        mn=1000\n",
        "        len_mx = 0\n",
        "        \n",
        "        for idx in range(self.label.shape[0]):\n",
        "            data=np.load(os.path.join(self.mid_pth, \"{0:06d}\".format(idx) + '.npy'), allow_pickle=True)\n",
        "            self.each_class_sum[self.label[idx]]+=data.shape[0]\n",
        "            if data.shape[0] > len_mx:\n",
        "                len_mx=data.shape[0]\n",
        "            tmp_max=np.max(data)\n",
        "            tmp_min=np.min(data)\n",
        "            if mx<tmp_max:\n",
        "                mx=tmp_max\n",
        "            if mn>tmp_min:\n",
        "                mn=tmp_min\n",
        "        self.mn=mn\n",
        "        self.mx=mx\n",
        "        self.pad = Padding(padding_size)\n",
        "        print(len_mx)\n",
        "            \n",
        "    def __len__(self):\n",
        "        return self.label.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        lbl = -1\n",
        "\n",
        "        if self.is_test is False:\n",
        "            lbl = self.label[idx]\n",
        "            \n",
        "        data=np.load(os.path.join(self.mid_pth, \"{0:06d}\".format(idx) + '.npy'), allow_pickle=True)\n",
        "        pred=np.load(os.path.join(self.pred_pth, \"{0:06d}\".format(idx) + '.npy'), allow_pickle=True)\n",
        "        data = (data-self.mn)/(self.mx-self.mn)\n",
        "        data = self.pad(data, pred)\n",
        "\n",
        "        #np.clip(data, 0,1,out=data)\n",
        "        data=torch.from_numpy(data.astype(np.float32))\n",
        "        return data , lbl\n",
        "            \n",
        "    def get_each_class_size(self):\n",
        "        return np.array(self.each_class_size)\n",
        "\n",
        "    def get_each_class_avg_len(self):\n",
        "        each_class_avg_len =  np.array(self.each_class_sum)/np.array(self.each_class_size)\n",
        "        all_class_avg_len = np.sum(np.array(self.each_class_sum))/np.sum(np.array(self.each_class_size))\n",
        "        return each_class_avg_len, all_class_avg_len"
      ],
      "metadata": {
        "id": "TOJcXwgou87v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE'\n",
        "myDataSet = MyLSTMDataset(base_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzuXtMEhvbQp",
        "outputId": "7df28cd8-caae-49f8-ebf5-2859a8e29cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN_LSTM, self).__init__()\n",
        "    #self.proj = nn.Linear(1792, 512)\n",
        "    self.lstm = nn.LSTM(input_size=1792, hidden_size=256, num_layers=3, dropout=0.15, bidirectional=True)\n",
        "    self.fc1 = nn.Linear(256, 128)\n",
        "    self.fc2 = nn.Linear(128, 3)\n",
        "  \n",
        "  def attnetwork(self, encoder_out, final_hidden):\n",
        "    hidden = final_hidden.squeeze(0)\n",
        "    #M = torch.tanh(encoder_out)\n",
        "    attn_weights = torch.bmm(encoder_out, hidden.unsqueeze(2)).squeeze(2)\n",
        "    soft_attn_weights = F.softmax(attn_weights, 1)\n",
        "    new_hidden = torch.bmm(encoder_out.transpose(1,2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "    #print (wt.shape, new_hidden.shape)\n",
        "    #new_hidden = torch.tanh(new_hidden)\n",
        "    #print ('UP:', new_hidden, new_hidden.shape)\n",
        "        \n",
        "    return new_hidden\n",
        "  def forward(self, x_3d):\n",
        "    # bs, t, c, h, w\n",
        "    hidden = None\n",
        "    for t in range(x_3d.size(1)):\n",
        "      #x = self.proj(x_3d[:, t])\n",
        "      output, hidden = self.lstm(x_3d[:, t].unsqueeze(0), hidden) \n",
        "    \n",
        "\n",
        "    (hn, cn) = hidden\n",
        "    fbout = output[:, :, :256]+ output[:, :, 256:] #sum bidir outputs F+B\n",
        "    fbout = fbout.permute(1,0,2)\n",
        "    fbhn = (hn[-2,:,:]+hn[-1,:,:]).unsqueeze(0)\n",
        "    #print (fbhn.shape, fbout.shape)\n",
        "    attn_out = self.attnetwork(fbout, fbhn)\n",
        "\n",
        "    #x = self.fc1(out[-1, :, :])\n",
        "    x = self.fc1(attn_out)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Al01NaUsvtn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion = nn.CrossEntropyLoss()):\n",
        "  model.train()\n",
        "  loss_train = 0.0\n",
        "  correct_train = 0.0\n",
        "  num_train = len(train_loader)\n",
        "  for batch_idx, (audio, target) in enumerate(train_loader):\n",
        "    audio = audio.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(audio)\n",
        "\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_train += loss.item() / num_train\n",
        "    _, preds=torch.max(outputs,1)\n",
        "    correct_train+=torch.sum(preds==target).item()\n",
        "  \n",
        "  return loss_train, correct_train\n",
        "\n",
        "\n",
        "def evaluate(model, testloader, criterion = nn.CrossEntropyLoss()):\n",
        "  model.eval()\n",
        "  loss_test = 0\n",
        "  correct_test=0\n",
        "  num_val = len(testloader)\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (audio, target) in enumerate(testloader):\n",
        "      audio = audio.to(device)\n",
        "      target = target.to(device)\n",
        "      outputs = model.forward(audio)\n",
        "      loss = criterion(outputs, target)\n",
        "      loss_test += loss.item() / num_val\n",
        "      _, preds=torch.max(outputs,1)\n",
        "      correct_test+=torch.sum(preds==target).item()\n",
        "  \n",
        "  return loss_test, correct_test\n"
      ],
      "metadata": {
        "id": "SbSFO1DLw-PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 16\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "n_samples = len(myDataSet)\n",
        "assert n_samples == 684, \"684\"\n",
        "\n",
        "model = CNN_LSTM().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = 584\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(myDataSet, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=False)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch {}/{} train loss:{:.4f} train acc:{:.2f}% \".format(epoch+1,epochs, loss_train, 100 * correct_train/num_train))\n",
        "  print(\"Epoch {}/{} val loss:{:.4f} val acc:{:.2f}% \".format(epoch+1,epochs, loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  # if loss_val < best_loss:\n",
        "  #   best_loss = loss_val\n",
        "  #   torch.save(model, os.path.join(base_path, 'audios', \"best_loss.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    best_train = correct_train\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_lstm.pth\"))\n",
        "  \n",
        "  if correct_val == best_acc and best_train < correct_train:\n",
        "    best_acc = correct_val\n",
        "    best_train = correct_train\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_lstm.pth\"))\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejbKNrd_wi2T",
        "outputId": "b8e1ff1c-305d-4db7-ba03-4095a0506f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200 train loss:1.0452 train acc:46.40% \n",
            "Epoch 1/200 val loss:0.9191 val acc:41.00% \n",
            "Epoch 2/200 train loss:0.9919 train acc:50.00% \n",
            "Epoch 2/200 val loss:0.9260 val acc:61.00% \n",
            "Epoch 3/200 train loss:0.9881 train acc:49.14% \n",
            "Epoch 3/200 val loss:0.8794 val acc:58.00% \n",
            "Epoch 4/200 train loss:0.9932 train acc:51.54% \n",
            "Epoch 4/200 val loss:0.8525 val acc:66.00% \n",
            "Epoch 5/200 train loss:0.9659 train acc:49.66% \n",
            "Epoch 5/200 val loss:0.8356 val acc:60.00% \n",
            "Epoch 6/200 train loss:0.8339 train acc:46.92% \n",
            "Epoch 6/200 val loss:0.7376 val acc:67.00% \n",
            "Epoch 7/200 train loss:0.7594 train acc:64.21% \n",
            "Epoch 7/200 val loss:0.6884 val acc:63.00% \n",
            "Epoch 8/200 train loss:0.6278 train acc:68.32% \n",
            "Epoch 8/200 val loss:0.5749 val acc:76.00% \n",
            "Epoch 9/200 train loss:0.6022 train acc:63.36% \n",
            "Epoch 9/200 val loss:0.5800 val acc:73.00% \n",
            "Epoch 10/200 train loss:0.5727 train acc:64.38% \n",
            "Epoch 10/200 val loss:0.5843 val acc:73.00% \n",
            "Epoch 11/200 train loss:0.5532 train acc:64.73% \n",
            "Epoch 11/200 val loss:0.5212 val acc:79.00% \n",
            "Epoch 12/200 train loss:0.5567 train acc:68.15% \n",
            "Epoch 12/200 val loss:0.5119 val acc:77.00% \n",
            "Epoch 13/200 train loss:0.6193 train acc:69.52% \n",
            "Epoch 13/200 val loss:0.5090 val acc:73.00% \n",
            "Epoch 14/200 train loss:0.5685 train acc:67.12% \n",
            "Epoch 14/200 val loss:0.4821 val acc:82.00% \n",
            "Epoch 15/200 train loss:0.5788 train acc:69.01% \n",
            "Epoch 15/200 val loss:0.4596 val acc:82.00% \n",
            "Epoch 16/200 train loss:0.5301 train acc:72.77% \n",
            "Epoch 16/200 val loss:0.4868 val acc:80.00% \n",
            "Epoch 17/200 train loss:0.5325 train acc:71.40% \n",
            "Epoch 17/200 val loss:0.5664 val acc:76.00% \n",
            "Epoch 18/200 train loss:0.7241 train acc:66.95% \n",
            "Epoch 18/200 val loss:0.4606 val acc:83.00% \n",
            "Epoch 19/200 train loss:0.5301 train acc:74.14% \n",
            "Epoch 19/200 val loss:0.5904 val acc:71.00% \n",
            "Epoch 20/200 train loss:0.5568 train acc:67.98% \n",
            "Epoch 20/200 val loss:0.4613 val acc:77.00% \n",
            "Epoch 21/200 train loss:0.5063 train acc:73.29% \n",
            "Epoch 21/200 val loss:0.4319 val acc:84.00% \n",
            "Epoch 22/200 train loss:0.4951 train acc:74.66% \n",
            "Epoch 22/200 val loss:0.4204 val acc:83.00% \n",
            "Epoch 23/200 train loss:0.5233 train acc:72.43% \n",
            "Epoch 23/200 val loss:0.4530 val acc:82.00% \n",
            "Epoch 24/200 train loss:0.5259 train acc:72.09% \n",
            "Epoch 24/200 val loss:0.4015 val acc:85.00% \n",
            "Epoch 25/200 train loss:0.4880 train acc:75.51% \n",
            "Epoch 25/200 val loss:0.4143 val acc:81.00% \n",
            "Epoch 26/200 train loss:0.5055 train acc:74.66% \n",
            "Epoch 26/200 val loss:0.4188 val acc:82.00% \n",
            "Epoch 27/200 train loss:0.5026 train acc:73.29% \n",
            "Epoch 27/200 val loss:0.4743 val acc:78.00% \n",
            "Epoch 28/200 train loss:0.4919 train acc:73.80% \n",
            "Epoch 28/200 val loss:0.4223 val acc:81.00% \n",
            "Epoch 29/200 train loss:0.4849 train acc:75.68% \n",
            "Epoch 29/200 val loss:0.4181 val acc:81.00% \n",
            "Epoch 30/200 train loss:0.4740 train acc:77.05% \n",
            "Epoch 30/200 val loss:0.3862 val acc:84.00% \n",
            "Epoch 31/200 train loss:0.4718 train acc:78.42% \n",
            "Epoch 31/200 val loss:0.5214 val acc:78.00% \n",
            "Epoch 32/200 train loss:0.4738 train acc:76.03% \n",
            "Epoch 32/200 val loss:0.4062 val acc:83.00% \n",
            "Epoch 33/200 train loss:0.5294 train acc:69.86% \n",
            "Epoch 33/200 val loss:0.5364 val acc:77.00% \n",
            "Epoch 34/200 train loss:0.5266 train acc:67.47% \n",
            "Epoch 34/200 val loss:0.4965 val acc:82.00% \n",
            "Epoch 35/200 train loss:0.5080 train acc:75.00% \n",
            "Epoch 35/200 val loss:0.4071 val acc:83.00% \n",
            "Epoch 36/200 train loss:0.4882 train acc:73.97% \n",
            "Epoch 36/200 val loss:0.4265 val acc:82.00% \n",
            "Epoch 37/200 train loss:0.4802 train acc:76.37% \n",
            "Epoch 37/200 val loss:0.3897 val acc:83.00% \n",
            "Epoch 38/200 train loss:0.4808 train acc:76.54% \n",
            "Epoch 38/200 val loss:0.4083 val acc:85.00% \n",
            "Epoch 39/200 train loss:0.4715 train acc:77.91% \n",
            "Epoch 39/200 val loss:0.4095 val acc:83.00% \n",
            "Epoch 40/200 train loss:0.4924 train acc:75.86% \n",
            "Epoch 40/200 val loss:0.4475 val acc:84.00% \n",
            "Epoch 41/200 train loss:0.4766 train acc:77.74% \n",
            "Epoch 41/200 val loss:0.4118 val acc:84.00% \n",
            "Epoch 42/200 train loss:0.4658 train acc:76.20% \n",
            "Epoch 42/200 val loss:0.3890 val acc:84.00% \n",
            "Epoch 43/200 train loss:0.4723 train acc:77.05% \n",
            "Epoch 43/200 val loss:0.4217 val acc:83.00% \n",
            "Epoch 44/200 train loss:0.4911 train acc:75.51% \n",
            "Epoch 44/200 val loss:0.4037 val acc:82.00% \n",
            "Epoch 45/200 train loss:0.5193 train acc:75.17% \n",
            "Epoch 45/200 val loss:0.4416 val acc:80.00% \n",
            "Epoch 46/200 train loss:0.4663 train acc:77.23% \n",
            "Epoch 46/200 val loss:0.3994 val acc:84.00% \n",
            "Epoch 47/200 train loss:0.4781 train acc:77.05% \n",
            "Epoch 47/200 val loss:0.4708 val acc:79.00% \n",
            "Epoch 48/200 train loss:0.4753 train acc:77.40% \n",
            "Epoch 48/200 val loss:0.3636 val acc:85.00% \n",
            "Epoch 49/200 train loss:0.4653 train acc:78.08% \n",
            "Epoch 49/200 val loss:0.4754 val acc:79.00% \n",
            "Epoch 50/200 train loss:0.4867 train acc:73.97% \n",
            "Epoch 50/200 val loss:0.3872 val acc:83.00% \n",
            "Epoch 51/200 train loss:0.4595 train acc:75.51% \n",
            "Epoch 51/200 val loss:0.4055 val acc:84.00% \n",
            "Epoch 52/200 train loss:0.5846 train acc:76.03% \n",
            "Epoch 52/200 val loss:0.3820 val acc:85.00% \n",
            "Epoch 53/200 train loss:0.4816 train acc:75.17% \n",
            "Epoch 53/200 val loss:0.3818 val acc:84.00% \n",
            "Epoch 54/200 train loss:0.4825 train acc:76.20% \n",
            "Epoch 54/200 val loss:0.3934 val acc:83.00% \n",
            "Epoch 55/200 train loss:0.4670 train acc:75.86% \n",
            "Epoch 55/200 val loss:0.3889 val acc:84.00% \n",
            "Epoch 56/200 train loss:0.4632 train acc:77.40% \n",
            "Epoch 56/200 val loss:0.3679 val acc:83.00% \n",
            "Epoch 57/200 train loss:0.4521 train acc:77.57% \n",
            "Epoch 57/200 val loss:0.3982 val acc:84.00% \n",
            "Epoch 58/200 train loss:0.4751 train acc:75.34% \n",
            "Epoch 58/200 val loss:0.4301 val acc:82.00% \n",
            "Epoch 59/200 train loss:0.4645 train acc:76.88% \n",
            "Epoch 59/200 val loss:0.4182 val acc:83.00% \n",
            "Epoch 60/200 train loss:0.4560 train acc:77.57% \n",
            "Epoch 60/200 val loss:0.4400 val acc:79.00% \n",
            "Epoch 61/200 train loss:0.4636 train acc:76.71% \n",
            "Epoch 61/200 val loss:0.4153 val acc:79.00% \n",
            "Epoch 62/200 train loss:0.4521 train acc:76.54% \n",
            "Epoch 62/200 val loss:0.3782 val acc:84.00% \n",
            "Epoch 63/200 train loss:0.4486 train acc:75.00% \n",
            "Epoch 63/200 val loss:0.4264 val acc:82.00% \n",
            "Epoch 64/200 train loss:0.4900 train acc:75.00% \n",
            "Epoch 64/200 val loss:0.4095 val acc:84.00% \n",
            "Epoch 65/200 train loss:0.4560 train acc:75.00% \n",
            "Epoch 65/200 val loss:0.4033 val acc:81.00% \n",
            "Epoch 66/200 train loss:0.4450 train acc:76.88% \n",
            "Epoch 66/200 val loss:0.5046 val acc:69.00% \n",
            "Epoch 67/200 train loss:0.4656 train acc:73.80% \n",
            "Epoch 67/200 val loss:0.4271 val acc:74.00% \n",
            "Epoch 68/200 train loss:0.4700 train acc:77.57% \n",
            "Epoch 68/200 val loss:0.4241 val acc:82.00% \n",
            "Epoch 69/200 train loss:0.4558 train acc:74.66% \n",
            "Epoch 69/200 val loss:0.4260 val acc:81.00% \n",
            "Epoch 70/200 train loss:0.4387 train acc:77.05% \n",
            "Epoch 70/200 val loss:0.3688 val acc:83.00% \n",
            "Epoch 71/200 train loss:0.4412 train acc:76.88% \n",
            "Epoch 71/200 val loss:0.3941 val acc:83.00% \n",
            "Epoch 72/200 train loss:0.4494 train acc:77.91% \n",
            "Epoch 72/200 val loss:0.3820 val acc:84.00% \n",
            "Epoch 73/200 train loss:0.4583 train acc:76.71% \n",
            "Epoch 73/200 val loss:0.4171 val acc:84.00% \n",
            "Epoch 74/200 train loss:0.4353 train acc:77.23% \n",
            "Epoch 74/200 val loss:0.4072 val acc:82.00% \n",
            "Epoch 75/200 train loss:0.4434 train acc:74.83% \n",
            "Epoch 75/200 val loss:0.3791 val acc:85.00% \n",
            "Epoch 76/200 train loss:0.4263 train acc:79.28% \n",
            "Epoch 76/200 val loss:0.3854 val acc:85.00% \n",
            "Epoch 77/200 train loss:0.4504 train acc:78.60% \n",
            "Epoch 77/200 val loss:0.3753 val acc:83.00% \n",
            "Epoch 78/200 train loss:0.4385 train acc:78.08% \n",
            "Epoch 78/200 val loss:0.3766 val acc:81.00% \n",
            "Epoch 79/200 train loss:0.4366 train acc:77.57% \n",
            "Epoch 79/200 val loss:0.4121 val acc:82.00% \n",
            "Epoch 80/200 train loss:0.4283 train acc:75.68% \n",
            "Epoch 80/200 val loss:0.4372 val acc:76.00% \n",
            "Epoch 81/200 train loss:0.4316 train acc:76.88% \n",
            "Epoch 81/200 val loss:0.4022 val acc:76.00% \n",
            "Epoch 82/200 train loss:0.4384 train acc:79.11% \n",
            "Epoch 82/200 val loss:0.3794 val acc:84.00% \n",
            "Epoch 83/200 train loss:0.4423 train acc:78.08% \n",
            "Epoch 83/200 val loss:0.4440 val acc:75.00% \n",
            "Epoch 84/200 train loss:0.4415 train acc:76.54% \n",
            "Epoch 84/200 val loss:0.4100 val acc:78.00% \n",
            "Epoch 85/200 train loss:0.4042 train acc:79.79% \n",
            "Epoch 85/200 val loss:0.3768 val acc:78.00% \n",
            "Epoch 86/200 train loss:0.4262 train acc:76.88% \n",
            "Epoch 86/200 val loss:0.4091 val acc:85.00% \n",
            "Epoch 87/200 train loss:0.4079 train acc:78.08% \n",
            "Epoch 87/200 val loss:0.3722 val acc:81.00% \n",
            "Epoch 88/200 train loss:0.4442 train acc:77.05% \n",
            "Epoch 88/200 val loss:0.3669 val acc:84.00% \n",
            "Epoch 89/200 train loss:0.4184 train acc:77.23% \n",
            "Epoch 89/200 val loss:0.4212 val acc:74.00% \n",
            "Epoch 90/200 train loss:0.4138 train acc:79.45% \n",
            "Epoch 90/200 val loss:0.3825 val acc:83.00% \n",
            "Epoch 91/200 train loss:0.4570 train acc:77.23% \n",
            "Epoch 91/200 val loss:0.4042 val acc:80.00% \n",
            "Epoch 92/200 train loss:0.4189 train acc:77.05% \n",
            "Epoch 92/200 val loss:0.4384 val acc:74.00% \n",
            "Epoch 93/200 train loss:0.4368 train acc:78.94% \n",
            "Epoch 93/200 val loss:0.3942 val acc:83.00% \n",
            "Epoch 94/200 train loss:0.4326 train acc:77.40% \n",
            "Epoch 94/200 val loss:0.4073 val acc:76.00% \n",
            "Epoch 95/200 train loss:0.4243 train acc:77.40% \n",
            "Epoch 95/200 val loss:0.4085 val acc:82.00% \n",
            "Epoch 96/200 train loss:0.4212 train acc:74.83% \n",
            "Epoch 96/200 val loss:0.3921 val acc:84.00% \n",
            "Epoch 97/200 train loss:0.4268 train acc:79.11% \n",
            "Epoch 97/200 val loss:0.4070 val acc:77.00% \n",
            "Epoch 98/200 train loss:0.4203 train acc:78.08% \n",
            "Epoch 98/200 val loss:0.3987 val acc:83.00% \n",
            "Epoch 99/200 train loss:0.4100 train acc:78.94% \n",
            "Epoch 99/200 val loss:0.3871 val acc:84.00% \n",
            "Epoch 100/200 train loss:0.4115 train acc:77.74% \n",
            "Epoch 100/200 val loss:0.3815 val acc:80.00% \n",
            "Epoch 101/200 train loss:0.4066 train acc:78.42% \n",
            "Epoch 101/200 val loss:0.3753 val acc:78.00% \n",
            "Epoch 102/200 train loss:0.4200 train acc:78.77% \n",
            "Epoch 102/200 val loss:0.4096 val acc:77.00% \n",
            "Epoch 103/200 train loss:0.4004 train acc:79.28% \n",
            "Epoch 103/200 val loss:0.4291 val acc:84.00% \n",
            "Epoch 104/200 train loss:0.4067 train acc:79.62% \n",
            "Epoch 104/200 val loss:0.4248 val acc:78.00% \n",
            "Epoch 105/200 train loss:1.0200 train acc:64.21% \n",
            "Epoch 105/200 val loss:0.6086 val acc:67.00% \n",
            "Epoch 106/200 train loss:0.4993 train acc:76.54% \n",
            "Epoch 106/200 val loss:0.4131 val acc:74.00% \n",
            "Epoch 107/200 train loss:0.4678 train acc:79.28% \n",
            "Epoch 107/200 val loss:0.4410 val acc:79.00% \n",
            "Epoch 108/200 train loss:0.4297 train acc:79.28% \n",
            "Epoch 108/200 val loss:0.3936 val acc:82.00% \n",
            "Epoch 109/200 train loss:0.4104 train acc:78.42% \n",
            "Epoch 109/200 val loss:0.4124 val acc:79.00% \n",
            "Epoch 110/200 train loss:0.4132 train acc:78.42% \n",
            "Epoch 110/200 val loss:0.4037 val acc:76.00% \n",
            "Epoch 111/200 train loss:0.4039 train acc:76.71% \n",
            "Epoch 111/200 val loss:0.4967 val acc:70.00% \n",
            "Epoch 112/200 train loss:0.4036 train acc:79.45% \n",
            "Epoch 112/200 val loss:0.4118 val acc:81.00% \n",
            "Epoch 113/200 train loss:0.3983 train acc:79.11% \n",
            "Epoch 113/200 val loss:0.4080 val acc:82.00% \n",
            "Epoch 114/200 train loss:0.4154 train acc:79.28% \n",
            "Epoch 114/200 val loss:0.4196 val acc:75.00% \n",
            "Epoch 115/200 train loss:0.4024 train acc:77.05% \n",
            "Epoch 115/200 val loss:0.4461 val acc:81.00% \n",
            "Epoch 116/200 train loss:0.4026 train acc:79.62% \n",
            "Epoch 116/200 val loss:0.4574 val acc:80.00% \n",
            "Epoch 117/200 train loss:0.4223 train acc:77.23% \n",
            "Epoch 117/200 val loss:0.4845 val acc:73.00% \n",
            "Epoch 118/200 train loss:0.4322 train acc:78.77% \n",
            "Epoch 118/200 val loss:0.4149 val acc:74.00% \n",
            "Epoch 119/200 train loss:0.3971 train acc:76.88% \n",
            "Epoch 119/200 val loss:0.3857 val acc:80.00% \n",
            "Epoch 120/200 train loss:0.3942 train acc:79.45% \n",
            "Epoch 120/200 val loss:0.4086 val acc:77.00% \n",
            "Epoch 121/200 train loss:0.4094 train acc:78.42% \n",
            "Epoch 121/200 val loss:0.4114 val acc:76.00% \n",
            "Epoch 122/200 train loss:0.4041 train acc:80.99% \n",
            "Epoch 122/200 val loss:0.3737 val acc:84.00% \n",
            "Epoch 123/200 train loss:0.3972 train acc:79.79% \n",
            "Epoch 123/200 val loss:0.4684 val acc:75.00% \n",
            "Epoch 124/200 train loss:0.4012 train acc:78.08% \n",
            "Epoch 124/200 val loss:0.3905 val acc:84.00% \n",
            "Epoch 125/200 train loss:0.3937 train acc:79.79% \n",
            "Epoch 125/200 val loss:0.3964 val acc:77.00% \n",
            "Epoch 126/200 train loss:0.3844 train acc:80.82% \n",
            "Epoch 126/200 val loss:0.4238 val acc:82.00% \n",
            "Epoch 127/200 train loss:0.3982 train acc:79.11% \n",
            "Epoch 127/200 val loss:0.4118 val acc:81.00% \n",
            "Epoch 128/200 train loss:0.3961 train acc:78.94% \n",
            "Epoch 128/200 val loss:0.4426 val acc:77.00% \n",
            "Epoch 129/200 train loss:0.3777 train acc:80.31% \n",
            "Epoch 129/200 val loss:0.4080 val acc:83.00% \n",
            "Epoch 130/200 train loss:0.3959 train acc:78.77% \n",
            "Epoch 130/200 val loss:0.4372 val acc:76.00% \n",
            "Epoch 131/200 train loss:0.4068 train acc:77.40% \n",
            "Epoch 131/200 val loss:0.5373 val acc:80.00% \n",
            "Epoch 132/200 train loss:0.3914 train acc:78.42% \n",
            "Epoch 132/200 val loss:0.4320 val acc:76.00% \n",
            "Epoch 133/200 train loss:0.4272 train acc:79.62% \n",
            "Epoch 133/200 val loss:0.4212 val acc:82.00% \n",
            "Epoch 134/200 train loss:0.3872 train acc:79.62% \n",
            "Epoch 134/200 val loss:0.4069 val acc:79.00% \n",
            "Epoch 135/200 train loss:0.3842 train acc:80.14% \n",
            "Epoch 135/200 val loss:0.4498 val acc:75.00% \n",
            "Epoch 136/200 train loss:0.3776 train acc:79.28% \n",
            "Epoch 136/200 val loss:0.4412 val acc:75.00% \n",
            "Epoch 137/200 train loss:0.3673 train acc:79.62% \n",
            "Epoch 137/200 val loss:0.4530 val acc:79.00% \n",
            "Epoch 138/200 train loss:0.3916 train acc:80.65% \n",
            "Epoch 138/200 val loss:0.4307 val acc:73.00% \n",
            "Epoch 139/200 train loss:0.4117 train acc:78.08% \n",
            "Epoch 139/200 val loss:0.4063 val acc:81.00% \n",
            "Epoch 140/200 train loss:0.3847 train acc:78.77% \n",
            "Epoch 140/200 val loss:0.4344 val acc:77.00% \n",
            "Epoch 141/200 train loss:0.3903 train acc:79.62% \n",
            "Epoch 141/200 val loss:0.4077 val acc:80.00% \n",
            "Epoch 142/200 train loss:0.3777 train acc:80.82% \n",
            "Epoch 142/200 val loss:0.4321 val acc:79.00% \n",
            "Epoch 143/200 train loss:0.3832 train acc:79.97% \n",
            "Epoch 143/200 val loss:0.4237 val acc:80.00% \n",
            "Epoch 144/200 train loss:0.3782 train acc:81.68% \n",
            "Epoch 144/200 val loss:0.4225 val acc:78.00% \n",
            "Epoch 145/200 train loss:0.3872 train acc:80.82% \n",
            "Epoch 145/200 val loss:0.4553 val acc:77.00% \n",
            "Epoch 146/200 train loss:0.4592 train acc:77.74% \n",
            "Epoch 146/200 val loss:0.4181 val acc:76.00% \n",
            "Epoch 147/200 train loss:0.4444 train acc:78.94% \n",
            "Epoch 147/200 val loss:0.4092 val acc:83.00% \n",
            "Epoch 148/200 train loss:0.4062 train acc:77.57% \n",
            "Epoch 148/200 val loss:0.5230 val acc:83.00% \n",
            "Epoch 149/200 train loss:0.4945 train acc:77.40% \n",
            "Epoch 149/200 val loss:0.4714 val acc:80.00% \n",
            "Epoch 150/200 train loss:0.3705 train acc:79.79% \n",
            "Epoch 150/200 val loss:0.4558 val acc:80.00% \n",
            "Epoch 151/200 train loss:0.3795 train acc:79.28% \n",
            "Epoch 151/200 val loss:0.4454 val acc:80.00% \n",
            "Epoch 152/200 train loss:0.3904 train acc:80.48% \n",
            "Epoch 152/200 val loss:0.4220 val acc:82.00% \n",
            "Epoch 153/200 train loss:0.3765 train acc:79.45% \n",
            "Epoch 153/200 val loss:0.4235 val acc:81.00% \n",
            "Epoch 154/200 train loss:0.3857 train acc:80.99% \n",
            "Epoch 154/200 val loss:0.4682 val acc:80.00% \n",
            "Epoch 155/200 train loss:0.3827 train acc:78.60% \n",
            "Epoch 155/200 val loss:0.4398 val acc:76.00% \n",
            "Epoch 156/200 train loss:0.3732 train acc:81.16% \n",
            "Epoch 156/200 val loss:0.4452 val acc:81.00% \n",
            "Epoch 157/200 train loss:0.3726 train acc:80.99% \n",
            "Epoch 157/200 val loss:0.5526 val acc:78.00% \n",
            "Epoch 158/200 train loss:0.4066 train acc:78.94% \n",
            "Epoch 158/200 val loss:0.4201 val acc:81.00% \n",
            "Epoch 159/200 train loss:0.3689 train acc:80.99% \n",
            "Epoch 159/200 val loss:0.4502 val acc:80.00% \n",
            "Epoch 160/200 train loss:0.3662 train acc:81.34% \n",
            "Epoch 160/200 val loss:0.4579 val acc:81.00% \n",
            "Epoch 161/200 train loss:0.3940 train acc:78.77% \n",
            "Epoch 161/200 val loss:0.4217 val acc:81.00% \n",
            "Epoch 162/200 train loss:0.3683 train acc:80.48% \n",
            "Epoch 162/200 val loss:0.5645 val acc:76.00% \n",
            "Epoch 163/200 train loss:0.3702 train acc:79.79% \n",
            "Epoch 163/200 val loss:0.4542 val acc:79.00% \n",
            "Epoch 164/200 train loss:0.3923 train acc:81.85% \n",
            "Epoch 164/200 val loss:0.4696 val acc:79.00% \n",
            "Epoch 165/200 train loss:0.3843 train acc:80.99% \n",
            "Epoch 165/200 val loss:0.4289 val acc:77.00% \n",
            "Epoch 166/200 train loss:0.3731 train acc:81.85% \n",
            "Epoch 166/200 val loss:0.4731 val acc:74.00% \n",
            "Epoch 167/200 train loss:0.3814 train acc:81.16% \n",
            "Epoch 167/200 val loss:0.4946 val acc:74.00% \n",
            "Epoch 168/200 train loss:0.3683 train acc:80.14% \n",
            "Epoch 168/200 val loss:0.5083 val acc:76.00% \n",
            "Epoch 169/200 train loss:0.3490 train acc:80.31% \n",
            "Epoch 169/200 val loss:0.4773 val acc:78.00% \n",
            "Epoch 170/200 train loss:0.3764 train acc:81.85% \n",
            "Epoch 170/200 val loss:0.4473 val acc:80.00% \n",
            "Epoch 171/200 train loss:0.3543 train acc:83.22% \n",
            "Epoch 171/200 val loss:0.4676 val acc:79.00% \n",
            "Epoch 172/200 train loss:0.3566 train acc:81.16% \n",
            "Epoch 172/200 val loss:0.4939 val acc:79.00% \n",
            "Epoch 173/200 train loss:0.3927 train acc:80.65% \n",
            "Epoch 173/200 val loss:0.5303 val acc:75.00% \n",
            "Epoch 174/200 train loss:0.3751 train acc:81.34% \n",
            "Epoch 174/200 val loss:0.4447 val acc:78.00% \n",
            "Epoch 175/200 train loss:0.4228 train acc:80.14% \n",
            "Epoch 175/200 val loss:0.4698 val acc:79.00% \n",
            "Epoch 176/200 train loss:0.3568 train acc:82.19% \n",
            "Epoch 176/200 val loss:0.4884 val acc:76.00% \n",
            "Epoch 177/200 train loss:0.3331 train acc:83.39% \n",
            "Epoch 177/200 val loss:0.4780 val acc:72.00% \n",
            "Epoch 178/200 train loss:0.3957 train acc:78.94% \n",
            "Epoch 178/200 val loss:0.4663 val acc:75.00% \n",
            "Epoch 179/200 train loss:0.3640 train acc:82.88% \n",
            "Epoch 179/200 val loss:0.5002 val acc:76.00% \n",
            "Epoch 180/200 train loss:0.4877 train acc:79.62% \n",
            "Epoch 180/200 val loss:0.5176 val acc:77.00% \n",
            "Epoch 181/200 train loss:0.3544 train acc:81.51% \n",
            "Epoch 181/200 val loss:0.4707 val acc:79.00% \n",
            "Epoch 182/200 train loss:0.4005 train acc:79.11% \n",
            "Epoch 182/200 val loss:0.4876 val acc:75.00% \n",
            "Epoch 183/200 train loss:0.3705 train acc:81.68% \n",
            "Epoch 183/200 val loss:0.4510 val acc:80.00% \n",
            "Epoch 184/200 train loss:0.3684 train acc:80.82% \n",
            "Epoch 184/200 val loss:0.4537 val acc:76.00% \n",
            "Epoch 185/200 train loss:0.3374 train acc:80.31% \n",
            "Epoch 185/200 val loss:0.4709 val acc:78.00% \n",
            "Epoch 186/200 train loss:0.3516 train acc:82.88% \n",
            "Epoch 186/200 val loss:0.4539 val acc:78.00% \n",
            "Epoch 187/200 train loss:0.3409 train acc:80.65% \n",
            "Epoch 187/200 val loss:0.4553 val acc:79.00% \n",
            "Epoch 188/200 train loss:0.3755 train acc:79.79% \n",
            "Epoch 188/200 val loss:0.4692 val acc:76.00% \n",
            "Epoch 189/200 train loss:0.3446 train acc:83.05% \n",
            "Epoch 189/200 val loss:0.4621 val acc:81.00% \n",
            "Epoch 190/200 train loss:0.3631 train acc:82.36% \n",
            "Epoch 190/200 val loss:0.4843 val acc:75.00% \n",
            "Epoch 191/200 train loss:0.5464 train acc:78.94% \n",
            "Epoch 191/200 val loss:0.7409 val acc:65.00% \n",
            "Epoch 192/200 train loss:0.5506 train acc:75.68% \n",
            "Epoch 192/200 val loss:0.5583 val acc:74.00% \n",
            "Epoch 193/200 train loss:0.4343 train acc:79.28% \n",
            "Epoch 193/200 val loss:0.4304 val acc:82.00% \n",
            "Epoch 194/200 train loss:0.4166 train acc:80.31% \n",
            "Epoch 194/200 val loss:0.4282 val acc:82.00% \n",
            "Epoch 195/200 train loss:0.4600 train acc:79.28% \n",
            "Epoch 195/200 val loss:0.4395 val acc:81.00% \n",
            "Epoch 196/200 train loss:0.4371 train acc:79.97% \n",
            "Epoch 196/200 val loss:0.4598 val acc:82.00% \n",
            "Epoch 197/200 train loss:0.4089 train acc:80.14% \n",
            "Epoch 197/200 val loss:0.4961 val acc:82.00% \n",
            "Epoch 198/200 train loss:0.3895 train acc:80.31% \n",
            "Epoch 198/200 val loss:0.5032 val acc:75.00% \n",
            "Epoch 199/200 train loss:0.4025 train acc:82.53% \n",
            "Epoch 199/200 val loss:0.4481 val acc:77.00% \n",
            "Epoch 200/200 train loss:0.3751 train acc:81.16% \n",
            "Epoch 200/200 val loss:0.5028 val acc:75.00% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 16\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "n_samples = len(myDataSet)\n",
        "assert n_samples == 684, \"684\"\n",
        "\n",
        "model = CNN_LSTM().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = 584\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(myDataSet, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=False)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch {}/{} train loss:{:.4f} train acc:{:.2f}% \".format(epoch+1,epochs, loss_train, 100 * correct_train/num_train))\n",
        "  print(\"Epoch {}/{} val loss:{:.4f} val acc:{:.2f}% \".format(epoch+1,epochs, loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  # if loss_val < best_loss:\n",
        "  #   best_loss = loss_val\n",
        "  #   torch.save(model, os.path.join(base_path, 'audios', \"best_loss.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    best_train = correct_train\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_lstm_att.pth\"))\n",
        "  \n",
        "  if correct_val == best_acc and best_train < correct_train:\n",
        "    best_acc = correct_val\n",
        "    best_train = correct_train\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_lstm_att.pth\"))\n",
        "\n",
        "    \n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9J7Kv-WJI5N",
        "outputId": "582f4913-b83d-48f7-c9c7-4bc314677c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200 train loss:1.0357 train acc:42.64% \n",
            "Epoch 1/200 val loss:0.9610 val acc:35.00% \n",
            "Epoch 2/200 train loss:1.0011 train acc:51.54% \n",
            "Epoch 2/200 val loss:1.0445 val acc:59.00% \n",
            "Epoch 3/200 train loss:0.9815 train acc:48.97% \n",
            "Epoch 3/200 val loss:1.0195 val acc:54.00% \n",
            "Epoch 4/200 train loss:0.9543 train acc:55.65% \n",
            "Epoch 4/200 val loss:0.9913 val acc:57.00% \n",
            "Epoch 5/200 train loss:0.9740 train acc:49.83% \n",
            "Epoch 5/200 val loss:0.9485 val acc:58.00% \n",
            "Epoch 6/200 train loss:0.8934 train acc:51.88% \n",
            "Epoch 6/200 val loss:0.7982 val acc:51.00% \n",
            "Epoch 7/200 train loss:0.7878 train acc:55.14% \n",
            "Epoch 7/200 val loss:0.6595 val acc:74.00% \n",
            "Epoch 8/200 train loss:0.7241 train acc:61.47% \n",
            "Epoch 8/200 val loss:0.6126 val acc:70.00% \n",
            "Epoch 9/200 train loss:0.5849 train acc:67.64% \n",
            "Epoch 9/200 val loss:0.6113 val acc:68.00% \n",
            "Epoch 10/200 train loss:0.6306 train acc:64.04% \n",
            "Epoch 10/200 val loss:0.5440 val acc:67.00% \n",
            "Epoch 11/200 train loss:0.5608 train acc:66.95% \n",
            "Epoch 11/200 val loss:0.5514 val acc:69.00% \n",
            "Epoch 12/200 train loss:0.6027 train acc:70.03% \n",
            "Epoch 12/200 val loss:0.5475 val acc:74.00% \n",
            "Epoch 13/200 train loss:0.5789 train acc:71.58% \n",
            "Epoch 13/200 val loss:1.0092 val acc:67.00% \n",
            "Epoch 14/200 train loss:0.6116 train acc:71.58% \n",
            "Epoch 14/200 val loss:0.5433 val acc:77.00% \n",
            "Epoch 15/200 train loss:0.5146 train acc:72.95% \n",
            "Epoch 15/200 val loss:0.5346 val acc:74.00% \n",
            "Epoch 16/200 train loss:0.5385 train acc:74.49% \n",
            "Epoch 16/200 val loss:0.6215 val acc:67.00% \n",
            "Epoch 17/200 train loss:0.5090 train acc:73.63% \n",
            "Epoch 17/200 val loss:0.4798 val acc:79.00% \n",
            "Epoch 18/200 train loss:0.5655 train acc:71.92% \n",
            "Epoch 18/200 val loss:0.5562 val acc:74.00% \n",
            "Epoch 19/200 train loss:0.5142 train acc:75.86% \n",
            "Epoch 19/200 val loss:0.4892 val acc:77.00% \n",
            "Epoch 20/200 train loss:0.4851 train acc:75.86% \n",
            "Epoch 20/200 val loss:0.4684 val acc:80.00% \n",
            "Epoch 21/200 train loss:0.4910 train acc:76.88% \n",
            "Epoch 21/200 val loss:0.5252 val acc:80.00% \n",
            "Epoch 22/200 train loss:0.6357 train acc:72.95% \n",
            "Epoch 22/200 val loss:0.6516 val acc:75.00% \n",
            "Epoch 23/200 train loss:0.5177 train acc:75.17% \n",
            "Epoch 23/200 val loss:0.5195 val acc:77.00% \n",
            "Epoch 24/200 train loss:0.4878 train acc:75.34% \n",
            "Epoch 24/200 val loss:0.5689 val acc:78.00% \n",
            "Epoch 25/200 train loss:0.5075 train acc:75.00% \n",
            "Epoch 25/200 val loss:0.4646 val acc:80.00% \n",
            "Epoch 26/200 train loss:0.4784 train acc:75.86% \n",
            "Epoch 26/200 val loss:0.5448 val acc:75.00% \n",
            "Epoch 27/200 train loss:0.5304 train acc:75.00% \n",
            "Epoch 27/200 val loss:0.5188 val acc:76.00% \n",
            "Epoch 28/200 train loss:0.6708 train acc:73.80% \n",
            "Epoch 28/200 val loss:0.5257 val acc:75.00% \n",
            "Epoch 29/200 train loss:0.4734 train acc:76.88% \n",
            "Epoch 29/200 val loss:0.5097 val acc:78.00% \n",
            "Epoch 30/200 train loss:0.5007 train acc:76.37% \n",
            "Epoch 30/200 val loss:0.5007 val acc:76.00% \n",
            "Epoch 31/200 train loss:0.5017 train acc:74.32% \n",
            "Epoch 31/200 val loss:0.5342 val acc:76.00% \n",
            "Epoch 32/200 train loss:0.5546 train acc:74.49% \n",
            "Epoch 32/200 val loss:0.5107 val acc:78.00% \n",
            "Epoch 33/200 train loss:0.4724 train acc:77.05% \n",
            "Epoch 33/200 val loss:0.4777 val acc:80.00% \n",
            "Epoch 34/200 train loss:0.4738 train acc:77.57% \n",
            "Epoch 34/200 val loss:0.4638 val acc:80.00% \n",
            "Epoch 35/200 train loss:0.4711 train acc:77.40% \n",
            "Epoch 35/200 val loss:0.4547 val acc:80.00% \n",
            "Epoch 36/200 train loss:0.4793 train acc:76.88% \n",
            "Epoch 36/200 val loss:0.4861 val acc:80.00% \n",
            "Epoch 37/200 train loss:0.4850 train acc:78.08% \n",
            "Epoch 37/200 val loss:0.4929 val acc:79.00% \n",
            "Epoch 38/200 train loss:0.4584 train acc:79.11% \n",
            "Epoch 38/200 val loss:0.5500 val acc:78.00% \n",
            "Epoch 39/200 train loss:0.4696 train acc:77.91% \n",
            "Epoch 39/200 val loss:0.4926 val acc:78.00% \n",
            "Epoch 40/200 train loss:0.4467 train acc:78.25% \n",
            "Epoch 40/200 val loss:0.4602 val acc:80.00% \n",
            "Epoch 41/200 train loss:0.4744 train acc:77.74% \n",
            "Epoch 41/200 val loss:0.4789 val acc:81.00% \n",
            "Epoch 42/200 train loss:0.4761 train acc:77.74% \n",
            "Epoch 42/200 val loss:0.5066 val acc:81.00% \n",
            "Epoch 43/200 train loss:0.4583 train acc:78.08% \n",
            "Epoch 43/200 val loss:0.5316 val acc:74.00% \n",
            "Epoch 44/200 train loss:0.4609 train acc:77.05% \n",
            "Epoch 44/200 val loss:0.5254 val acc:75.00% \n",
            "Epoch 45/200 train loss:0.4770 train acc:78.25% \n",
            "Epoch 45/200 val loss:0.6002 val acc:73.00% \n",
            "Epoch 46/200 train loss:0.6384 train acc:72.43% \n",
            "Epoch 46/200 val loss:0.4632 val acc:79.00% \n",
            "Epoch 47/200 train loss:0.4804 train acc:76.71% \n",
            "Epoch 47/200 val loss:0.5266 val acc:71.00% \n",
            "Epoch 48/200 train loss:0.4772 train acc:77.40% \n",
            "Epoch 48/200 val loss:0.4808 val acc:80.00% \n",
            "Epoch 49/200 train loss:0.4523 train acc:77.74% \n",
            "Epoch 49/200 val loss:0.4468 val acc:80.00% \n",
            "Epoch 50/200 train loss:0.4422 train acc:78.08% \n",
            "Epoch 50/200 val loss:0.4936 val acc:77.00% \n",
            "Epoch 51/200 train loss:0.4566 train acc:77.57% \n",
            "Epoch 51/200 val loss:0.4441 val acc:76.00% \n",
            "Epoch 52/200 train loss:0.4372 train acc:76.54% \n",
            "Epoch 52/200 val loss:0.4367 val acc:80.00% \n",
            "Epoch 53/200 train loss:0.4362 train acc:78.42% \n",
            "Epoch 53/200 val loss:0.3910 val acc:81.00% \n",
            "Epoch 54/200 train loss:0.4783 train acc:74.32% \n",
            "Epoch 54/200 val loss:0.5390 val acc:78.00% \n",
            "Epoch 55/200 train loss:0.4787 train acc:77.05% \n",
            "Epoch 55/200 val loss:0.4706 val acc:80.00% \n",
            "Epoch 56/200 train loss:0.4475 train acc:78.08% \n",
            "Epoch 56/200 val loss:0.4654 val acc:80.00% \n",
            "Epoch 57/200 train loss:0.4224 train acc:78.25% \n",
            "Epoch 57/200 val loss:0.5192 val acc:77.00% \n",
            "Epoch 58/200 train loss:0.4461 train acc:78.08% \n",
            "Epoch 58/200 val loss:0.4639 val acc:77.00% \n",
            "Epoch 59/200 train loss:0.4760 train acc:77.23% \n",
            "Epoch 59/200 val loss:0.4799 val acc:78.00% \n",
            "Epoch 60/200 train loss:0.4366 train acc:78.42% \n",
            "Epoch 60/200 val loss:0.4922 val acc:79.00% \n",
            "Epoch 61/200 train loss:0.4479 train acc:78.08% \n",
            "Epoch 61/200 val loss:0.4815 val acc:80.00% \n",
            "Epoch 62/200 train loss:0.4617 train acc:78.08% \n",
            "Epoch 62/200 val loss:0.4964 val acc:80.00% \n",
            "Epoch 63/200 train loss:0.4366 train acc:77.74% \n",
            "Epoch 63/200 val loss:0.4223 val acc:80.00% \n",
            "Epoch 64/200 train loss:0.4814 train acc:77.40% \n",
            "Epoch 64/200 val loss:0.4830 val acc:78.00% \n",
            "Epoch 65/200 train loss:0.4370 train acc:77.40% \n",
            "Epoch 65/200 val loss:0.5089 val acc:75.00% \n",
            "Epoch 66/200 train loss:0.4281 train acc:77.23% \n",
            "Epoch 66/200 val loss:0.4999 val acc:75.00% \n",
            "Epoch 67/200 train loss:0.4563 train acc:75.51% \n",
            "Epoch 67/200 val loss:0.4302 val acc:79.00% \n",
            "Epoch 68/200 train loss:0.4297 train acc:79.11% \n",
            "Epoch 68/200 val loss:0.4697 val acc:76.00% \n",
            "Epoch 69/200 train loss:0.4387 train acc:77.91% \n",
            "Epoch 69/200 val loss:0.4666 val acc:78.00% \n",
            "Epoch 70/200 train loss:0.4568 train acc:76.88% \n",
            "Epoch 70/200 val loss:0.4923 val acc:75.00% \n",
            "Epoch 71/200 train loss:0.4540 train acc:78.60% \n",
            "Epoch 71/200 val loss:0.4708 val acc:75.00% \n",
            "Epoch 72/200 train loss:0.4330 train acc:79.11% \n",
            "Epoch 72/200 val loss:0.5386 val acc:74.00% \n",
            "Epoch 73/200 train loss:0.4425 train acc:77.74% \n",
            "Epoch 73/200 val loss:0.4384 val acc:77.00% \n",
            "Epoch 74/200 train loss:0.4317 train acc:77.57% \n",
            "Epoch 74/200 val loss:0.5594 val acc:77.00% \n",
            "Epoch 75/200 train loss:0.4466 train acc:77.40% \n",
            "Epoch 75/200 val loss:0.4756 val acc:74.00% \n",
            "Epoch 76/200 train loss:0.4466 train acc:78.60% \n",
            "Epoch 76/200 val loss:0.4292 val acc:78.00% \n",
            "Epoch 77/200 train loss:0.4846 train acc:75.86% \n",
            "Epoch 77/200 val loss:0.4438 val acc:78.00% \n",
            "Epoch 78/200 train loss:0.4227 train acc:80.31% \n",
            "Epoch 78/200 val loss:0.4737 val acc:73.00% \n",
            "Epoch 79/200 train loss:0.4351 train acc:76.88% \n",
            "Epoch 79/200 val loss:0.5000 val acc:75.00% \n",
            "Epoch 80/200 train loss:0.4734 train acc:74.83% \n",
            "Epoch 80/200 val loss:0.4302 val acc:79.00% \n",
            "Epoch 81/200 train loss:0.4223 train acc:78.42% \n",
            "Epoch 81/200 val loss:0.4120 val acc:81.00% \n",
            "Epoch 82/200 train loss:0.4715 train acc:76.37% \n",
            "Epoch 82/200 val loss:0.5360 val acc:76.00% \n",
            "Epoch 83/200 train loss:0.4652 train acc:76.71% \n",
            "Epoch 83/200 val loss:0.5750 val acc:76.00% \n",
            "Epoch 84/200 train loss:0.4325 train acc:78.25% \n",
            "Epoch 84/200 val loss:0.4707 val acc:79.00% \n",
            "Epoch 85/200 train loss:0.4341 train acc:78.60% \n",
            "Epoch 85/200 val loss:0.5879 val acc:72.00% \n",
            "Epoch 86/200 train loss:0.4445 train acc:77.74% \n",
            "Epoch 86/200 val loss:0.5187 val acc:75.00% \n",
            "Epoch 87/200 train loss:0.4353 train acc:76.37% \n",
            "Epoch 87/200 val loss:0.4891 val acc:77.00% \n",
            "Epoch 88/200 train loss:0.4421 train acc:77.74% \n",
            "Epoch 88/200 val loss:0.4659 val acc:79.00% \n",
            "Epoch 89/200 train loss:0.4351 train acc:76.88% \n",
            "Epoch 89/200 val loss:0.4702 val acc:81.00% \n",
            "Epoch 90/200 train loss:0.4202 train acc:78.77% \n",
            "Epoch 90/200 val loss:0.4060 val acc:81.00% \n",
            "Epoch 91/200 train loss:0.4209 train acc:78.25% \n",
            "Epoch 91/200 val loss:0.4336 val acc:81.00% \n",
            "Epoch 92/200 train loss:0.4586 train acc:77.40% \n",
            "Epoch 92/200 val loss:0.5333 val acc:75.00% \n",
            "Epoch 93/200 train loss:0.4226 train acc:77.05% \n",
            "Epoch 93/200 val loss:0.4198 val acc:76.00% \n",
            "Epoch 94/200 train loss:0.4346 train acc:76.71% \n",
            "Epoch 94/200 val loss:0.4780 val acc:75.00% \n",
            "Epoch 95/200 train loss:0.4376 train acc:77.91% \n",
            "Epoch 95/200 val loss:0.4287 val acc:81.00% \n",
            "Epoch 96/200 train loss:0.4408 train acc:77.23% \n",
            "Epoch 96/200 val loss:0.4859 val acc:70.00% \n",
            "Epoch 97/200 train loss:0.4446 train acc:76.88% \n",
            "Epoch 97/200 val loss:0.4396 val acc:78.00% \n",
            "Epoch 98/200 train loss:0.4285 train acc:78.08% \n",
            "Epoch 98/200 val loss:0.4433 val acc:80.00% \n",
            "Epoch 99/200 train loss:0.4239 train acc:78.42% \n",
            "Epoch 99/200 val loss:0.4580 val acc:79.00% \n",
            "Epoch 100/200 train loss:0.4099 train acc:79.45% \n",
            "Epoch 100/200 val loss:0.4012 val acc:77.00% \n",
            "Epoch 101/200 train loss:0.4174 train acc:75.86% \n",
            "Epoch 101/200 val loss:0.4360 val acc:80.00% \n",
            "Epoch 102/200 train loss:0.4414 train acc:75.86% \n",
            "Epoch 102/200 val loss:0.3925 val acc:80.00% \n",
            "Epoch 103/200 train loss:0.4346 train acc:78.60% \n",
            "Epoch 103/200 val loss:0.4794 val acc:77.00% \n",
            "Epoch 104/200 train loss:0.5470 train acc:76.88% \n",
            "Epoch 104/200 val loss:0.4505 val acc:79.00% \n",
            "Epoch 105/200 train loss:0.4484 train acc:78.60% \n",
            "Epoch 105/200 val loss:0.4077 val acc:82.00% \n",
            "Epoch 106/200 train loss:0.4415 train acc:78.25% \n",
            "Epoch 106/200 val loss:0.4301 val acc:81.00% \n",
            "Epoch 107/200 train loss:0.4322 train acc:79.45% \n",
            "Epoch 107/200 val loss:0.4936 val acc:81.00% \n",
            "Epoch 108/200 train loss:0.4157 train acc:79.79% \n",
            "Epoch 108/200 val loss:0.4325 val acc:79.00% \n",
            "Epoch 109/200 train loss:0.4209 train acc:78.77% \n",
            "Epoch 109/200 val loss:0.4266 val acc:81.00% \n",
            "Epoch 110/200 train loss:0.4349 train acc:78.25% \n",
            "Epoch 110/200 val loss:0.4528 val acc:81.00% \n",
            "Epoch 111/200 train loss:0.4037 train acc:79.11% \n",
            "Epoch 111/200 val loss:0.4236 val acc:81.00% \n",
            "Epoch 112/200 train loss:0.4202 train acc:78.60% \n",
            "Epoch 112/200 val loss:0.4323 val acc:79.00% \n",
            "Epoch 113/200 train loss:0.4267 train acc:78.77% \n",
            "Epoch 113/200 val loss:0.4519 val acc:82.00% \n",
            "Epoch 114/200 train loss:0.4156 train acc:79.97% \n",
            "Epoch 114/200 val loss:0.4701 val acc:77.00% \n",
            "Epoch 115/200 train loss:0.4236 train acc:79.97% \n",
            "Epoch 115/200 val loss:0.4400 val acc:76.00% \n",
            "Epoch 116/200 train loss:0.4167 train acc:80.31% \n",
            "Epoch 116/200 val loss:0.4473 val acc:75.00% \n",
            "Epoch 117/200 train loss:0.4432 train acc:78.25% \n",
            "Epoch 117/200 val loss:0.4606 val acc:79.00% \n",
            "Epoch 118/200 train loss:0.4250 train acc:78.25% \n",
            "Epoch 118/200 val loss:0.4788 val acc:81.00% \n",
            "Epoch 119/200 train loss:0.4239 train acc:76.20% \n",
            "Epoch 119/200 val loss:0.4051 val acc:82.00% \n",
            "Epoch 120/200 train loss:0.4083 train acc:79.45% \n",
            "Epoch 120/200 val loss:0.4730 val acc:81.00% \n",
            "Epoch 121/200 train loss:0.4066 train acc:78.94% \n",
            "Epoch 121/200 val loss:0.4083 val acc:82.00% \n",
            "Epoch 122/200 train loss:0.4288 train acc:78.08% \n",
            "Epoch 122/200 val loss:0.4481 val acc:83.00% \n",
            "Epoch 123/200 train loss:0.4103 train acc:79.45% \n",
            "Epoch 123/200 val loss:0.4618 val acc:81.00% \n",
            "Epoch 124/200 train loss:0.4246 train acc:78.25% \n",
            "Epoch 124/200 val loss:0.4888 val acc:78.00% \n",
            "Epoch 125/200 train loss:0.4014 train acc:78.42% \n",
            "Epoch 125/200 val loss:0.4640 val acc:80.00% \n",
            "Epoch 126/200 train loss:0.4041 train acc:79.79% \n",
            "Epoch 126/200 val loss:0.4351 val acc:81.00% \n",
            "Epoch 127/200 train loss:0.4269 train acc:78.08% \n",
            "Epoch 127/200 val loss:0.5049 val acc:82.00% \n",
            "Epoch 128/200 train loss:0.4378 train acc:79.45% \n",
            "Epoch 128/200 val loss:0.3897 val acc:82.00% \n",
            "Epoch 129/200 train loss:0.4282 train acc:77.91% \n",
            "Epoch 129/200 val loss:0.5019 val acc:77.00% \n",
            "Epoch 130/200 train loss:0.4317 train acc:76.88% \n",
            "Epoch 130/200 val loss:0.4413 val acc:76.00% \n",
            "Epoch 131/200 train loss:0.4054 train acc:76.37% \n",
            "Epoch 131/200 val loss:0.4373 val acc:77.00% \n",
            "Epoch 132/200 train loss:0.3993 train acc:78.94% \n",
            "Epoch 132/200 val loss:0.4398 val acc:82.00% \n",
            "Epoch 133/200 train loss:0.4263 train acc:78.25% \n",
            "Epoch 133/200 val loss:0.4691 val acc:80.00% \n",
            "Epoch 134/200 train loss:0.4146 train acc:78.60% \n",
            "Epoch 134/200 val loss:0.4976 val acc:81.00% \n",
            "Epoch 135/200 train loss:0.4004 train acc:78.77% \n",
            "Epoch 135/200 val loss:0.4594 val acc:75.00% \n",
            "Epoch 136/200 train loss:0.4215 train acc:77.91% \n",
            "Epoch 136/200 val loss:0.5251 val acc:82.00% \n",
            "Epoch 137/200 train loss:0.4113 train acc:78.77% \n",
            "Epoch 137/200 val loss:0.5056 val acc:74.00% \n",
            "Epoch 138/200 train loss:0.4129 train acc:79.28% \n",
            "Epoch 138/200 val loss:0.4763 val acc:83.00% \n",
            "Epoch 139/200 train loss:0.4116 train acc:78.60% \n",
            "Epoch 139/200 val loss:0.4755 val acc:79.00% \n",
            "Epoch 140/200 train loss:0.4154 train acc:79.45% \n",
            "Epoch 140/200 val loss:0.5041 val acc:82.00% \n",
            "Epoch 141/200 train loss:0.4110 train acc:78.42% \n",
            "Epoch 141/200 val loss:0.4631 val acc:75.00% \n",
            "Epoch 142/200 train loss:0.4199 train acc:79.62% \n",
            "Epoch 142/200 val loss:0.4167 val acc:80.00% \n",
            "Epoch 143/200 train loss:0.4020 train acc:79.79% \n",
            "Epoch 143/200 val loss:0.3964 val acc:81.00% \n",
            "Epoch 144/200 train loss:0.4401 train acc:78.25% \n",
            "Epoch 144/200 val loss:0.4429 val acc:76.00% \n",
            "Epoch 145/200 train loss:0.4192 train acc:79.28% \n",
            "Epoch 145/200 val loss:0.4701 val acc:84.00% \n",
            "Epoch 146/200 train loss:0.4040 train acc:78.77% \n",
            "Epoch 146/200 val loss:0.4471 val acc:80.00% \n",
            "Epoch 147/200 train loss:0.4088 train acc:77.74% \n",
            "Epoch 147/200 val loss:0.4276 val acc:77.00% \n",
            "Epoch 148/200 train loss:0.4030 train acc:78.77% \n",
            "Epoch 148/200 val loss:0.4120 val acc:82.00% \n",
            "Epoch 149/200 train loss:0.3919 train acc:79.28% \n",
            "Epoch 149/200 val loss:0.3888 val acc:76.00% \n",
            "Epoch 150/200 train loss:0.4096 train acc:77.23% \n",
            "Epoch 150/200 val loss:0.5591 val acc:77.00% \n",
            "Epoch 151/200 train loss:0.4342 train acc:76.71% \n",
            "Epoch 151/200 val loss:0.4789 val acc:74.00% \n",
            "Epoch 152/200 train loss:0.4532 train acc:76.03% \n",
            "Epoch 152/200 val loss:0.4644 val acc:78.00% \n",
            "Epoch 153/200 train loss:0.4895 train acc:77.74% \n",
            "Epoch 153/200 val loss:0.4874 val acc:82.00% \n",
            "Epoch 154/200 train loss:0.4225 train acc:79.79% \n",
            "Epoch 154/200 val loss:0.5124 val acc:75.00% \n",
            "Epoch 155/200 train loss:0.4232 train acc:78.94% \n",
            "Epoch 155/200 val loss:0.4914 val acc:75.00% \n",
            "Epoch 156/200 train loss:0.3944 train acc:80.14% \n",
            "Epoch 156/200 val loss:0.5182 val acc:75.00% \n",
            "Epoch 157/200 train loss:0.4088 train acc:77.57% \n",
            "Epoch 157/200 val loss:0.4257 val acc:79.00% \n",
            "Epoch 158/200 train loss:0.3878 train acc:81.51% \n",
            "Epoch 158/200 val loss:0.4862 val acc:75.00% \n",
            "Epoch 159/200 train loss:0.4024 train acc:79.79% \n",
            "Epoch 159/200 val loss:0.4690 val acc:77.00% \n",
            "Epoch 160/200 train loss:0.4208 train acc:80.99% \n",
            "Epoch 160/200 val loss:0.4499 val acc:75.00% \n",
            "Epoch 161/200 train loss:0.4136 train acc:78.94% \n",
            "Epoch 161/200 val loss:0.5237 val acc:72.00% \n",
            "Epoch 162/200 train loss:0.4107 train acc:80.31% \n",
            "Epoch 162/200 val loss:0.4644 val acc:82.00% \n",
            "Epoch 163/200 train loss:0.4047 train acc:79.79% \n",
            "Epoch 163/200 val loss:0.4238 val acc:81.00% \n",
            "Epoch 164/200 train loss:0.4011 train acc:78.42% \n",
            "Epoch 164/200 val loss:0.4265 val acc:78.00% \n",
            "Epoch 165/200 train loss:0.4153 train acc:78.42% \n",
            "Epoch 165/200 val loss:0.4551 val acc:80.00% \n",
            "Epoch 166/200 train loss:0.3889 train acc:79.28% \n",
            "Epoch 166/200 val loss:0.4392 val acc:82.00% \n",
            "Epoch 167/200 train loss:0.3809 train acc:79.62% \n",
            "Epoch 167/200 val loss:0.5259 val acc:78.00% \n",
            "Epoch 168/200 train loss:0.3983 train acc:81.34% \n",
            "Epoch 168/200 val loss:0.5575 val acc:76.00% \n",
            "Epoch 169/200 train loss:0.4247 train acc:78.25% \n",
            "Epoch 169/200 val loss:0.4994 val acc:77.00% \n",
            "Epoch 170/200 train loss:0.3950 train acc:79.79% \n",
            "Epoch 170/200 val loss:0.4105 val acc:85.00% \n",
            "Epoch 171/200 train loss:0.4155 train acc:79.79% \n",
            "Epoch 171/200 val loss:0.4873 val acc:74.00% \n",
            "Epoch 172/200 train loss:0.3906 train acc:78.77% \n",
            "Epoch 172/200 val loss:0.4593 val acc:78.00% \n",
            "Epoch 173/200 train loss:0.3994 train acc:79.62% \n",
            "Epoch 173/200 val loss:0.4577 val acc:75.00% \n",
            "Epoch 174/200 train loss:0.3960 train acc:79.79% \n",
            "Epoch 174/200 val loss:0.4773 val acc:81.00% \n",
            "Epoch 175/200 train loss:0.4056 train acc:79.45% \n",
            "Epoch 175/200 val loss:0.5036 val acc:73.00% \n",
            "Epoch 176/200 train loss:0.3979 train acc:79.11% \n",
            "Epoch 176/200 val loss:0.4658 val acc:81.00% \n",
            "Epoch 177/200 train loss:0.3829 train acc:80.14% \n",
            "Epoch 177/200 val loss:0.4764 val acc:78.00% \n",
            "Epoch 178/200 train loss:0.3925 train acc:80.14% \n",
            "Epoch 178/200 val loss:0.3779 val acc:83.00% \n",
            "Epoch 179/200 train loss:0.4046 train acc:78.08% \n",
            "Epoch 179/200 val loss:0.4647 val acc:77.00% \n",
            "Epoch 180/200 train loss:0.3855 train acc:80.14% \n",
            "Epoch 180/200 val loss:0.4908 val acc:74.00% \n",
            "Epoch 181/200 train loss:0.4033 train acc:80.31% \n",
            "Epoch 181/200 val loss:0.5103 val acc:76.00% \n",
            "Epoch 182/200 train loss:0.3832 train acc:81.34% \n",
            "Epoch 182/200 val loss:0.3875 val acc:78.00% \n",
            "Epoch 183/200 train loss:0.4102 train acc:78.60% \n",
            "Epoch 183/200 val loss:0.3637 val acc:81.00% \n",
            "Epoch 184/200 train loss:0.4050 train acc:79.79% \n",
            "Epoch 184/200 val loss:0.4240 val acc:81.00% \n",
            "Epoch 185/200 train loss:0.4073 train acc:79.79% \n",
            "Epoch 185/200 val loss:0.4387 val acc:85.00% \n",
            "Epoch 186/200 train loss:0.3947 train acc:80.31% \n",
            "Epoch 186/200 val loss:0.4475 val acc:75.00% \n",
            "Epoch 187/200 train loss:0.3829 train acc:79.79% \n",
            "Epoch 187/200 val loss:0.4087 val acc:83.00% \n",
            "Epoch 188/200 train loss:0.4081 train acc:78.77% \n",
            "Epoch 188/200 val loss:0.4693 val acc:81.00% \n",
            "Epoch 189/200 train loss:0.3813 train acc:81.68% \n",
            "Epoch 189/200 val loss:0.4628 val acc:81.00% \n",
            "Epoch 190/200 train loss:0.3803 train acc:80.99% \n",
            "Epoch 190/200 val loss:0.3929 val acc:81.00% \n",
            "Epoch 191/200 train loss:0.3818 train acc:79.28% \n",
            "Epoch 191/200 val loss:0.3600 val acc:81.00% \n",
            "Epoch 192/200 train loss:0.3717 train acc:79.97% \n",
            "Epoch 192/200 val loss:0.3957 val acc:79.00% \n",
            "Epoch 193/200 train loss:0.3847 train acc:79.45% \n",
            "Epoch 193/200 val loss:0.3966 val acc:76.00% \n",
            "Epoch 194/200 train loss:0.3781 train acc:80.31% \n",
            "Epoch 194/200 val loss:0.4524 val acc:77.00% \n",
            "Epoch 195/200 train loss:0.3905 train acc:80.65% \n",
            "Epoch 195/200 val loss:0.5291 val acc:78.00% \n",
            "Epoch 196/200 train loss:0.3900 train acc:80.65% \n",
            "Epoch 196/200 val loss:0.4838 val acc:75.00% \n",
            "Epoch 197/200 train loss:0.3931 train acc:80.31% \n",
            "Epoch 197/200 val loss:0.5394 val acc:79.00% \n",
            "Epoch 198/200 train loss:0.3734 train acc:78.42% \n",
            "Epoch 198/200 val loss:0.4500 val acc:77.00% \n",
            "Epoch 199/200 train loss:0.3833 train acc:80.48% \n",
            "Epoch 199/200 val loss:0.4629 val acc:77.00% \n",
            "Epoch 200/200 train loss:0.3785 train acc:81.85% \n",
            "Epoch 200/200 val loss:0.4438 val acc:74.00% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k9MiKbxfJKyw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}