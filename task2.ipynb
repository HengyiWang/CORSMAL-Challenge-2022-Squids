{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAtLh0sm1I7M"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JB4zHBD1OCI",
        "outputId": "adc0b233-eca0-4fdc-f513-80920b3824a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/COSRMAL_CHALLENGE/CORSMAL-Challenge-2022-Squids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVrlJWn71PER",
        "outputId": "2cdaadd9-bcc9-4155-e186-9579fd932089"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/COSRMAL_CHALLENGE/CORSMAL-Challenge-2022-Squids\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pQfv4bkWeyrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f3d738-a961-4bee-d037-9172407fe230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import scipy\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.io.wavfile\n",
        "import time\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import json\n",
        "from utils import AudioProcessing, audioPreprocessing, voting\n",
        "from dataset import audioDataSet\n",
        "from models import *\n",
        "from helper import train_audio, evaluate_audio\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLLLJI151I7R"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "OK345_nrpxEU",
        "outputId": "62852252-aa6a-425c-c028-9efb08d4802d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cd941269-8c41-43a1-8d66-ca00763a8fc0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>container id</th>\n",
              "      <th>scenario</th>\n",
              "      <th>background</th>\n",
              "      <th>illumination</th>\n",
              "      <th>width at the top</th>\n",
              "      <th>width at the bottom</th>\n",
              "      <th>height</th>\n",
              "      <th>depth</th>\n",
              "      <th>container capacity</th>\n",
              "      <th>container mass</th>\n",
              "      <th>filling type</th>\n",
              "      <th>filling level</th>\n",
              "      <th>filling density</th>\n",
              "      <th>filling mass</th>\n",
              "      <th>object mass</th>\n",
              "      <th>handover starting frame</th>\n",
              "      <th>handover start timestamp</th>\n",
              "      <th>handover hand</th>\n",
              "      <th>action</th>\n",
              "      <th>nframes</th>\n",
              "      <th>folder_num</th>\n",
              "      <th>file_name</th>\n",
              "      <th>num</th>\n",
              "      <th>subject</th>\n",
              "      <th>filling_type</th>\n",
              "      <th>filling_level</th>\n",
              "      <th>back</th>\n",
              "      <th>light</th>\n",
              "      <th>camera_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.82</td>\n",
              "      <td>76.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>291576</td>\n",
              "      <td>2</td>\n",
              "      <td>s2_fi2_fu1_b1_l0</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>3209.397</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118483</td>\n",
              "      <td>7</td>\n",
              "      <td>s0_fi0_fu0_b0_l0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>93.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>572008</td>\n",
              "      <td>2</td>\n",
              "      <td>s0_fi3_fu1_b1_l0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.40</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1239.840</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141680</td>\n",
              "      <td>8</td>\n",
              "      <td>s0_fi0_fu0_b1_l0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>296.000</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34</td>\n",
              "      <td>45.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138681</td>\n",
              "      <td>4</td>\n",
              "      <td>s1_fi1_fu1_b1_l0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd941269-8c41-43a1-8d66-ca00763a8fc0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cd941269-8c41-43a1-8d66-ca00763a8fc0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cd941269-8c41-43a1-8d66-ca00763a8fc0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id  container id  scenario  background  ...  light  camera_id  start  end\n",
              "0   0             2         2           1  ...      0          2   0.75  3.5\n",
              "1   1             7         0           0  ...      0          2  -1.00 -1.0\n",
              "2   2             2         0           1  ...      0          2   3.40  6.5\n",
              "3   3             8         0           1  ...      0          2  -1.00 -1.0\n",
              "4   4             4         1           1  ...      0          2   0.75  1.8\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "gt = pd.read_csv('./files/train.csv')\n",
        "gt.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb-WRGUQp1kI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3c82347c08dd4398865e6db8022f348b",
            "b7c58ff9e6e24c3bb33017e1fe667829",
            "00f4779660c84ef390bf53e08a13e67c",
            "43c97afb2ae2403b96d13234b819658c",
            "f95c362f6a6748f8ba21d3ad5d1aedce",
            "1089e0445501489d8b7f4932693525ab",
            "54f3cde0cc1e43c0a8130d1fd60453e2",
            "f6355c88af2742e8ae90137efb82f8eb",
            "769f390ca88040c2a3a4b6d140e71f72",
            "7f8cebc4e6bb4a76a6c0279f9b461ac5",
            "521c91eb5f6a4c3588bbece9858b1d98"
          ]
        },
        "outputId": "383725aa-f1f1-4fcc-d152-747ddb5f5ab3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c82347c08dd4398865e6db8022f348b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/684 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "base_path = '/content/'\n",
        "audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio'\n",
        "mfcc_path = (os.path.join(base_path, 'audios', 'mfcc'))\n",
        "os.makedirs(mfcc_path, exist_ok=True)\n",
        "os.makedirs(os.path.join(base_path, 'audios'), exist_ok=True)\n",
        "\n",
        "audioPreprocessing(audio_folder, gt, base_path, mfcc_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESWl3lO21I7T"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTAZbi9Es7KO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f64cd429fb20492380d23d280ab22a61",
            "ce1cb672ceb64cc69b1fb309f07f8c64",
            "839fb92f74fa4f69af8fb0edd2688570",
            "0734a8234e8140c7b0e961db53d4f8fe",
            "2f31d80b6a0649ed9704eabfbaab92f3",
            "46f3597adefc47549c9c38084150bb4a",
            "51697d5b862c41abb8ec91c72dce8619",
            "9985a27fae844e46bed07e11d3fe3620",
            "7513f43a3c064b938463446181e7fd63",
            "69a928df30a946f499743feb1a5216da",
            "ed96e6968b054d51b674ec05ca452d9e"
          ]
        },
        "outputId": "bd7c2089-bfef-42b9-8e2a-9b4ea0a4dbae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset initializing...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f64cd429fb20492380d23d280ab22a61",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/31812 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "mydataset = audioDataSet(base_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iCXYy9Z__ZhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i67jE5JN1I7U"
      },
      "source": [
        "## Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oad1iOtNl01a",
        "outputId": "c033f748-1dcd-4f21-93cd-1d1ac609d313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200 train loss:1.0360 train acc:73.44% \n",
            "Epoch 1/200 val loss:0.9534 val acc:79.19% \n",
            "Epoch 2/200 train loss:0.9431 train acc:80.36% \n",
            "Epoch 2/200 val loss:0.9258 val acc:81.64% \n",
            "Epoch 3/200 train loss:0.9241 train acc:81.98% \n",
            "Epoch 3/200 val loss:0.9357 val acc:80.56% \n",
            "Epoch 4/200 train loss:0.9132 train acc:82.80% \n",
            "Epoch 4/200 val loss:0.9041 val acc:84.02% \n",
            "Epoch 5/200 train loss:0.8858 train acc:86.28% \n",
            "Epoch 5/200 val loss:1.0089 val acc:72.62% \n",
            "Epoch 6/200 train loss:0.8672 train acc:87.78% \n",
            "Epoch 6/200 val loss:0.8978 val acc:84.50% \n",
            "Epoch 7/200 train loss:0.8605 train acc:88.34% \n",
            "Epoch 7/200 val loss:0.9934 val acc:74.57% \n",
            "Epoch 8/200 train loss:0.8526 train acc:89.10% \n",
            "Epoch 8/200 val loss:0.8889 val acc:85.34% \n",
            "Epoch 9/200 train loss:0.8470 train acc:89.68% \n",
            "Epoch 9/200 val loss:0.8650 val acc:87.79% \n",
            "Epoch 10/200 train loss:0.8426 train acc:90.15% \n",
            "Epoch 10/200 val loss:0.8547 val acc:88.86% \n",
            "Epoch 11/200 train loss:0.8390 train acc:90.55% \n",
            "Epoch 11/200 val loss:0.8496 val acc:89.31% \n",
            "Epoch 12/200 train loss:0.8368 train acc:90.66% \n",
            "Epoch 12/200 val loss:0.8499 val acc:89.23% \n",
            "Epoch 13/200 train loss:0.8327 train acc:91.15% \n",
            "Epoch 13/200 val loss:0.9411 val acc:80.03% \n",
            "Epoch 14/200 train loss:0.8312 train acc:91.28% \n",
            "Epoch 14/200 val loss:0.8509 val acc:89.09% \n",
            "Epoch 15/200 train loss:0.8292 train acc:91.50% \n",
            "Epoch 15/200 val loss:0.9029 val acc:84.03% \n",
            "Epoch 16/200 train loss:0.8248 train acc:91.90% \n",
            "Epoch 16/200 val loss:0.9373 val acc:80.34% \n",
            "Epoch 17/200 train loss:0.8230 train acc:92.13% \n",
            "Epoch 17/200 val loss:0.8491 val acc:89.30% \n",
            "Epoch 18/200 train loss:0.8216 train acc:92.22% \n",
            "Epoch 18/200 val loss:0.8611 val acc:88.28% \n",
            "Epoch 19/200 train loss:0.8209 train acc:92.26% \n",
            "Epoch 19/200 val loss:0.8812 val acc:85.97% \n",
            "Epoch 20/200 train loss:0.8165 train acc:92.81% \n",
            "Epoch 20/200 val loss:0.8674 val acc:87.46% \n",
            "Epoch 21/200 train loss:0.8144 train acc:92.92% \n",
            "Epoch 21/200 val loss:0.8563 val acc:88.59% \n",
            "Epoch 22/200 train loss:0.8153 train acc:92.82% \n",
            "Epoch 22/200 val loss:0.9562 val acc:78.34% \n",
            "Epoch 23/200 train loss:0.8121 train acc:93.14% \n",
            "Epoch 23/200 val loss:0.8666 val acc:87.49% \n",
            "Epoch 24/200 train loss:0.8111 train acc:93.31% \n",
            "Epoch 24/200 val loss:0.8641 val acc:87.87% \n",
            "Epoch 25/200 train loss:0.8098 train acc:93.45% \n",
            "Epoch 25/200 val loss:0.8888 val acc:85.18% \n",
            "Epoch 26/200 train loss:0.8054 train acc:93.87% \n",
            "Epoch 26/200 val loss:0.8687 val acc:87.43% \n",
            "Epoch 27/200 train loss:0.8057 train acc:93.83% \n",
            "Epoch 27/200 val loss:0.8900 val acc:85.01% \n",
            "Epoch 28/200 train loss:0.8062 train acc:93.75% \n",
            "Epoch 28/200 val loss:0.8724 val acc:87.08% \n",
            "Epoch 29/200 train loss:0.8039 train acc:94.01% \n",
            "Epoch 29/200 val loss:0.8435 val acc:89.93% \n",
            "Epoch 30/200 train loss:0.8017 train acc:94.19% \n",
            "Epoch 30/200 val loss:0.9274 val acc:81.25% \n",
            "Epoch 31/200 train loss:0.7998 train acc:94.44% \n",
            "Epoch 31/200 val loss:0.9214 val acc:81.90% \n",
            "Epoch 32/200 train loss:0.8003 train acc:94.32% \n",
            "Epoch 32/200 val loss:0.8517 val acc:89.09% \n",
            "Epoch 33/200 train loss:0.7989 train acc:94.49% \n",
            "Epoch 33/200 val loss:0.8993 val acc:84.28% \n",
            "Epoch 34/200 train loss:0.7974 train acc:94.66% \n",
            "Epoch 34/200 val loss:0.9431 val acc:79.74% \n",
            "Epoch 35/200 train loss:0.7999 train acc:94.38% \n",
            "Epoch 35/200 val loss:0.8753 val acc:86.74% \n",
            "Epoch 36/200 train loss:0.7978 train acc:94.62% \n",
            "Epoch 36/200 val loss:0.8495 val acc:89.34% \n",
            "Epoch 37/200 train loss:0.7952 train acc:94.86% \n",
            "Epoch 37/200 val loss:0.9061 val acc:83.72% \n",
            "Epoch 38/200 train loss:0.7941 train acc:94.96% \n",
            "Epoch 38/200 val loss:0.8587 val acc:88.40% \n",
            "Epoch 39/200 train loss:0.7956 train acc:94.81% \n",
            "Epoch 39/200 val loss:0.8622 val acc:88.06% \n",
            "Epoch 40/200 train loss:0.7923 train acc:95.20% \n",
            "Epoch 40/200 val loss:0.8484 val acc:89.39% \n",
            "Epoch 41/200 train loss:0.7937 train acc:95.02% \n",
            "Epoch 41/200 val loss:0.8468 val acc:89.56% \n",
            "Epoch 42/200 train loss:0.7937 train acc:95.00% \n",
            "Epoch 42/200 val loss:0.8460 val acc:89.66% \n",
            "Epoch 43/200 train loss:0.7906 train acc:95.33% \n",
            "Epoch 43/200 val loss:0.8663 val acc:87.55% \n",
            "Epoch 44/200 train loss:0.7916 train acc:95.21% \n",
            "Epoch 44/200 val loss:0.8472 val acc:89.50% \n",
            "Epoch 45/200 train loss:0.7925 train acc:95.11% \n",
            "Epoch 45/200 val loss:0.9376 val acc:80.42% \n",
            "Epoch 46/200 train loss:0.7894 train acc:95.45% \n",
            "Epoch 46/200 val loss:0.8531 val acc:89.01% \n",
            "Epoch 47/200 train loss:0.7907 train acc:95.32% \n",
            "Epoch 47/200 val loss:0.8548 val acc:88.84% \n",
            "Epoch 48/200 train loss:0.7904 train acc:95.33% \n",
            "Epoch 48/200 val loss:0.8680 val acc:87.38% \n",
            "Epoch 49/200 train loss:0.7888 train acc:95.50% \n",
            "Epoch 49/200 val loss:0.8672 val acc:87.57% \n",
            "Epoch 50/200 train loss:0.7894 train acc:95.45% \n",
            "Epoch 50/200 val loss:0.8378 val acc:90.46% \n",
            "Epoch 51/200 train loss:0.7879 train acc:95.60% \n",
            "Epoch 51/200 val loss:0.8643 val acc:87.84% \n",
            "Epoch 52/200 train loss:0.7893 train acc:95.45% \n",
            "Epoch 52/200 val loss:0.8572 val acc:88.61% \n",
            "Epoch 53/200 train loss:0.7871 train acc:95.66% \n",
            "Epoch 53/200 val loss:0.8453 val acc:89.67% \n",
            "Epoch 54/200 train loss:0.7874 train acc:95.60% \n",
            "Epoch 54/200 val loss:0.9923 val acc:74.85% \n",
            "Epoch 55/200 train loss:0.7882 train acc:95.53% \n",
            "Epoch 55/200 val loss:0.8462 val acc:89.78% \n",
            "Epoch 56/200 train loss:0.7863 train acc:95.72% \n",
            "Epoch 56/200 val loss:0.8948 val acc:84.76% \n",
            "Epoch 57/200 train loss:0.7873 train acc:95.61% \n",
            "Epoch 57/200 val loss:0.8378 val acc:90.48% \n",
            "Epoch 58/200 train loss:0.7843 train acc:95.95% \n",
            "Epoch 58/200 val loss:0.9086 val acc:83.28% \n",
            "Epoch 59/200 train loss:0.7836 train acc:96.05% \n",
            "Epoch 59/200 val loss:0.9462 val acc:79.44% \n",
            "Epoch 60/200 train loss:0.7869 train acc:95.66% \n",
            "Epoch 60/200 val loss:0.8373 val acc:90.46% \n",
            "Epoch 61/200 train loss:0.7848 train acc:95.89% \n",
            "Epoch 61/200 val loss:0.8842 val acc:85.87% \n",
            "Epoch 62/200 train loss:0.7863 train acc:95.72% \n",
            "Epoch 62/200 val loss:0.8810 val acc:86.20% \n",
            "Epoch 63/200 train loss:0.7828 train acc:96.10% \n",
            "Epoch 63/200 val loss:0.8509 val acc:89.08% \n",
            "Epoch 64/200 train loss:0.7847 train acc:95.92% \n",
            "Epoch 64/200 val loss:0.8609 val acc:88.29% \n",
            "Epoch 65/200 train loss:0.7868 train acc:95.68% \n",
            "Epoch 65/200 val loss:0.8899 val acc:85.31% \n",
            "Epoch 66/200 train loss:0.7846 train acc:95.92% \n",
            "Epoch 66/200 val loss:0.8507 val acc:89.20% \n",
            "Epoch 67/200 train loss:0.7819 train acc:96.20% \n",
            "Epoch 67/200 val loss:0.8515 val acc:89.11% \n",
            "Epoch 68/200 train loss:0.7820 train acc:96.17% \n",
            "Epoch 68/200 val loss:0.8586 val acc:88.34% \n",
            "Epoch 69/200 train loss:0.7822 train acc:96.18% \n",
            "Epoch 69/200 val loss:0.8477 val acc:89.53% \n",
            "Epoch 70/200 train loss:0.7822 train acc:96.15% \n",
            "Epoch 70/200 val loss:0.8470 val acc:89.61% \n",
            "Epoch 71/200 train loss:0.7816 train acc:96.22% \n",
            "Epoch 71/200 val loss:0.8529 val acc:89.01% \n",
            "Epoch 72/200 train loss:0.7817 train acc:96.20% \n",
            "Epoch 72/200 val loss:0.8500 val acc:89.30% \n",
            "Epoch 73/200 train loss:0.7813 train acc:96.24% \n",
            "Epoch 73/200 val loss:0.8852 val acc:85.64% \n",
            "Epoch 74/200 train loss:0.7810 train acc:96.28% \n",
            "Epoch 74/200 val loss:0.9094 val acc:83.23% \n",
            "Epoch 75/200 train loss:0.7812 train acc:96.24% \n",
            "Epoch 75/200 val loss:0.8414 val acc:90.08% \n",
            "Epoch 76/200 train loss:0.7792 train acc:96.45% \n",
            "Epoch 76/200 val loss:0.8361 val acc:90.66% \n",
            "Epoch 77/200 train loss:0.7795 train acc:96.41% \n",
            "Epoch 77/200 val loss:0.8692 val acc:87.40% \n",
            "Epoch 78/200 train loss:0.7811 train acc:96.25% \n",
            "Epoch 78/200 val loss:0.8439 val acc:89.85% \n",
            "Epoch 79/200 train loss:0.7792 train acc:96.44% \n",
            "Epoch 79/200 val loss:0.8452 val acc:89.78% \n",
            "Epoch 80/200 train loss:0.7810 train acc:96.25% \n",
            "Epoch 80/200 val loss:0.8821 val acc:85.95% \n",
            "Epoch 81/200 train loss:0.7811 train acc:96.27% \n",
            "Epoch 81/200 val loss:0.8502 val acc:89.27% \n",
            "Epoch 82/200 train loss:0.7788 train acc:96.49% \n",
            "Epoch 82/200 val loss:0.8486 val acc:89.31% \n",
            "Epoch 83/200 train loss:0.7796 train acc:96.40% \n",
            "Epoch 83/200 val loss:0.8372 val acc:90.52% \n",
            "Epoch 84/200 train loss:0.7778 train acc:96.60% \n",
            "Epoch 84/200 val loss:0.8523 val acc:89.01% \n",
            "Epoch 85/200 train loss:0.7783 train acc:96.52% \n",
            "Epoch 85/200 val loss:0.8463 val acc:89.58% \n",
            "Epoch 86/200 train loss:0.7795 train acc:96.42% \n",
            "Epoch 86/200 val loss:0.8651 val acc:87.66% \n",
            "Epoch 87/200 train loss:0.7785 train acc:96.50% \n",
            "Epoch 87/200 val loss:0.8349 val acc:90.77% \n",
            "Epoch 88/200 train loss:0.7772 train acc:96.65% \n",
            "Epoch 88/200 val loss:0.8455 val acc:89.75% \n",
            "Epoch 89/200 train loss:0.7784 train acc:96.52% \n",
            "Epoch 89/200 val loss:0.8466 val acc:89.66% \n",
            "Epoch 90/200 train loss:0.7769 train acc:96.67% \n",
            "Epoch 90/200 val loss:0.8400 val acc:90.33% \n",
            "Epoch 91/200 train loss:0.7753 train acc:96.84% \n",
            "Epoch 91/200 val loss:0.8592 val acc:88.20% \n",
            "Epoch 92/200 train loss:0.7776 train acc:96.57% \n",
            "Epoch 92/200 val loss:0.8464 val acc:89.60% \n",
            "Epoch 93/200 train loss:0.7758 train acc:96.81% \n",
            "Epoch 93/200 val loss:0.8465 val acc:89.60% \n",
            "Epoch 94/200 train loss:0.7771 train acc:96.64% \n",
            "Epoch 94/200 val loss:0.8441 val acc:89.86% \n",
            "Epoch 95/200 train loss:0.7760 train acc:96.74% \n",
            "Epoch 95/200 val loss:0.9288 val acc:81.08% \n",
            "Epoch 96/200 train loss:0.7787 train acc:96.51% \n",
            "Epoch 96/200 val loss:0.8500 val acc:89.31% \n",
            "Epoch 97/200 train loss:0.7767 train acc:96.70% \n",
            "Epoch 97/200 val loss:0.8453 val acc:89.85% \n",
            "Epoch 98/200 train loss:0.7780 train acc:96.54% \n",
            "Epoch 98/200 val loss:1.0982 val acc:64.04% \n",
            "Epoch 99/200 train loss:0.7754 train acc:96.81% \n",
            "Epoch 99/200 val loss:0.8364 val acc:90.57% \n",
            "Epoch 100/200 train loss:0.7761 train acc:96.78% \n",
            "Epoch 100/200 val loss:0.8379 val acc:90.49% \n",
            "Epoch 101/200 train loss:0.7753 train acc:96.84% \n",
            "Epoch 101/200 val loss:0.8405 val acc:90.29% \n",
            "Epoch 102/200 train loss:0.7745 train acc:96.93% \n",
            "Epoch 102/200 val loss:0.8370 val acc:90.63% \n",
            "Epoch 103/200 train loss:0.7753 train acc:96.81% \n",
            "Epoch 103/200 val loss:0.8485 val acc:89.49% \n",
            "Epoch 104/200 train loss:0.7736 train acc:97.00% \n",
            "Epoch 104/200 val loss:0.8416 val acc:90.11% \n",
            "Epoch 105/200 train loss:0.7737 train acc:97.02% \n",
            "Epoch 105/200 val loss:0.8423 val acc:90.11% \n",
            "Epoch 106/200 train loss:0.7752 train acc:96.85% \n",
            "Epoch 106/200 val loss:0.8812 val acc:86.08% \n",
            "Epoch 107/200 train loss:0.7770 train acc:96.68% \n",
            "Epoch 107/200 val loss:0.8585 val acc:88.42% \n",
            "Epoch 108/200 train loss:0.7754 train acc:96.81% \n",
            "Epoch 108/200 val loss:0.8552 val acc:88.68% \n",
            "Epoch 109/200 train loss:0.7760 train acc:96.75% \n",
            "Epoch 109/200 val loss:0.8396 val acc:90.26% \n",
            "Epoch 110/200 train loss:0.7746 train acc:96.90% \n",
            "Epoch 110/200 val loss:0.8346 val acc:90.82% \n",
            "Epoch 111/200 train loss:0.7726 train acc:97.10% \n",
            "Epoch 111/200 val loss:0.8397 val acc:90.30% \n",
            "Epoch 112/200 train loss:0.7753 train acc:96.83% \n",
            "Epoch 112/200 val loss:0.8417 val acc:90.05% \n",
            "Epoch 113/200 train loss:0.7725 train acc:97.14% \n",
            "Epoch 113/200 val loss:0.8344 val acc:90.93% \n",
            "Epoch 114/200 train loss:0.7720 train acc:97.18% \n",
            "Epoch 114/200 val loss:0.8601 val acc:88.23% \n",
            "Epoch 115/200 train loss:0.7728 train acc:97.08% \n",
            "Epoch 115/200 val loss:0.8377 val acc:90.52% \n",
            "Epoch 116/200 train loss:0.7731 train acc:97.05% \n",
            "Epoch 116/200 val loss:0.8358 val acc:90.62% \n",
            "Epoch 117/200 train loss:0.7729 train acc:97.08% \n",
            "Epoch 117/200 val loss:0.8495 val acc:89.36% \n",
            "Epoch 118/200 train loss:0.7749 train acc:96.86% \n",
            "Epoch 118/200 val loss:0.8459 val acc:89.74% \n",
            "Epoch 119/200 train loss:0.7740 train acc:96.96% \n",
            "Epoch 119/200 val loss:0.8355 val acc:90.70% \n",
            "Epoch 120/200 train loss:0.7736 train acc:97.00% \n",
            "Epoch 120/200 val loss:0.8610 val acc:88.04% \n",
            "Epoch 121/200 train loss:0.7738 train acc:97.01% \n",
            "Epoch 121/200 val loss:0.8367 val acc:90.66% \n",
            "Epoch 122/200 train loss:0.7717 train acc:97.19% \n",
            "Epoch 122/200 val loss:0.8313 val acc:91.15% \n",
            "Epoch 123/200 train loss:0.7713 train acc:97.23% \n",
            "Epoch 123/200 val loss:0.8518 val acc:88.92% \n",
            "Epoch 124/200 train loss:0.7714 train acc:97.23% \n",
            "Epoch 124/200 val loss:0.8609 val acc:88.20% \n",
            "Epoch 125/200 train loss:0.7718 train acc:97.21% \n",
            "Epoch 125/200 val loss:0.8441 val acc:89.96% \n",
            "Epoch 126/200 train loss:0.7703 train acc:97.35% \n",
            "Epoch 126/200 val loss:0.8379 val acc:90.44% \n",
            "Epoch 127/200 train loss:0.7707 train acc:97.32% \n",
            "Epoch 127/200 val loss:0.8412 val acc:90.16% \n",
            "Epoch 128/200 train loss:0.7718 train acc:97.20% \n",
            "Epoch 128/200 val loss:0.8416 val acc:90.05% \n",
            "Epoch 129/200 train loss:0.7737 train acc:96.98% \n",
            "Epoch 129/200 val loss:0.8949 val acc:84.71% \n",
            "Epoch 130/200 train loss:0.7725 train acc:97.12% \n",
            "Epoch 130/200 val loss:0.9009 val acc:84.14% \n",
            "Epoch 131/200 train loss:0.7717 train acc:97.19% \n",
            "Epoch 131/200 val loss:0.8399 val acc:90.41% \n",
            "Epoch 132/200 train loss:0.7704 train acc:97.33% \n",
            "Epoch 132/200 val loss:0.8673 val acc:87.52% \n",
            "Epoch 133/200 train loss:0.7725 train acc:97.10% \n",
            "Epoch 133/200 val loss:0.9922 val acc:74.71% \n",
            "Epoch 134/200 train loss:0.7711 train acc:97.26% \n",
            "Epoch 134/200 val loss:0.8365 val acc:90.65% \n",
            "Epoch 135/200 train loss:0.7698 train acc:97.39% \n",
            "Epoch 135/200 val loss:0.8395 val acc:90.32% \n",
            "Epoch 136/200 train loss:0.7706 train acc:97.32% \n",
            "Epoch 136/200 val loss:0.8370 val acc:90.55% \n",
            "Epoch 137/200 train loss:0.7712 train acc:97.25% \n",
            "Epoch 137/200 val loss:0.9025 val acc:83.89% \n",
            "Epoch 138/200 train loss:0.7697 train acc:97.41% \n",
            "Epoch 138/200 val loss:0.8335 val acc:90.90% \n",
            "Epoch 139/200 train loss:0.7695 train acc:97.38% \n",
            "Epoch 139/200 val loss:0.8545 val acc:88.84% \n",
            "Epoch 140/200 train loss:0.7700 train acc:97.36% \n",
            "Epoch 140/200 val loss:0.8449 val acc:89.75% \n",
            "Epoch 141/200 train loss:0.7696 train acc:97.40% \n",
            "Epoch 141/200 val loss:0.8430 val acc:89.93% \n",
            "Epoch 142/200 train loss:0.7684 train acc:97.52% \n",
            "Epoch 142/200 val loss:0.8360 val acc:90.65% \n",
            "Epoch 143/200 train loss:0.7683 train acc:97.54% \n",
            "Epoch 143/200 val loss:1.0154 val acc:72.54% \n",
            "Epoch 144/200 train loss:0.7706 train acc:97.30% \n",
            "Epoch 144/200 val loss:0.8374 val acc:90.54% \n",
            "Epoch 145/200 train loss:0.7705 train acc:97.31% \n",
            "Epoch 145/200 val loss:0.8809 val acc:85.98% \n",
            "Epoch 146/200 train loss:0.7700 train acc:97.38% \n",
            "Epoch 146/200 val loss:0.8518 val acc:89.08% \n",
            "Epoch 147/200 train loss:0.7714 train acc:97.21% \n",
            "Epoch 147/200 val loss:0.8347 val acc:90.84% \n",
            "Epoch 148/200 train loss:0.7699 train acc:97.38% \n",
            "Epoch 148/200 val loss:0.8337 val acc:90.92% \n",
            "Epoch 149/200 train loss:0.7694 train acc:97.42% \n",
            "Epoch 149/200 val loss:0.8342 val acc:90.87% \n",
            "Epoch 150/200 train loss:0.7696 train acc:97.42% \n",
            "Epoch 150/200 val loss:0.8486 val acc:89.38% \n",
            "Epoch 151/200 train loss:0.7683 train acc:97.54% \n",
            "Epoch 151/200 val loss:0.8508 val acc:89.22% \n",
            "Epoch 152/200 train loss:0.7672 train acc:97.67% \n",
            "Epoch 152/200 val loss:0.8418 val acc:90.07% \n",
            "Epoch 153/200 train loss:0.7715 train acc:97.22% \n",
            "Epoch 153/200 val loss:0.8484 val acc:89.42% \n",
            "Epoch 154/200 train loss:0.7695 train acc:97.42% \n",
            "Epoch 154/200 val loss:0.8357 val acc:90.66% \n",
            "Epoch 155/200 train loss:0.7686 train acc:97.51% \n",
            "Epoch 155/200 val loss:0.8302 val acc:91.37% \n",
            "Epoch 156/200 train loss:0.7682 train acc:97.56% \n",
            "Epoch 156/200 val loss:0.8541 val acc:88.75% \n",
            "Epoch 157/200 train loss:0.7687 train acc:97.50% \n",
            "Epoch 157/200 val loss:0.8269 val acc:91.62% \n",
            "Epoch 158/200 train loss:0.7682 train acc:97.56% \n",
            "Epoch 158/200 val loss:0.8364 val acc:90.59% \n",
            "Epoch 159/200 train loss:0.7683 train acc:97.52% \n",
            "Epoch 159/200 val loss:0.8285 val acc:91.39% \n",
            "Epoch 160/200 train loss:0.7706 train acc:97.29% \n",
            "Epoch 160/200 val loss:0.8368 val acc:90.57% \n",
            "Epoch 161/200 train loss:0.7687 train acc:97.49% \n",
            "Epoch 161/200 val loss:0.8408 val acc:90.21% \n",
            "Epoch 162/200 train loss:0.7676 train acc:97.61% \n",
            "Epoch 162/200 val loss:0.8290 val acc:91.43% \n",
            "Epoch 163/200 train loss:0.7675 train acc:97.62% \n",
            "Epoch 163/200 val loss:0.8245 val acc:91.87% \n",
            "Epoch 164/200 train loss:0.7680 train acc:97.58% \n",
            "Epoch 164/200 val loss:0.8353 val acc:90.79% \n",
            "Epoch 165/200 train loss:0.7685 train acc:97.52% \n",
            "Epoch 165/200 val loss:0.8392 val acc:90.33% \n",
            "Epoch 166/200 train loss:0.7677 train acc:97.58% \n",
            "Epoch 166/200 val loss:0.8336 val acc:90.92% \n",
            "Epoch 167/200 train loss:0.7689 train acc:97.47% \n",
            "Epoch 167/200 val loss:0.8816 val acc:86.19% \n",
            "Epoch 168/200 train loss:0.7680 train acc:97.58% \n",
            "Epoch 168/200 val loss:0.8264 val acc:91.64% \n",
            "Epoch 169/200 train loss:0.7677 train acc:97.61% \n",
            "Epoch 169/200 val loss:0.8368 val acc:90.62% \n",
            "Epoch 170/200 train loss:0.7669 train acc:97.68% \n",
            "Epoch 170/200 val loss:0.9737 val acc:76.83% \n",
            "Epoch 171/200 train loss:0.7694 train acc:97.41% \n",
            "Epoch 171/200 val loss:0.8413 val acc:90.16% \n",
            "Epoch 172/200 train loss:0.7667 train acc:97.68% \n",
            "Epoch 172/200 val loss:0.8304 val acc:91.26% \n",
            "Epoch 173/200 train loss:0.7668 train acc:97.70% \n",
            "Epoch 173/200 val loss:0.8572 val acc:88.43% \n",
            "Epoch 174/200 train loss:0.7685 train acc:97.52% \n",
            "Epoch 174/200 val loss:0.8916 val acc:85.07% \n",
            "Epoch 175/200 train loss:0.7669 train acc:97.69% \n",
            "Epoch 175/200 val loss:0.8388 val acc:90.33% \n",
            "Epoch 176/200 train loss:0.7656 train acc:97.82% \n",
            "Epoch 176/200 val loss:0.8344 val acc:90.84% \n",
            "Epoch 177/200 train loss:0.7657 train acc:97.80% \n",
            "Epoch 177/200 val loss:0.8368 val acc:90.55% \n",
            "Epoch 178/200 train loss:0.7695 train acc:97.43% \n",
            "Epoch 178/200 val loss:0.8321 val acc:91.03% \n",
            "Epoch 179/200 train loss:0.7676 train acc:97.61% \n",
            "Epoch 179/200 val loss:0.8282 val acc:91.47% \n",
            "Epoch 180/200 train loss:0.7667 train acc:97.72% \n",
            "Epoch 180/200 val loss:0.8643 val acc:87.71% \n",
            "Epoch 181/200 train loss:0.7673 train acc:97.63% \n",
            "Epoch 181/200 val loss:0.8368 val acc:90.57% \n",
            "Epoch 182/200 train loss:0.7685 train acc:97.50% \n",
            "Epoch 182/200 val loss:0.8569 val acc:88.50% \n",
            "Epoch 183/200 train loss:0.7685 train acc:97.50% \n",
            "Epoch 183/200 val loss:0.9029 val acc:83.81% \n",
            "Epoch 184/200 train loss:0.7677 train acc:97.60% \n",
            "Epoch 184/200 val loss:0.8271 val acc:91.67% \n",
            "Epoch 185/200 train loss:0.7650 train acc:97.87% \n",
            "Epoch 185/200 val loss:0.8253 val acc:91.75% \n",
            "Epoch 186/200 train loss:0.7651 train acc:97.85% \n",
            "Epoch 186/200 val loss:0.8378 val acc:90.52% \n",
            "Epoch 187/200 train loss:0.7656 train acc:97.79% \n",
            "Epoch 187/200 val loss:0.8965 val acc:84.54% \n",
            "Epoch 188/200 train loss:0.7667 train acc:97.69% \n",
            "Epoch 188/200 val loss:0.8632 val acc:87.99% \n",
            "Epoch 189/200 train loss:0.7667 train acc:97.69% \n",
            "Epoch 189/200 val loss:0.8347 val acc:90.74% \n",
            "Epoch 190/200 train loss:0.7665 train acc:97.71% \n",
            "Epoch 190/200 val loss:0.8268 val acc:91.54% \n",
            "Epoch 191/200 train loss:0.7657 train acc:97.79% \n",
            "Epoch 191/200 val loss:0.8259 val acc:91.73% \n",
            "Epoch 192/200 train loss:0.7652 train acc:97.84% \n",
            "Epoch 192/200 val loss:0.8213 val acc:92.19% \n",
            "Epoch 193/200 train loss:0.7676 train acc:97.60% \n",
            "Epoch 193/200 val loss:0.8603 val acc:88.23% \n",
            "Epoch 194/200 train loss:0.7690 train acc:97.46% \n",
            "Epoch 194/200 val loss:0.8516 val acc:88.97% \n",
            "Epoch 195/200 train loss:0.7667 train acc:97.70% \n",
            "Epoch 195/200 val loss:0.8308 val acc:91.10% \n",
            "Epoch 196/200 train loss:0.7648 train acc:97.89% \n",
            "Epoch 196/200 val loss:0.8268 val acc:91.62% \n",
            "Epoch 197/200 train loss:0.7658 train acc:97.77% \n",
            "Epoch 197/200 val loss:0.8396 val acc:90.51% \n",
            "Epoch 198/200 train loss:0.7652 train acc:97.83% \n",
            "Epoch 198/200 val loss:0.8366 val acc:90.62% \n",
            "Epoch 199/200 train loss:0.7671 train acc:97.65% \n",
            "Epoch 199/200 val loss:0.8394 val acc:90.40% \n",
            "Epoch 200/200 train loss:0.7658 train acc:97.78% \n",
            "Epoch 200/200 val loss:0.8389 val acc:90.37% \n"
          ]
        }
      ],
      "source": [
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-5\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  loss_train, correct_train = train_audio(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "  print(\"Epoch {}/{} train loss:{:.4f} train acc:{:.2f}% \".format(epoch+1,epochs, loss_train, 100 * correct_train/num_train))\n",
        "  print(\"Epoch {}/{} val loss:{:.4f} val acc:{:.2f}% \".format(epoch+1,epochs, loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  if loss_val < best_loss:\n",
        "    best_loss = loss_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_loss.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_val.pth\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho3pWlGf1I7W"
      },
      "source": [
        "## MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPonh4SL1_M9",
        "outputId": "8556c6bd-9364-4e66-a6fe-7eb3112f461b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/200 train loss:0.6436 train acc:76.81% val loss:0.6356 val acc:75.44%\n",
            "2/200 train loss:0.3759 train acc:87.12% val loss:0.8538 val acc:69.65%\n",
            "3/200 train loss:0.3205 train acc:89.04% val loss:0.3209 val acc:88.75%\n",
            "4/200 train loss:0.2887 train acc:89.76% val loss:0.3830 val acc:88.10%\n",
            "5/200 train loss:0.2623 train acc:90.82% val loss:4.0947 val acc:37.92%\n",
            "6/200 train loss:0.2388 train acc:91.45% val loss:0.5306 val acc:85.60%\n",
            "7/200 train loss:0.2206 train acc:92.23% val loss:0.4271 val acc:85.93%\n",
            "8/200 train loss:0.2042 train acc:92.79% val loss:0.3086 val acc:89.66%\n",
            "9/200 train loss:0.1900 train acc:93.00% val loss:0.3779 val acc:87.02%\n",
            "10/200 train loss:0.1671 train acc:93.95% val loss:0.4665 val acc:86.23%\n",
            "11/200 train loss:0.1490 train acc:94.59% val loss:0.4374 val acc:84.21%\n",
            "12/200 train loss:0.1354 train acc:95.16% val loss:0.4699 val acc:84.11%\n",
            "13/200 train loss:0.1264 train acc:95.49% val loss:0.5517 val acc:86.75%\n",
            "14/200 train loss:0.1105 train acc:95.83% val loss:0.5975 val acc:82.51%\n",
            "15/200 train loss:0.0909 train acc:96.73% val loss:0.5483 val acc:87.46%\n",
            "16/200 train loss:0.0849 train acc:96.82% val loss:0.6471 val acc:80.37%\n",
            "17/200 train loss:0.0747 train acc:97.38% val loss:0.4912 val acc:89.89%\n",
            "18/200 train loss:0.0755 train acc:97.21% val loss:0.3970 val acc:87.93%\n",
            "19/200 train loss:0.0654 train acc:97.78% val loss:0.3595 val acc:89.53%\n",
            "20/200 train loss:0.0526 train acc:98.09% val loss:0.3757 val acc:91.20%\n",
            "21/200 train loss:0.0535 train acc:98.06% val loss:0.4843 val acc:90.71%\n",
            "22/200 train loss:0.0484 train acc:98.24% val loss:0.3753 val acc:90.88%\n",
            "23/200 train loss:0.0534 train acc:98.19% val loss:0.3942 val acc:89.72%\n",
            "24/200 train loss:0.0446 train acc:98.44% val loss:0.5791 val acc:88.84%\n",
            "25/200 train loss:0.0479 train acc:98.26% val loss:0.3874 val acc:91.45%\n",
            "26/200 train loss:0.0411 train acc:98.60% val loss:0.3357 val acc:92.25%\n",
            "27/200 train loss:0.0499 train acc:98.26% val loss:0.3548 val acc:90.68%\n",
            "28/200 train loss:0.0347 train acc:98.72% val loss:0.4968 val acc:88.81%\n",
            "29/200 train loss:0.0397 train acc:98.68% val loss:0.6247 val acc:81.24%\n",
            "30/200 train loss:0.0336 train acc:98.86% val loss:0.5043 val acc:87.88%\n",
            "31/200 train loss:0.0389 train acc:98.69% val loss:0.4023 val acc:91.56%\n",
            "32/200 train loss:0.0313 train acc:98.92% val loss:0.3813 val acc:91.84%\n",
            "33/200 train loss:0.0320 train acc:98.86% val loss:0.4522 val acc:89.49%\n",
            "34/200 train loss:0.0320 train acc:98.90% val loss:0.6965 val acc:83.51%\n",
            "35/200 train loss:0.0345 train acc:98.84% val loss:0.3720 val acc:91.12%\n",
            "36/200 train loss:0.0281 train acc:99.03% val loss:0.5478 val acc:86.30%\n",
            "37/200 train loss:0.0299 train acc:98.90% val loss:0.2545 val acc:93.98%\n",
            "38/200 train loss:0.0306 train acc:98.99% val loss:0.3202 val acc:91.07%\n",
            "39/200 train loss:0.0276 train acc:99.00% val loss:0.7015 val acc:86.11%\n",
            "40/200 train loss:0.0339 train acc:98.79% val loss:0.5578 val acc:88.26%\n",
            "41/200 train loss:0.0270 train acc:99.06% val loss:0.4575 val acc:91.48%\n",
            "42/200 train loss:0.0242 train acc:99.16% val loss:0.7295 val acc:87.43%\n",
            "43/200 train loss:0.0304 train acc:98.98% val loss:0.7459 val acc:86.59%\n",
            "44/200 train loss:0.0315 train acc:98.94% val loss:0.5310 val acc:91.12%\n",
            "45/200 train loss:0.0240 train acc:99.17% val loss:0.3719 val acc:90.55%\n",
            "46/200 train loss:0.0227 train acc:99.26% val loss:0.5446 val acc:91.17%\n",
            "47/200 train loss:0.0254 train acc:99.12% val loss:0.4377 val acc:90.65%\n",
            "48/200 train loss:0.0240 train acc:99.25% val loss:1.2009 val acc:72.36%\n",
            "49/200 train loss:0.0342 train acc:98.81% val loss:0.2588 val acc:93.10%\n",
            "50/200 train loss:0.0168 train acc:99.42% val loss:0.3432 val acc:93.53%\n",
            "51/200 train loss:0.0233 train acc:99.23% val loss:0.2730 val acc:93.64%\n",
            "52/200 train loss:0.0242 train acc:99.23% val loss:0.3682 val acc:92.00%\n",
            "53/200 train loss:0.0204 train acc:99.35% val loss:0.4094 val acc:89.31%\n",
            "54/200 train loss:0.0196 train acc:99.34% val loss:0.4719 val acc:89.03%\n",
            "55/200 train loss:0.0249 train acc:99.11% val loss:0.4283 val acc:91.42%\n",
            "56/200 train loss:0.0219 train acc:99.23% val loss:0.7742 val acc:86.88%\n",
            "57/200 train loss:0.0183 train acc:99.45% val loss:0.6011 val acc:88.04%\n",
            "58/200 train loss:0.0218 train acc:99.30% val loss:0.4043 val acc:90.46%\n",
            "59/200 train loss:0.0163 train acc:99.50% val loss:0.7328 val acc:88.97%\n",
            "60/200 train loss:0.0221 train acc:99.15% val loss:0.5047 val acc:87.47%\n",
            "61/200 train loss:0.0207 train acc:99.26% val loss:0.5624 val acc:90.73%\n",
            "62/200 train loss:0.0206 train acc:99.31% val loss:0.3368 val acc:93.34%\n",
            "63/200 train loss:0.0159 train acc:99.49% val loss:0.3246 val acc:92.90%\n",
            "64/200 train loss:0.0226 train acc:99.22% val loss:0.2303 val acc:95.07%\n",
            "65/200 train loss:0.0150 train acc:99.50% val loss:0.3238 val acc:92.27%\n",
            "66/200 train loss:0.0196 train acc:99.31% val loss:0.4491 val acc:91.65%\n",
            "67/200 train loss:0.0202 train acc:99.30% val loss:0.3068 val acc:93.71%\n",
            "68/200 train loss:0.0125 train acc:99.61% val loss:1.0067 val acc:85.42%\n",
            "69/200 train loss:0.0218 train acc:99.29% val loss:0.9728 val acc:86.45%\n",
            "70/200 train loss:0.0262 train acc:99.14% val loss:0.2335 val acc:93.62%\n",
            "71/200 train loss:0.0280 train acc:99.03% val loss:0.3125 val acc:93.04%\n",
            "72/200 train loss:0.0086 train acc:99.72% val loss:0.5024 val acc:91.58%\n",
            "73/200 train loss:0.0112 train acc:99.64% val loss:0.3637 val acc:92.74%\n",
            "74/200 train loss:0.0149 train acc:99.49% val loss:0.6031 val acc:85.92%\n",
            "75/200 train loss:0.0135 train acc:99.59% val loss:0.5208 val acc:91.25%\n",
            "76/200 train loss:0.0135 train acc:99.58% val loss:0.4604 val acc:92.14%\n",
            "77/200 train loss:0.0199 train acc:99.31% val loss:0.5889 val acc:88.48%\n",
            "78/200 train loss:0.0149 train acc:99.50% val loss:0.3969 val acc:92.11%\n",
            "79/200 train loss:0.0181 train acc:99.41% val loss:0.7074 val acc:89.61%\n",
            "80/200 train loss:0.0161 train acc:99.46% val loss:0.4768 val acc:89.33%\n",
            "81/200 train loss:0.0154 train acc:99.51% val loss:0.4102 val acc:92.27%\n",
            "82/200 train loss:0.0170 train acc:99.41% val loss:0.4261 val acc:91.56%\n",
            "83/200 train loss:0.0100 train acc:99.70% val loss:0.2823 val acc:93.48%\n",
            "84/200 train loss:0.0177 train acc:99.48% val loss:0.3092 val acc:93.48%\n",
            "85/200 train loss:0.0136 train acc:99.56% val loss:0.2574 val acc:94.55%\n",
            "86/200 train loss:0.0155 train acc:99.47% val loss:0.5405 val acc:88.54%\n",
            "87/200 train loss:0.0159 train acc:99.52% val loss:0.3096 val acc:94.17%\n",
            "88/200 train loss:0.0141 train acc:99.58% val loss:0.3581 val acc:92.77%\n",
            "89/200 train loss:0.0119 train acc:99.61% val loss:0.4340 val acc:92.14%\n",
            "90/200 train loss:0.0124 train acc:99.60% val loss:0.2383 val acc:94.81%\n",
            "91/200 train loss:0.0177 train acc:99.43% val loss:0.3375 val acc:93.15%\n",
            "92/200 train loss:0.0112 train acc:99.64% val loss:0.3156 val acc:93.15%\n",
            "93/200 train loss:0.0083 train acc:99.74% val loss:0.3052 val acc:93.76%\n",
            "94/200 train loss:0.0149 train acc:99.49% val loss:0.4189 val acc:92.77%\n",
            "95/200 train loss:0.0161 train acc:99.49% val loss:0.2335 val acc:94.58%\n",
            "96/200 train loss:0.0110 train acc:99.62% val loss:0.3082 val acc:93.38%\n",
            "97/200 train loss:0.0205 train acc:99.33% val loss:0.6259 val acc:86.66%\n",
            "98/200 train loss:0.0080 train acc:99.74% val loss:0.3074 val acc:93.27%\n",
            "99/200 train loss:0.0057 train acc:99.87% val loss:0.2706 val acc:94.77%\n",
            "100/200 train loss:0.0163 train acc:99.47% val loss:0.3105 val acc:94.20%\n",
            "101/200 train loss:0.0126 train acc:99.56% val loss:0.7588 val acc:86.85%\n",
            "102/200 train loss:0.0131 train acc:99.56% val loss:0.2877 val acc:94.63%\n",
            "103/200 train loss:0.0098 train acc:99.67% val loss:0.2653 val acc:93.79%\n",
            "104/200 train loss:0.0140 train acc:99.56% val loss:0.2972 val acc:94.26%\n",
            "105/200 train loss:0.0093 train acc:99.69% val loss:0.2436 val acc:94.69%\n",
            "106/200 train loss:0.0159 train acc:99.48% val loss:0.4434 val acc:90.21%\n",
            "107/200 train loss:0.0105 train acc:99.65% val loss:0.5098 val acc:90.30%\n",
            "108/200 train loss:0.0116 train acc:99.63% val loss:0.3040 val acc:93.68%\n",
            "109/200 train loss:0.0116 train acc:99.62% val loss:0.4879 val acc:91.56%\n",
            "110/200 train loss:0.0202 train acc:99.43% val loss:0.5717 val acc:85.43%\n",
            "111/200 train loss:0.0175 train acc:99.38% val loss:0.3773 val acc:91.62%\n",
            "112/200 train loss:0.0151 train acc:99.54% val loss:0.3586 val acc:92.57%\n",
            "113/200 train loss:0.0092 train acc:99.73% val loss:0.4873 val acc:91.34%\n",
            "114/200 train loss:0.0094 train acc:99.68% val loss:0.2845 val acc:94.55%\n",
            "115/200 train loss:0.0101 train acc:99.66% val loss:0.4853 val acc:90.49%\n",
            "116/200 train loss:0.0132 train acc:99.56% val loss:0.2732 val acc:94.34%\n",
            "117/200 train loss:0.0096 train acc:99.69% val loss:0.3180 val acc:94.12%\n",
            "118/200 train loss:0.0101 train acc:99.71% val loss:0.6035 val acc:88.26%\n",
            "119/200 train loss:0.0115 train acc:99.63% val loss:0.6277 val acc:90.07%\n",
            "120/200 train loss:0.0120 train acc:99.61% val loss:0.3440 val acc:93.53%\n",
            "121/200 train loss:0.0074 train acc:99.73% val loss:0.3864 val acc:91.40%\n",
            "122/200 train loss:0.0146 train acc:99.56% val loss:0.4472 val acc:89.58%\n",
            "123/200 train loss:0.0068 train acc:99.79% val loss:0.4148 val acc:91.91%\n",
            "124/200 train loss:0.0085 train acc:99.70% val loss:0.4612 val acc:92.39%\n",
            "125/200 train loss:0.0144 train acc:99.50% val loss:0.6780 val acc:87.58%\n",
            "126/200 train loss:0.0089 train acc:99.71% val loss:0.3697 val acc:93.29%\n",
            "127/200 train loss:0.0073 train acc:99.77% val loss:0.5685 val acc:91.06%\n",
            "128/200 train loss:0.0124 train acc:99.57% val loss:0.4070 val acc:93.29%\n",
            "129/200 train loss:0.0116 train acc:99.63% val loss:0.5907 val acc:91.50%\n",
            "130/200 train loss:0.0117 train acc:99.61% val loss:0.3267 val acc:94.00%\n",
            "131/200 train loss:0.0128 train acc:99.58% val loss:0.3125 val acc:92.85%\n",
            "132/200 train loss:0.0086 train acc:99.71% val loss:0.7650 val acc:85.15%\n",
            "133/200 train loss:0.0078 train acc:99.77% val loss:0.5065 val acc:91.21%\n",
            "134/200 train loss:0.0133 train acc:99.58% val loss:0.4963 val acc:89.28%\n",
            "135/200 train loss:0.0140 train acc:99.48% val loss:0.2564 val acc:94.33%\n",
            "136/200 train loss:0.0083 train acc:99.75% val loss:0.3872 val acc:92.49%\n",
            "137/200 train loss:0.0061 train acc:99.77% val loss:0.2595 val acc:95.05%\n",
            "138/200 train loss:0.0057 train acc:99.83% val loss:0.2696 val acc:94.83%\n",
            "139/200 train loss:0.0189 train acc:99.39% val loss:0.3446 val acc:93.35%\n",
            "140/200 train loss:0.0096 train acc:99.69% val loss:0.2472 val acc:94.91%\n",
            "141/200 train loss:0.0075 train acc:99.75% val loss:0.2807 val acc:95.22%\n",
            "142/200 train loss:0.0055 train acc:99.83% val loss:0.4893 val acc:91.34%\n",
            "143/200 train loss:0.0066 train acc:99.82% val loss:0.4454 val acc:93.12%\n",
            "144/200 train loss:0.0119 train acc:99.63% val loss:0.4485 val acc:91.12%\n",
            "145/200 train loss:0.0080 train acc:99.75% val loss:0.2332 val acc:95.14%\n",
            "146/200 train loss:0.0081 train acc:99.72% val loss:0.3491 val acc:93.57%\n",
            "147/200 train loss:0.0148 train acc:99.54% val loss:0.3254 val acc:92.57%\n",
            "148/200 train loss:0.0084 train acc:99.74% val loss:0.5324 val acc:91.51%\n",
            "149/200 train loss:0.0052 train acc:99.79% val loss:0.2577 val acc:95.44%\n",
            "150/200 train loss:0.0088 train acc:99.69% val loss:0.3094 val acc:93.48%\n",
            "151/200 train loss:0.0115 train acc:99.62% val loss:0.4328 val acc:93.04%\n",
            "152/200 train loss:0.0075 train acc:99.74% val loss:0.2444 val acc:95.38%\n",
            "153/200 train loss:0.0066 train acc:99.78% val loss:0.3923 val acc:93.09%\n",
            "154/200 train loss:0.0133 train acc:99.58% val loss:0.4249 val acc:92.82%\n",
            "155/200 train loss:0.0091 train acc:99.72% val loss:0.2435 val acc:94.39%\n",
            "156/200 train loss:0.0056 train acc:99.82% val loss:0.2518 val acc:95.18%\n",
            "157/200 train loss:0.0084 train acc:99.71% val loss:0.2552 val acc:95.03%\n",
            "158/200 train loss:0.0067 train acc:99.78% val loss:0.3003 val acc:94.33%\n",
            "159/200 train loss:0.0091 train acc:99.69% val loss:0.9435 val acc:84.00%\n",
            "160/200 train loss:0.0100 train acc:99.65% val loss:0.5575 val acc:92.13%\n",
            "161/200 train loss:0.0062 train acc:99.80% val loss:0.4729 val acc:93.10%\n",
            "162/200 train loss:0.0087 train acc:99.72% val loss:0.4405 val acc:92.24%\n",
            "163/200 train loss:0.0091 train acc:99.71% val loss:0.4898 val acc:89.83%\n",
            "164/200 train loss:0.0123 train acc:99.59% val loss:0.5243 val acc:89.28%\n",
            "165/200 train loss:0.0035 train acc:99.89% val loss:0.2531 val acc:95.32%\n",
            "166/200 train loss:0.0049 train acc:99.84% val loss:0.2681 val acc:95.14%\n",
            "167/200 train loss:0.0099 train acc:99.65% val loss:0.4726 val acc:90.07%\n",
            "168/200 train loss:0.0153 train acc:99.60% val loss:0.2990 val acc:94.67%\n",
            "169/200 train loss:0.0068 train acc:99.80% val loss:0.2861 val acc:94.48%\n",
            "170/200 train loss:0.0052 train acc:99.85% val loss:0.4103 val acc:90.60%\n",
            "171/200 train loss:0.0099 train acc:99.69% val loss:0.8229 val acc:87.49%\n",
            "172/200 train loss:0.0093 train acc:99.69% val loss:0.2445 val acc:95.32%\n",
            "173/200 train loss:0.0025 train acc:99.95% val loss:0.2614 val acc:95.32%\n",
            "174/200 train loss:0.0100 train acc:99.69% val loss:0.5644 val acc:87.68%\n",
            "175/200 train loss:0.0140 train acc:99.54% val loss:0.3366 val acc:93.49%\n",
            "176/200 train loss:0.0056 train acc:99.83% val loss:0.2595 val acc:94.91%\n",
            "177/200 train loss:0.0095 train acc:99.67% val loss:0.3398 val acc:92.90%\n",
            "178/200 train loss:0.0107 train acc:99.64% val loss:0.2391 val acc:95.19%\n",
            "179/200 train loss:0.0033 train acc:99.90% val loss:0.2930 val acc:94.94%\n",
            "180/200 train loss:0.0050 train acc:99.83% val loss:0.3102 val acc:94.37%\n",
            "181/200 train loss:0.0110 train acc:99.65% val loss:0.4533 val acc:90.27%\n",
            "182/200 train loss:0.0080 train acc:99.73% val loss:0.3135 val acc:93.51%\n",
            "183/200 train loss:0.0102 train acc:99.64% val loss:0.4049 val acc:93.32%\n",
            "184/200 train loss:0.0067 train acc:99.77% val loss:0.2974 val acc:94.30%\n",
            "185/200 train loss:0.0074 train acc:99.76% val loss:0.2821 val acc:95.02%\n",
            "186/200 train loss:0.0069 train acc:99.75% val loss:0.3682 val acc:93.56%\n",
            "187/200 train loss:0.0060 train acc:99.77% val loss:0.2522 val acc:95.46%\n",
            "188/200 train loss:0.0088 train acc:99.71% val loss:0.2463 val acc:95.08%\n",
            "189/200 train loss:0.0144 train acc:99.52% val loss:0.2279 val acc:94.64%\n",
            "190/200 train loss:0.0084 train acc:99.73% val loss:0.2987 val acc:94.11%\n",
            "191/200 train loss:0.0051 train acc:99.85% val loss:0.2362 val acc:94.97%\n",
            "192/200 train loss:0.0090 train acc:99.71% val loss:0.3365 val acc:92.17%\n",
            "193/200 train loss:0.0056 train acc:99.82% val loss:0.3058 val acc:94.59%\n",
            "194/200 train loss:0.0045 train acc:99.84% val loss:0.2717 val acc:94.33%\n",
            "195/200 train loss:0.0102 train acc:99.66% val loss:0.2567 val acc:94.52%\n",
            "196/200 train loss:0.0033 train acc:99.91% val loss:0.2796 val acc:95.27%\n",
            "197/200 train loss:0.0047 train acc:99.83% val loss:0.3789 val acc:93.65%\n",
            "198/200 train loss:0.0098 train acc:99.66% val loss:0.2747 val acc:94.74%\n",
            "199/200 train loss:0.0083 train acc:99.71% val loss:0.2924 val acc:95.02%\n",
            "200/200 train loss:0.0105 train acc:99.65% val loss:0.4870 val acc:89.69%\n"
          ]
        }
      ],
      "source": [
        "from models import MobileNetV3_Large\n",
        "\n",
        "mobile_save = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2'\n",
        "\n",
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-3\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = MobileNetV3_Large(input_channel=8, num_classes=4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  loss_train, correct_train = train_audio(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(mobile_save, \n",
        "                                              'mobile{:.2f}.pth'.format(100 * correct_val/num_val)))\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models import MobileNetV3_Large, mbv2_ca\n",
        "\n",
        "mobile_save = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2/mobileCA'\n",
        "\n",
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-3\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = mbv2_ca(in_c=8, num_classes=4).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  loss_train, correct_train = train_audio(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(mobile_save, \n",
        "                                              'mobile-ca{:.2f}.pth'.format(100 * correct_val/num_val)))\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tvLT-jVwyfwe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd3a602-b16f-4331-97e2-13320d53cbfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/200 train loss:0.6129 train acc:77.83% val loss:0.5256 val acc:82.24%\n",
            "2/200 train loss:0.3848 train acc:86.98% val loss:0.3936 val acc:87.07%\n",
            "3/200 train loss:0.3113 train acc:89.09% val loss:0.3765 val acc:87.47%\n",
            "4/200 train loss:0.2804 train acc:90.13% val loss:0.4528 val acc:86.00%\n",
            "5/200 train loss:0.2646 train acc:90.46% val loss:0.3243 val acc:88.97%\n",
            "6/200 train loss:0.2361 train acc:91.46% val loss:0.3081 val acc:89.17%\n",
            "7/200 train loss:0.2193 train acc:92.07% val loss:0.3885 val acc:86.74%\n",
            "8/200 train loss:0.2048 train acc:92.39% val loss:0.5264 val acc:88.23%\n",
            "9/200 train loss:0.1860 train acc:93.35% val loss:0.3215 val acc:90.21%\n",
            "10/200 train loss:0.1710 train acc:93.88% val loss:0.2684 val acc:90.33%\n",
            "11/200 train loss:0.1490 train acc:94.54% val loss:0.5264 val acc:84.47%\n",
            "12/200 train loss:0.1292 train acc:95.16% val loss:0.4551 val acc:88.40%\n",
            "13/200 train loss:0.1265 train acc:95.33% val loss:0.3580 val acc:90.85%\n",
            "14/200 train loss:0.1154 train acc:95.78% val loss:0.5583 val acc:87.51%\n",
            "15/200 train loss:0.1042 train acc:96.17% val loss:0.4106 val acc:89.39%\n",
            "16/200 train loss:0.0928 train acc:96.55% val loss:0.4361 val acc:90.26%\n",
            "17/200 train loss:0.0832 train acc:97.02% val loss:0.2972 val acc:90.81%\n",
            "18/200 train loss:0.0811 train acc:96.95% val loss:0.3222 val acc:91.23%\n",
            "19/200 train loss:0.0752 train acc:97.26% val loss:0.2849 val acc:92.68%\n",
            "20/200 train loss:0.0662 train acc:97.55% val loss:0.4015 val acc:90.63%\n",
            "21/200 train loss:0.0678 train acc:97.69% val loss:0.2772 val acc:91.62%\n",
            "22/200 train loss:0.0554 train acc:97.97% val loss:0.2792 val acc:93.59%\n",
            "23/200 train loss:0.0517 train acc:98.11% val loss:0.3579 val acc:91.89%\n",
            "24/200 train loss:0.0482 train acc:98.16% val loss:1.5559 val acc:76.22%\n",
            "25/200 train loss:0.0495 train acc:98.23% val loss:0.2410 val acc:93.90%\n",
            "26/200 train loss:0.0485 train acc:98.36% val loss:0.4204 val acc:88.98%\n",
            "27/200 train loss:0.0474 train acc:98.30% val loss:0.4179 val acc:88.12%\n",
            "28/200 train loss:0.0502 train acc:98.18% val loss:0.2850 val acc:92.35%\n",
            "29/200 train loss:0.0413 train acc:98.59% val loss:0.2661 val acc:93.54%\n",
            "30/200 train loss:0.0401 train acc:98.71% val loss:0.3151 val acc:91.25%\n",
            "31/200 train loss:0.0448 train acc:98.41% val loss:0.2799 val acc:90.90%\n",
            "32/200 train loss:0.0380 train acc:98.64% val loss:0.7166 val acc:88.31%\n",
            "33/200 train loss:0.0410 train acc:98.63% val loss:0.3241 val acc:92.90%\n",
            "34/200 train loss:0.0342 train acc:98.81% val loss:0.3748 val acc:91.23%\n",
            "35/200 train loss:0.0348 train acc:98.79% val loss:0.3863 val acc:92.24%\n",
            "36/200 train loss:0.0301 train acc:98.94% val loss:0.3874 val acc:90.29%\n",
            "37/200 train loss:0.0346 train acc:98.84% val loss:0.5344 val acc:90.88%\n",
            "38/200 train loss:0.0304 train acc:98.97% val loss:0.3476 val acc:93.35%\n",
            "39/200 train loss:0.0384 train acc:98.74% val loss:0.2388 val acc:93.24%\n",
            "40/200 train loss:0.0263 train acc:99.08% val loss:0.4298 val acc:92.83%\n",
            "41/200 train loss:0.0266 train acc:99.03% val loss:0.2012 val acc:94.31%\n",
            "42/200 train loss:0.0318 train acc:98.96% val loss:0.3573 val acc:92.49%\n",
            "43/200 train loss:0.0219 train acc:99.29% val loss:0.2392 val acc:94.92%\n",
            "44/200 train loss:0.0304 train acc:98.95% val loss:0.2904 val acc:93.86%\n",
            "45/200 train loss:0.0268 train acc:99.04% val loss:0.2198 val acc:94.86%\n",
            "46/200 train loss:0.0297 train acc:98.99% val loss:0.4818 val acc:92.09%\n",
            "47/200 train loss:0.0254 train acc:99.11% val loss:0.2725 val acc:93.21%\n",
            "48/200 train loss:0.0275 train acc:99.01% val loss:0.3335 val acc:92.79%\n",
            "49/200 train loss:0.0270 train acc:99.14% val loss:0.4592 val acc:90.54%\n",
            "50/200 train loss:0.0288 train acc:99.04% val loss:0.2412 val acc:93.37%\n",
            "51/200 train loss:0.0187 train acc:99.36% val loss:0.4494 val acc:92.58%\n",
            "52/200 train loss:0.0223 train acc:99.24% val loss:0.2500 val acc:94.69%\n",
            "53/200 train loss:0.0232 train acc:99.25% val loss:0.4165 val acc:91.51%\n",
            "54/200 train loss:0.0262 train acc:99.09% val loss:0.2909 val acc:94.17%\n",
            "55/200 train loss:0.0215 train acc:99.29% val loss:1.5639 val acc:82.62%\n",
            "56/200 train loss:0.0232 train acc:99.27% val loss:0.3258 val acc:93.12%\n",
            "57/200 train loss:0.0224 train acc:99.23% val loss:0.2362 val acc:94.23%\n",
            "58/200 train loss:0.0207 train acc:99.34% val loss:0.3615 val acc:93.93%\n",
            "59/200 train loss:0.0181 train acc:99.40% val loss:0.2911 val acc:94.58%\n",
            "60/200 train loss:0.0254 train acc:99.09% val loss:0.3113 val acc:93.09%\n",
            "61/200 train loss:0.0211 train acc:99.32% val loss:0.2745 val acc:94.30%\n",
            "62/200 train loss:0.0165 train acc:99.43% val loss:0.2903 val acc:94.58%\n",
            "63/200 train loss:0.0210 train acc:99.31% val loss:0.3293 val acc:93.09%\n",
            "64/200 train loss:0.0198 train acc:99.30% val loss:0.3607 val acc:93.42%\n",
            "65/200 train loss:0.0169 train acc:99.41% val loss:0.3488 val acc:93.42%\n",
            "66/200 train loss:0.0175 train acc:99.41% val loss:0.3969 val acc:92.66%\n",
            "67/200 train loss:0.0178 train acc:99.42% val loss:0.4320 val acc:92.53%\n",
            "68/200 train loss:0.0225 train acc:99.27% val loss:0.3284 val acc:93.73%\n",
            "69/200 train loss:0.0195 train acc:99.36% val loss:0.2101 val acc:95.24%\n",
            "70/200 train loss:0.0154 train acc:99.49% val loss:0.2385 val acc:95.00%\n",
            "71/200 train loss:0.0167 train acc:99.40% val loss:0.2253 val acc:95.21%\n",
            "72/200 train loss:0.0163 train acc:99.43% val loss:0.2324 val acc:94.94%\n",
            "73/200 train loss:0.0156 train acc:99.46% val loss:0.2683 val acc:94.48%\n",
            "74/200 train loss:0.0150 train acc:99.47% val loss:0.2198 val acc:94.91%\n",
            "75/200 train loss:0.0161 train acc:99.44% val loss:0.2524 val acc:94.39%\n",
            "76/200 train loss:0.0143 train acc:99.56% val loss:0.2408 val acc:94.81%\n",
            "77/200 train loss:0.0152 train acc:99.49% val loss:0.4985 val acc:92.00%\n",
            "78/200 train loss:0.0247 train acc:99.14% val loss:0.2258 val acc:94.20%\n",
            "79/200 train loss:0.0201 train acc:99.34% val loss:0.2089 val acc:95.43%\n",
            "80/200 train loss:0.0154 train acc:99.50% val loss:0.2134 val acc:94.56%\n",
            "81/200 train loss:0.0139 train acc:99.51% val loss:0.3185 val acc:93.05%\n",
            "82/200 train loss:0.0146 train acc:99.43% val loss:0.3065 val acc:93.40%\n",
            "83/200 train loss:0.0100 train acc:99.68% val loss:0.2986 val acc:93.04%\n",
            "84/200 train loss:0.0121 train acc:99.59% val loss:0.5002 val acc:92.16%\n",
            "85/200 train loss:0.0194 train acc:99.36% val loss:0.3497 val acc:93.21%\n",
            "86/200 train loss:0.0117 train acc:99.62% val loss:0.2440 val acc:94.86%\n",
            "87/200 train loss:0.0186 train acc:99.37% val loss:0.3757 val acc:92.53%\n",
            "88/200 train loss:0.0144 train acc:99.49% val loss:0.2320 val acc:94.88%\n",
            "89/200 train loss:0.0147 train acc:99.48% val loss:0.3921 val acc:93.24%\n",
            "90/200 train loss:0.0154 train acc:99.48% val loss:0.2141 val acc:95.21%\n",
            "91/200 train loss:0.0124 train acc:99.57% val loss:0.2772 val acc:94.50%\n",
            "92/200 train loss:0.0152 train acc:99.52% val loss:0.2523 val acc:95.05%\n",
            "93/200 train loss:0.0122 train acc:99.58% val loss:0.2557 val acc:93.95%\n",
            "94/200 train loss:0.0155 train acc:99.43% val loss:0.3243 val acc:93.38%\n",
            "95/200 train loss:0.0150 train acc:99.48% val loss:0.2850 val acc:94.25%\n",
            "96/200 train loss:0.0094 train acc:99.69% val loss:0.2440 val acc:95.05%\n",
            "97/200 train loss:0.0123 train acc:99.58% val loss:0.3897 val acc:93.26%\n",
            "98/200 train loss:0.0175 train acc:99.41% val loss:0.2500 val acc:95.14%\n",
            "99/200 train loss:0.0121 train acc:99.57% val loss:0.2795 val acc:94.91%\n",
            "100/200 train loss:0.0099 train acc:99.68% val loss:0.5003 val acc:89.45%\n",
            "101/200 train loss:0.0179 train acc:99.38% val loss:0.3146 val acc:93.87%\n",
            "102/200 train loss:0.0093 train acc:99.69% val loss:0.2361 val acc:95.52%\n",
            "103/200 train loss:0.0157 train acc:99.47% val loss:0.2354 val acc:95.13%\n",
            "104/200 train loss:0.0100 train acc:99.67% val loss:0.4843 val acc:92.28%\n",
            "105/200 train loss:0.0115 train acc:99.63% val loss:0.2521 val acc:94.97%\n",
            "106/200 train loss:0.0112 train acc:99.63% val loss:0.2105 val acc:95.68%\n",
            "107/200 train loss:0.0133 train acc:99.52% val loss:0.2810 val acc:93.81%\n",
            "108/200 train loss:0.0166 train acc:99.46% val loss:0.2716 val acc:94.97%\n",
            "109/200 train loss:0.0110 train acc:99.64% val loss:0.2188 val acc:95.79%\n",
            "110/200 train loss:0.0090 train acc:99.71% val loss:0.2295 val acc:95.68%\n",
            "111/200 train loss:0.0126 train acc:99.59% val loss:0.2073 val acc:95.87%\n",
            "112/200 train loss:0.0078 train acc:99.75% val loss:0.2312 val acc:95.79%\n",
            "113/200 train loss:0.0113 train acc:99.62% val loss:0.3243 val acc:94.53%\n",
            "114/200 train loss:0.0136 train acc:99.55% val loss:0.1954 val acc:95.93%\n",
            "115/200 train loss:0.0112 train acc:99.61% val loss:0.3805 val acc:94.04%\n",
            "116/200 train loss:0.0127 train acc:99.56% val loss:0.3120 val acc:93.98%\n",
            "117/200 train loss:0.0107 train acc:99.62% val loss:0.2974 val acc:94.30%\n",
            "118/200 train loss:0.0103 train acc:99.65% val loss:0.2204 val acc:95.82%\n",
            "119/200 train loss:0.0088 train acc:99.71% val loss:0.2209 val acc:95.88%\n",
            "120/200 train loss:0.0158 train acc:99.43% val loss:0.2061 val acc:95.49%\n",
            "121/200 train loss:0.0096 train acc:99.69% val loss:0.2536 val acc:95.16%\n",
            "122/200 train loss:0.0104 train acc:99.67% val loss:0.2237 val acc:95.02%\n",
            "123/200 train loss:0.0116 train acc:99.61% val loss:0.2833 val acc:94.86%\n",
            "124/200 train loss:0.0090 train acc:99.74% val loss:0.2364 val acc:95.77%\n",
            "125/200 train loss:0.0103 train acc:99.66% val loss:0.2429 val acc:95.62%\n",
            "126/200 train loss:0.0128 train acc:99.53% val loss:0.2580 val acc:95.51%\n",
            "127/200 train loss:0.0091 train acc:99.72% val loss:0.3686 val acc:93.92%\n",
            "128/200 train loss:0.0104 train acc:99.68% val loss:0.2960 val acc:94.30%\n",
            "129/200 train loss:0.0133 train acc:99.59% val loss:0.4134 val acc:93.38%\n",
            "130/200 train loss:0.0098 train acc:99.67% val loss:0.2340 val acc:95.36%\n",
            "131/200 train loss:0.0083 train acc:99.71% val loss:0.2127 val acc:95.55%\n",
            "132/200 train loss:0.0094 train acc:99.67% val loss:0.2527 val acc:95.65%\n",
            "133/200 train loss:0.0092 train acc:99.70% val loss:0.2252 val acc:95.58%\n",
            "134/200 train loss:0.0078 train acc:99.78% val loss:0.3209 val acc:94.67%\n",
            "135/200 train loss:0.0086 train acc:99.71% val loss:0.2403 val acc:94.20%\n",
            "136/200 train loss:0.0123 train acc:99.61% val loss:0.2411 val acc:95.38%\n",
            "137/200 train loss:0.0078 train acc:99.72% val loss:0.2336 val acc:95.69%\n",
            "138/200 train loss:0.0103 train acc:99.62% val loss:0.2697 val acc:94.86%\n",
            "139/200 train loss:0.0109 train acc:99.61% val loss:0.3249 val acc:94.77%\n",
            "140/200 train loss:0.0121 train acc:99.64% val loss:0.3440 val acc:94.26%\n",
            "141/200 train loss:0.0061 train acc:99.78% val loss:0.3889 val acc:94.19%\n",
            "142/200 train loss:0.0120 train acc:99.63% val loss:0.2064 val acc:95.73%\n",
            "143/200 train loss:0.0065 train acc:99.77% val loss:0.2445 val acc:95.43%\n",
            "144/200 train loss:0.0095 train acc:99.69% val loss:0.2421 val acc:95.74%\n",
            "145/200 train loss:0.0077 train acc:99.73% val loss:0.5319 val acc:93.23%\n",
            "146/200 train loss:0.0109 train acc:99.69% val loss:0.2973 val acc:94.92%\n",
            "147/200 train loss:0.0071 train acc:99.78% val loss:0.2565 val acc:95.43%\n",
            "148/200 train loss:0.0070 train acc:99.77% val loss:0.2243 val acc:95.90%\n",
            "149/200 train loss:0.0080 train acc:99.72% val loss:0.3417 val acc:94.20%\n",
            "150/200 train loss:0.0165 train acc:99.50% val loss:0.2193 val acc:94.89%\n",
            "151/200 train loss:0.0091 train acc:99.68% val loss:0.2894 val acc:93.32%\n",
            "152/200 train loss:0.0103 train acc:99.66% val loss:0.2239 val acc:94.63%\n",
            "153/200 train loss:0.0091 train acc:99.66% val loss:0.2687 val acc:94.91%\n",
            "154/200 train loss:0.0116 train acc:99.65% val loss:0.2401 val acc:93.92%\n",
            "155/200 train loss:0.0087 train acc:99.71% val loss:0.1927 val acc:96.13%\n",
            "156/200 train loss:0.0037 train acc:99.87% val loss:0.2790 val acc:95.24%\n",
            "157/200 train loss:0.0095 train acc:99.66% val loss:0.3237 val acc:93.40%\n",
            "158/200 train loss:0.0136 train acc:99.55% val loss:0.2741 val acc:95.24%\n",
            "159/200 train loss:0.0056 train acc:99.80% val loss:0.2380 val acc:95.77%\n",
            "160/200 train loss:0.0072 train acc:99.79% val loss:0.2523 val acc:95.00%\n",
            "161/200 train loss:0.0087 train acc:99.71% val loss:0.2039 val acc:96.35%\n",
            "162/200 train loss:0.0094 train acc:99.67% val loss:0.2904 val acc:95.19%\n",
            "163/200 train loss:0.0139 train acc:99.54% val loss:0.2599 val acc:94.69%\n",
            "164/200 train loss:0.0071 train acc:99.74% val loss:0.2226 val acc:95.84%\n",
            "165/200 train loss:0.0070 train acc:99.74% val loss:0.2327 val acc:95.66%\n",
            "166/200 train loss:0.0084 train acc:99.76% val loss:0.2466 val acc:95.24%\n",
            "167/200 train loss:0.0104 train acc:99.65% val loss:0.2062 val acc:95.82%\n",
            "168/200 train loss:0.0093 train acc:99.71% val loss:0.2916 val acc:95.13%\n",
            "169/200 train loss:0.0071 train acc:99.79% val loss:0.2705 val acc:94.92%\n",
            "170/200 train loss:0.0076 train acc:99.77% val loss:0.2539 val acc:95.52%\n",
            "171/200 train loss:0.0097 train acc:99.70% val loss:0.2648 val acc:95.29%\n",
            "172/200 train loss:0.0109 train acc:99.63% val loss:0.2553 val acc:94.12%\n",
            "173/200 train loss:0.0097 train acc:99.66% val loss:0.1887 val acc:96.23%\n",
            "174/200 train loss:0.0075 train acc:99.75% val loss:0.2258 val acc:95.24%\n",
            "175/200 train loss:0.0079 train acc:99.72% val loss:0.4571 val acc:93.04%\n",
            "176/200 train loss:0.0068 train acc:99.78% val loss:0.2490 val acc:95.62%\n",
            "177/200 train loss:0.0090 train acc:99.71% val loss:0.1892 val acc:96.29%\n",
            "178/200 train loss:0.0078 train acc:99.75% val loss:0.2055 val acc:96.20%\n",
            "179/200 train loss:0.0060 train acc:99.81% val loss:0.2354 val acc:95.65%\n",
            "180/200 train loss:0.0083 train acc:99.75% val loss:0.2275 val acc:95.11%\n",
            "181/200 train loss:0.0116 train acc:99.59% val loss:0.1938 val acc:96.26%\n",
            "182/200 train loss:0.0091 train acc:99.73% val loss:0.4013 val acc:91.91%\n",
            "183/200 train loss:0.0092 train acc:99.67% val loss:0.1841 val acc:96.34%\n",
            "184/200 train loss:0.0077 train acc:99.76% val loss:0.2023 val acc:96.24%\n",
            "185/200 train loss:0.0094 train acc:99.67% val loss:0.2479 val acc:95.47%\n",
            "186/200 train loss:0.0055 train acc:99.81% val loss:0.3032 val acc:94.97%\n",
            "187/200 train loss:0.0061 train acc:99.79% val loss:0.2281 val acc:95.40%\n",
            "188/200 train loss:0.0090 train acc:99.73% val loss:0.3461 val acc:91.54%\n",
            "189/200 train loss:0.0096 train acc:99.70% val loss:0.2414 val acc:95.60%\n",
            "190/200 train loss:0.0047 train acc:99.82% val loss:0.2521 val acc:95.87%\n",
            "191/200 train loss:0.0059 train acc:99.82% val loss:0.2122 val acc:96.24%\n",
            "192/200 train loss:0.0051 train acc:99.81% val loss:0.2444 val acc:95.30%\n",
            "193/200 train loss:0.0107 train acc:99.63% val loss:0.2413 val acc:95.85%\n",
            "194/200 train loss:0.0074 train acc:99.74% val loss:0.2314 val acc:95.91%\n",
            "195/200 train loss:0.0109 train acc:99.59% val loss:0.1766 val acc:95.73%\n",
            "196/200 train loss:0.0050 train acc:99.83% val loss:0.2069 val acc:96.31%\n",
            "197/200 train loss:0.0080 train acc:99.70% val loss:0.2350 val acc:95.43%\n",
            "198/200 train loss:0.0105 train acc:99.61% val loss:0.2496 val acc:95.85%\n",
            "199/200 train loss:0.0061 train acc:99.78% val loss:0.2543 val acc:95.52%\n",
            "200/200 train loss:0.0064 train acc:99.82% val loss:0.1908 val acc:96.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kny_KG_1I7Y"
      },
      "source": [
        "## EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3vy1vjm1I7Y"
      },
      "outputs": [],
      "source": [
        "my_save_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE'\n",
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = effnetv2_xl().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True,\n",
        "                            num_workers=1)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_train, correct_train = train_audio(model, train_loader, optimizer, device)\n",
        "  loss_val, correct_val = evaluate_audio(model, val_loader, device, criterion = nn.CrossEntropyLoss())\n",
        "\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "  \n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(my_save_path, \n",
        "                                              'audios', \n",
        "                                              'efficient',\n",
        "                                              \"XL-{:.2f}.pth\".format(100 * correct_val/num_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGN88s7v1I7Z"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IRmiIp0hrkk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf70625-5bfc-4a1f-8e05-7b1aed15b83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed_time:185.6604723930359sec\n"
          ]
        }
      ],
      "source": [
        "model_pth = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2/mobileCA/mobile-ca96.35.pth'\n",
        "\n",
        "model_pretrained = mbv2_ca(in_c=8, num_classes=4)\n",
        "model_pretrained.load_state_dict(torch.load(model_pth))\n",
        "model_pretrained.to(device)\n",
        "model_pretrained.eval()\n",
        "\n",
        "voting_dir = '/content/drive/MyDrive/COSRMAL_CHALLENGE/task2/results'\n",
        "audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/test_pub/audio'\n",
        "\n",
        "voting(audio_folder, voting_dir, model_pretrained, device, save_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "nv4DQKgjsHjV",
        "outputId": "6ede9f46-bcf7-48e4-8427-46c4fa87f946"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f2970e67-4342-4a18-89dd-16639a446f81\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_num</th>\n",
              "      <th>file</th>\n",
              "      <th>count_pred</th>\n",
              "      <th>final_pred</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>000000</th>\n",
              "      <td>0</td>\n",
              "      <td>000000</td>\n",
              "      <td>[6, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000001</th>\n",
              "      <td>1</td>\n",
              "      <td>000001</td>\n",
              "      <td>[10, 17, 2, 0]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000002</th>\n",
              "      <td>2</td>\n",
              "      <td>000002</td>\n",
              "      <td>[24, 41, 0, 0]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000003</th>\n",
              "      <td>3</td>\n",
              "      <td>000003</td>\n",
              "      <td>[46, 0, 0, 43]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000004</th>\n",
              "      <td>4</td>\n",
              "      <td>000004</td>\n",
              "      <td>[13, 0, 0, 24]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000223</th>\n",
              "      <td>223</td>\n",
              "      <td>000223</td>\n",
              "      <td>[67, 0, 0, 27]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000224</th>\n",
              "      <td>224</td>\n",
              "      <td>000224</td>\n",
              "      <td>[12, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000225</th>\n",
              "      <td>225</td>\n",
              "      <td>000225</td>\n",
              "      <td>[56, 0, 0, 23]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000226</th>\n",
              "      <td>226</td>\n",
              "      <td>000226</td>\n",
              "      <td>[19, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000227</th>\n",
              "      <td>227</td>\n",
              "      <td>000227</td>\n",
              "      <td>[25, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>228 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2970e67-4342-4a18-89dd-16639a446f81')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f2970e67-4342-4a18-89dd-16639a446f81 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f2970e67-4342-4a18-89dd-16639a446f81');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       data_num  ...                                               pred\n",
              "000000        0  ...                                 [0, 0, 0, 0, 0, 0]\n",
              "000001        1  ...  [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "000002        2  ...  [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
              "000003        3  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, ...\n",
              "000004        4  ...  [0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
              "...         ...  ...                                                ...\n",
              "000223      223  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...\n",
              "000224      224  ...               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "000225      225  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "000226      226  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "000227      227  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[228 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "f = open(os.path.join(voting_dir, \"voting.json\"))\n",
        "vote_js = json.load(f)\n",
        "\n",
        "vote = pd.DataFrame(vote_js).T\n",
        "vote"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vote.to_csv('vote.csv', index=False)"
      ],
      "metadata": {
        "id": "yfG4ArDttrFq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG3xQQqJoQpv",
        "outputId": "4c29b1c8-5005-4e50-8883-9b5659c4d5a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acc: 100.00%\n"
          ]
        }
      ],
      "source": [
        "gt = pd.read_csv('files/train.csv')\n",
        "acc = np.sum(gt['filling_type'].to_numpy() == vote['final_pred'].to_numpy()) / len(gt['filling_type'])\n",
        "print('Acc: {:.2f}%'.format(100 * acc))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "CLLLJI151I7R",
        "ESWl3lO21I7T"
      ],
      "name": "task2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c82347c08dd4398865e6db8022f348b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b7c58ff9e6e24c3bb33017e1fe667829",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_00f4779660c84ef390bf53e08a13e67c",
              "IPY_MODEL_43c97afb2ae2403b96d13234b819658c",
              "IPY_MODEL_f95c362f6a6748f8ba21d3ad5d1aedce"
            ]
          }
        },
        "b7c58ff9e6e24c3bb33017e1fe667829": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00f4779660c84ef390bf53e08a13e67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1089e0445501489d8b7f4932693525ab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_54f3cde0cc1e43c0a8130d1fd60453e2"
          }
        },
        "43c97afb2ae2403b96d13234b819658c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f6355c88af2742e8ae90137efb82f8eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 684,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 684,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_769f390ca88040c2a3a4b6d140e71f72"
          }
        },
        "f95c362f6a6748f8ba21d3ad5d1aedce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f8cebc4e6bb4a76a6c0279f9b461ac5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 684/684 [05:45&lt;00:00,  2.01it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_521c91eb5f6a4c3588bbece9858b1d98"
          }
        },
        "1089e0445501489d8b7f4932693525ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "54f3cde0cc1e43c0a8130d1fd60453e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6355c88af2742e8ae90137efb82f8eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "769f390ca88040c2a3a4b6d140e71f72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f8cebc4e6bb4a76a6c0279f9b461ac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "521c91eb5f6a4c3588bbece9858b1d98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f64cd429fb20492380d23d280ab22a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce1cb672ceb64cc69b1fb309f07f8c64",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_839fb92f74fa4f69af8fb0edd2688570",
              "IPY_MODEL_0734a8234e8140c7b0e961db53d4f8fe",
              "IPY_MODEL_2f31d80b6a0649ed9704eabfbaab92f3"
            ]
          }
        },
        "ce1cb672ceb64cc69b1fb309f07f8c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "839fb92f74fa4f69af8fb0edd2688570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46f3597adefc47549c9c38084150bb4a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_51697d5b862c41abb8ec91c72dce8619"
          }
        },
        "0734a8234e8140c7b0e961db53d4f8fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9985a27fae844e46bed07e11d3fe3620",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 31812,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 31812,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7513f43a3c064b938463446181e7fd63"
          }
        },
        "2f31d80b6a0649ed9704eabfbaab92f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_69a928df30a946f499743feb1a5216da",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 31812/31812 [01:22&lt;00:00, 340.51it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed96e6968b054d51b674ec05ca452d9e"
          }
        },
        "46f3597adefc47549c9c38084150bb4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "51697d5b862c41abb8ec91c72dce8619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9985a27fae844e46bed07e11d3fe3620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7513f43a3c064b938463446181e7fd63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69a928df30a946f499743feb1a5216da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed96e6968b054d51b674ec05ca452d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}