{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioFeatures.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "pQfv4bkWeyrX"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.io.wavfile\n",
        "import time\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gt = pd.read_csv('/content/drive/MyDrive/COSRMAL_CHALLENGE/train.csv')\n",
        "gt.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "OK345_nrpxEU",
        "outputId": "cf3ec167-c7b1-4224-dbda-7734686d00b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-31ee6f9f-dad9-42a4-b9b3-dfeec96e6ea2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>container id</th>\n",
              "      <th>scenario</th>\n",
              "      <th>background</th>\n",
              "      <th>illumination</th>\n",
              "      <th>width at the top</th>\n",
              "      <th>width at the bottom</th>\n",
              "      <th>height</th>\n",
              "      <th>depth</th>\n",
              "      <th>container capacity</th>\n",
              "      <th>container mass</th>\n",
              "      <th>filling type</th>\n",
              "      <th>filling level</th>\n",
              "      <th>filling density</th>\n",
              "      <th>filling mass</th>\n",
              "      <th>object mass</th>\n",
              "      <th>handover starting frame</th>\n",
              "      <th>handover start timestamp</th>\n",
              "      <th>handover hand</th>\n",
              "      <th>action</th>\n",
              "      <th>nframes</th>\n",
              "      <th>folder_num</th>\n",
              "      <th>file_name</th>\n",
              "      <th>num</th>\n",
              "      <th>subject</th>\n",
              "      <th>filling_type</th>\n",
              "      <th>filling_level</th>\n",
              "      <th>back</th>\n",
              "      <th>light</th>\n",
              "      <th>camera_id</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.82</td>\n",
              "      <td>76.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>291576</td>\n",
              "      <td>2</td>\n",
              "      <td>s2_fi2_fu1_b1_l0</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>193.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>3209.397</td>\n",
              "      <td>59.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>118483</td>\n",
              "      <td>7</td>\n",
              "      <td>s0_fi0_fu0_b0_l0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>69.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>185.000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>93.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>572008</td>\n",
              "      <td>2</td>\n",
              "      <td>s0_fi3_fu1_b1_l0</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3.40</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1239.840</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>141680</td>\n",
              "      <td>8</td>\n",
              "      <td>s0_fi0_fu0_b1_l0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>91.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>296.000</td>\n",
              "      <td>86.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34</td>\n",
              "      <td>45.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>138681</td>\n",
              "      <td>4</td>\n",
              "      <td>s1_fi1_fu1_b1_l0</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31ee6f9f-dad9-42a4-b9b3-dfeec96e6ea2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-31ee6f9f-dad9-42a4-b9b3-dfeec96e6ea2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-31ee6f9f-dad9-42a4-b9b3-dfeec96e6ea2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   id  container id  scenario  background  ...  light  camera_id  start  end\n",
              "0   0             2         2           1  ...      0          2   0.75  3.5\n",
              "1   1             7         0           0  ...      0          2  -1.00 -1.0\n",
              "2   2             2         0           1  ...      0          2   3.40  6.5\n",
              "3   3             8         0           1  ...      0          2  -1.00 -1.0\n",
              "4   4             4         1           1  ...      0          2   0.75  1.8\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioProcessing():\n",
        "    def __init__(self,sample_rate,signal,frame_length_t=0.025,frame_stride_t=0.01,nfilt =64):\n",
        "        \n",
        "        self.sample_rate=sample_rate\n",
        "        self.signal = signal\n",
        "        self.frame_length_t=frame_length_t\n",
        "        self.frame_stride_t=frame_stride_t\n",
        "        self.signal_length_t=float(signal.shape[0]/sample_rate)\n",
        "        self.frame_length=int(round(frame_length_t * sample_rate)) #number of samples\n",
        "        self.frame_step=int(round(frame_stride_t * sample_rate))\n",
        "        self.signal_length = signal.shape[0]\n",
        "        self.nfilt=nfilt\n",
        "        self.num_frames = int(np.ceil(float(np.abs(self.signal_length - self.frame_length)) / self.frame_step))\n",
        "        self.pad_signal_length=self.num_frames * self.frame_step + self.frame_length\n",
        "        self.NFFT=512\n",
        "        \n",
        "    def cal_frames(self):\n",
        "        z = np.zeros([self.pad_signal_length - self.signal_length,8])\n",
        "        pad_signal = np.concatenate([self.signal, z], 0)\n",
        "        indices = np.tile(np.arange(0, self.frame_length), (self.num_frames, 1)) + np.tile(np.arange(0, self.num_frames * self.frame_step, self.frame_step), (self.frame_length, 1)).T\n",
        "        frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
        "        return frames\n",
        "        \n",
        "    def calc_MFCC(self):\n",
        "        # 291576\n",
        "        pre_emphasis=0.97\n",
        "\n",
        "        # (n,8)\n",
        "        emphasized_signal=np.concatenate([self.signal[0,:].reshape([1,-1]),  self.signal[1:,:] - pre_emphasis * self.signal[:-1,:]], 0)\n",
        "        z = np.zeros([self.pad_signal_length - self.signal_length,8])\n",
        "        pad_signal = np.concatenate([emphasized_signal, z], 0)\n",
        "        indices = np.tile(np.arange(0, self.frame_length), (self.num_frames, 1)) + np.tile(np.arange(0, self.num_frames * self.frame_step, self.frame_step), (self.frame_length, 1)).T\n",
        "        frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
        "        frames=frames*np.hamming(self.frame_length).reshape(1,-1,1)\n",
        "        frames=frames.transpose(0,2,1)\n",
        "        mag_frames = np.absolute(np.fft.rfft(frames,self.NFFT))\n",
        "        pow_frames = ((1.0 / self.NFFT) * ((mag_frames) ** 2))\n",
        "        filter_banks = np.dot(pow_frames, self.cal_fbank().T)\n",
        "        filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
        "        filter_banks = 20 * np.log10(filter_banks)  # dB\n",
        "        filter_banks =filter_banks.transpose(0,2,1)\n",
        "        \n",
        "        return filter_banks\n",
        "           \n",
        "    def cal_fbank(self):\n",
        "        \n",
        "        low_freq_mel = 0\n",
        "        high_freq_mel = (2595 * np.log10(1 + (self.sample_rate / 2) / 700))  \n",
        "        mel_points = np.linspace(low_freq_mel, high_freq_mel, self.nfilt + 2)  \n",
        "        hz_points = (700 * (10**(mel_points / 2595) - 1)) \n",
        "        bin = np.floor((self.NFFT + 1) * hz_points / self.sample_rate)\n",
        "        fbank = np.zeros((self.nfilt, int(np.floor(self.NFFT / 2 + 1))))\n",
        "        for m in range(1, self.nfilt + 1):\n",
        "            f_m_minus = int(bin[m - 1])   # left\n",
        "            f_m = int(bin[m])             # center\n",
        "            f_m_plus = int(bin[m + 1])    # right\n",
        "\n",
        "            for k in range(f_m_minus, f_m):\n",
        "                fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
        "            for k in range(f_m, f_m_plus):\n",
        "                fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
        "        return fbank"
      ],
      "metadata": {
        "id": "E4dlxplIpfCQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE/'\n",
        "audio_folder = '/content/drive/MyDrive/COSRMAL_CHALLENGE/train/audio/'\n",
        "os.makedirs(os.path.join(base_path, 'audios'), exist_ok=True)\n",
        "mfcc_path = (os.path.join(base_path, 'audios', 'mfcc'))\n",
        "raw_path = (os.path.join(base_path, 'audios', 'raw'))\n",
        "# os.makedirs(mfcc_path,exist_ok=True)\n",
        "# os.makedirs(raw_path,exist_ok=True)\n",
        "\n",
        "# audio_paths = [os.path.join(audio_folder, path) for path in sorted(os.listdir(audio_folder))]\n",
        "# save_size=64\n",
        "# ratio_step = 0.25\n",
        "# count = 0\n",
        "# pouring_or_shaking_list = []\n",
        "# file_idx_list = []\n",
        "# filling_type_list = []\n",
        "# pbar = tqdm(total=len(audio_paths))\n",
        "\n",
        "# for i, path in enumerate(audio_paths):\n",
        "#   id = i\n",
        "#   start_time = gt[gt.id==id]['start'].item()\n",
        "#   end_time = gt[gt.id==id]['end'].item()\n",
        "#   filling_type = gt[gt.id==id]['filling_type'].item()\n",
        "#   sample_rate, signal = scipy.io.wavfile.read(path)\n",
        "#   ap = AudioProcessing(sample_rate,signal,nfilt=save_size)\n",
        "#   mfcc = ap.calc_MFCC()\n",
        "#   raw_frames = ap.cal_frames()\n",
        "#   mfcc_length=mfcc.shape[0]\n",
        "\n",
        "#   if mfcc_length < save_size:\n",
        "#     print(\"file {} is too short\".format(id))\n",
        "#   else:\n",
        "#     f_step=int(mfcc.shape[1]*ratio_step)\n",
        "#     f_length=mfcc.shape[1]\n",
        "#     save_mfcc_num=int(np.ceil(float(np.abs(mfcc_length - save_size)) / f_step))\n",
        "\n",
        "#     for i in range(save_mfcc_num):\n",
        "#       count += 1\n",
        "#       tmp_mfcc = mfcc[i*f_step:save_size+i*f_step,: ,:]\n",
        "#       if start_time == -1:\n",
        "#           pouring_or_shaking_list.append(0)\n",
        "#       elif start_time/ap.signal_length_t*mfcc_length<i*f_step+f_length*0.75 and end_time/ap.signal_length_t*mfcc_length>i*f_step+f_length*0.25:\n",
        "#           pouring_or_shaking_list.append(1) \n",
        "#       else:\n",
        "#           pouring_or_shaking_list.append(0)\n",
        "      \n",
        "#       filling_type_list.append(filling_type)\n",
        "#       file_idx_list.append(id)\n",
        "\n",
        "#       np.save(os.path.join(mfcc_path, \"{0:06d}\".format(count)), tmp_mfcc)\n",
        "#   pbar.update()\n",
        "\n",
        "\n",
        "# np.save(os.path.join(base_path, 'audios', 'pouring_or_shaking'), np.array(pouring_or_shaking_list) )\n",
        "# np.save(os.path.join(base_path, 'audios', 'filling_type'), np.array(filling_type_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb-WRGUQp1kI",
        "outputId": "f5d05c56-19fa-4c12-e2db-3d207f189b86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 684/684 [09:43<00:00,  1.04s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filling_type = np.load(os.path.join(base_path, 'audios', 'filling_type.npy'))\n",
        "pouring_or_shaking = np.load(os.path.join(base_path,  'audios', 'pouring_or_shaking.npy'))\n",
        "\n",
        "label = filling_type * pouring_or_shaking"
      ],
      "metadata": {
        "id": "dNahj5LNbU4A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class audioDataSet(Dataset):\n",
        "  def __init__(self,root_pth,test=False,transform = None):\n",
        "    print(\"Dataset initializing...\")\n",
        "    class_num=4\n",
        "    self.audio_pth = os.path.join(root_pth, 'audios', 'mfcc')\n",
        "    filling_type = np.load(os.path.join(root_pth, 'audios', 'filling_type.npy'))\n",
        "    pouring_or_shaking = np.load(os.path.join(root_pth,  'audios', 'pouring_or_shaking.npy'))\n",
        "    self.label = filling_type * pouring_or_shaking\n",
        "    self.is_test=test\n",
        "    self.each_class_size = []\n",
        "    for i in range(class_num):\n",
        "        self.each_class_size.append(np.count_nonzero(self.label==i))\n",
        "    mx=0\n",
        "    mn=1000\n",
        "    for idx in tqdm(range(self.label.shape[0])):\n",
        "        data=np.load(os.path.join(self.audio_pth, \"{0:06d}\".format(idx+1) + '.npy'), allow_pickle=True)\n",
        "        tmp_max=np.max(data)\n",
        "        tmp_min=np.min(data)\n",
        "        if mx<tmp_max:\n",
        "            mx=tmp_max\n",
        "        if mn>tmp_min:\n",
        "            mn=tmp_min\n",
        "    self.mn=mn\n",
        "    self.mx=mx\n",
        "  def __len__(self):\n",
        "    return self.label.shape[0]\n",
        "  def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        lbl = -1\n",
        "\n",
        "        if self.is_test is False:\n",
        "            lbl = self.label[idx]\n",
        "        data=np.load(os.path.join(self.audio_pth, \"{0:06d}\".format(idx+1) + '.npy'), allow_pickle=True)\n",
        "        data= (data-self.mn)/(self.mx-self.mn)\n",
        "        data=data.transpose(2,0,1)\n",
        "        data=torch.from_numpy(data.astype(np.float32))\n",
        "        return data , lbl\n",
        "            \n",
        "  def get_each_class_size(self):\n",
        "    return np.array(self.each_class_size)"
      ],
      "metadata": {
        "id": "Cofl-UhojZwq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "model = Net(8, 4).to(device)\n",
        "summary(model, input_size=(32, 8, 64, 64))"
      ],
      "metadata": {
        "id": "vdYjOKUXw4uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv01 = nn.Conv2d(8, 32, 3,padding=1)#64\n",
        "        self.conv02 = nn.Conv2d(32, 32, 3,padding=1)#64\n",
        "        self.bn1=nn.BatchNorm2d(32)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)#32\n",
        "\n",
        "        self.conv03 = nn.Conv2d(32, 64, 3,padding=1)#32\n",
        "        self.conv04 = nn.Conv2d(64, 64, 3,padding=1)#32\n",
        "        self.bn2=nn.BatchNorm2d(64)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)#16\n",
        "\n",
        "        self.conv05 = nn.Conv2d(64, 128, 3,padding=1)#16\n",
        "        self.conv06 = nn.Conv2d(128, 128, 3,padding=1)#16\n",
        "        self.conv07 = nn.Conv2d(128, 128, 3,padding=1)#16\n",
        "        self.bn3=nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)#8\n",
        "\n",
        "        self.conv08 = nn.Conv2d(128, 256, 3,padding=1)#8\n",
        "        self.conv09 = nn.Conv2d(256, 256, 3,padding=1)#8\n",
        "        self.conv10 = nn.Conv2d(256, 256, 3,padding=1)#8\n",
        "        self.bn4=nn.BatchNorm2d(256)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)#4\n",
        "\n",
        "        self.conv11 = nn.Conv2d(256, 256, 3,padding=1)#4\n",
        "        self.conv12 = nn.Conv2d(256, 256, 3,padding=1)#4\n",
        "        self.conv13 = nn.Conv2d(256, 256, 3,padding=1)#4\n",
        "        self.bn5=nn.BatchNorm2d(256)\n",
        "        self.pool5 = nn.MaxPool2d(2, 2)#2\n",
        "\n",
        "       \n",
        "        self.fc1 = nn.Linear(256 * 2 * 2, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, 4)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv01(x))\n",
        "        x = F.relu(self.conv02(x))\n",
        "        x = self.pool1(self.bn1(x))\n",
        "\n",
        "        x = F.relu(self.conv03(x))\n",
        "        x = F.relu(self.conv04(x))\n",
        "        x = self.pool2(self.bn2(x))\n",
        "\n",
        "        x = F.relu(self.conv05(x))\n",
        "        x = F.relu(self.conv06(x))\n",
        "        x = F.relu(self.conv07(x))\n",
        "        x = self.pool3(self.bn3(x))\n",
        "\n",
        "        x = F.relu(self.conv08(x))\n",
        "        x = F.relu(self.conv09(x))\n",
        "        x = F.relu(self.conv10(x))\n",
        "        x = self.pool4(self.bn4(x))\n",
        "\n",
        "        x = F.relu(self.conv11(x))\n",
        "        x = F.relu(self.conv12(x))\n",
        "        x = F.relu(self.conv13(x))\n",
        "        x = self.pool5(self.bn5(x))\n",
        "\n",
        "\n",
        "        x = x.view(-1, 256 * 2 * 2)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return self.softmax(x)   "
      ],
      "metadata": {
        "id": "UB-Rc5Sd_VA4"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydataset = audioDataSet(base_path)"
      ],
      "metadata": {
        "id": "hTAZbi9Es7KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion = nn.CrossEntropyLoss()):\n",
        "  model.train()\n",
        "  loss_train = 0.0\n",
        "  correct_train = 0.0\n",
        "  num_train = len(train_loader)\n",
        "  for batch_idx, (audio, target) in enumerate(train_loader):\n",
        "    audio = audio.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(audio)\n",
        "\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_train += loss.item() / num_train\n",
        "    _, preds=torch.max(outputs,1)\n",
        "    correct_train+=torch.sum(preds==target).item()\n",
        "  \n",
        "  return loss_train, correct_train\n",
        "\n",
        "\n",
        "def evaluate(model, testloader, criterion = nn.CrossEntropyLoss()):\n",
        "  model.eval()\n",
        "  loss_test = 0\n",
        "  correct_test=0\n",
        "  num_val = len(testloader)\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (audio, target) in enumerate(testloader):\n",
        "      audio = audio.to(device)\n",
        "      target = target.to(device)\n",
        "      outputs = model.forward(audio)\n",
        "      loss = criterion(outputs, target)\n",
        "      loss_test += loss.item() / num_val\n",
        "      _, preds=torch.max(outputs,1)\n",
        "      correct_test+=torch.sum(preds==target).item()\n",
        "  \n",
        "  return loss_test, correct_test\n"
      ],
      "metadata": {
        "id": "kUH-0jkOqI2x"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-5\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"Epoch {}/{} train loss:{:.4f} train acc:{:.2f}% \".format(epoch+1,epochs, loss_train, 100 * correct_train/num_train))\n",
        "  print(\"Epoch {}/{} val loss:{:.4f} val acc:{:.2f}% \".format(epoch+1,epochs, loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  if loss_val < best_loss:\n",
        "    best_loss = loss_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_loss.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"best_val.pth\"))\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oad1iOtNl01a",
        "outputId": "c033f748-1dcd-4f21-93cd-1d1ac609d313"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200 train loss:1.0360 train acc:73.44% \n",
            "Epoch 1/200 val loss:0.9534 val acc:79.19% \n",
            "Epoch 2/200 train loss:0.9431 train acc:80.36% \n",
            "Epoch 2/200 val loss:0.9258 val acc:81.64% \n",
            "Epoch 3/200 train loss:0.9241 train acc:81.98% \n",
            "Epoch 3/200 val loss:0.9357 val acc:80.56% \n",
            "Epoch 4/200 train loss:0.9132 train acc:82.80% \n",
            "Epoch 4/200 val loss:0.9041 val acc:84.02% \n",
            "Epoch 5/200 train loss:0.8858 train acc:86.28% \n",
            "Epoch 5/200 val loss:1.0089 val acc:72.62% \n",
            "Epoch 6/200 train loss:0.8672 train acc:87.78% \n",
            "Epoch 6/200 val loss:0.8978 val acc:84.50% \n",
            "Epoch 7/200 train loss:0.8605 train acc:88.34% \n",
            "Epoch 7/200 val loss:0.9934 val acc:74.57% \n",
            "Epoch 8/200 train loss:0.8526 train acc:89.10% \n",
            "Epoch 8/200 val loss:0.8889 val acc:85.34% \n",
            "Epoch 9/200 train loss:0.8470 train acc:89.68% \n",
            "Epoch 9/200 val loss:0.8650 val acc:87.79% \n",
            "Epoch 10/200 train loss:0.8426 train acc:90.15% \n",
            "Epoch 10/200 val loss:0.8547 val acc:88.86% \n",
            "Epoch 11/200 train loss:0.8390 train acc:90.55% \n",
            "Epoch 11/200 val loss:0.8496 val acc:89.31% \n",
            "Epoch 12/200 train loss:0.8368 train acc:90.66% \n",
            "Epoch 12/200 val loss:0.8499 val acc:89.23% \n",
            "Epoch 13/200 train loss:0.8327 train acc:91.15% \n",
            "Epoch 13/200 val loss:0.9411 val acc:80.03% \n",
            "Epoch 14/200 train loss:0.8312 train acc:91.28% \n",
            "Epoch 14/200 val loss:0.8509 val acc:89.09% \n",
            "Epoch 15/200 train loss:0.8292 train acc:91.50% \n",
            "Epoch 15/200 val loss:0.9029 val acc:84.03% \n",
            "Epoch 16/200 train loss:0.8248 train acc:91.90% \n",
            "Epoch 16/200 val loss:0.9373 val acc:80.34% \n",
            "Epoch 17/200 train loss:0.8230 train acc:92.13% \n",
            "Epoch 17/200 val loss:0.8491 val acc:89.30% \n",
            "Epoch 18/200 train loss:0.8216 train acc:92.22% \n",
            "Epoch 18/200 val loss:0.8611 val acc:88.28% \n",
            "Epoch 19/200 train loss:0.8209 train acc:92.26% \n",
            "Epoch 19/200 val loss:0.8812 val acc:85.97% \n",
            "Epoch 20/200 train loss:0.8165 train acc:92.81% \n",
            "Epoch 20/200 val loss:0.8674 val acc:87.46% \n",
            "Epoch 21/200 train loss:0.8144 train acc:92.92% \n",
            "Epoch 21/200 val loss:0.8563 val acc:88.59% \n",
            "Epoch 22/200 train loss:0.8153 train acc:92.82% \n",
            "Epoch 22/200 val loss:0.9562 val acc:78.34% \n",
            "Epoch 23/200 train loss:0.8121 train acc:93.14% \n",
            "Epoch 23/200 val loss:0.8666 val acc:87.49% \n",
            "Epoch 24/200 train loss:0.8111 train acc:93.31% \n",
            "Epoch 24/200 val loss:0.8641 val acc:87.87% \n",
            "Epoch 25/200 train loss:0.8098 train acc:93.45% \n",
            "Epoch 25/200 val loss:0.8888 val acc:85.18% \n",
            "Epoch 26/200 train loss:0.8054 train acc:93.87% \n",
            "Epoch 26/200 val loss:0.8687 val acc:87.43% \n",
            "Epoch 27/200 train loss:0.8057 train acc:93.83% \n",
            "Epoch 27/200 val loss:0.8900 val acc:85.01% \n",
            "Epoch 28/200 train loss:0.8062 train acc:93.75% \n",
            "Epoch 28/200 val loss:0.8724 val acc:87.08% \n",
            "Epoch 29/200 train loss:0.8039 train acc:94.01% \n",
            "Epoch 29/200 val loss:0.8435 val acc:89.93% \n",
            "Epoch 30/200 train loss:0.8017 train acc:94.19% \n",
            "Epoch 30/200 val loss:0.9274 val acc:81.25% \n",
            "Epoch 31/200 train loss:0.7998 train acc:94.44% \n",
            "Epoch 31/200 val loss:0.9214 val acc:81.90% \n",
            "Epoch 32/200 train loss:0.8003 train acc:94.32% \n",
            "Epoch 32/200 val loss:0.8517 val acc:89.09% \n",
            "Epoch 33/200 train loss:0.7989 train acc:94.49% \n",
            "Epoch 33/200 val loss:0.8993 val acc:84.28% \n",
            "Epoch 34/200 train loss:0.7974 train acc:94.66% \n",
            "Epoch 34/200 val loss:0.9431 val acc:79.74% \n",
            "Epoch 35/200 train loss:0.7999 train acc:94.38% \n",
            "Epoch 35/200 val loss:0.8753 val acc:86.74% \n",
            "Epoch 36/200 train loss:0.7978 train acc:94.62% \n",
            "Epoch 36/200 val loss:0.8495 val acc:89.34% \n",
            "Epoch 37/200 train loss:0.7952 train acc:94.86% \n",
            "Epoch 37/200 val loss:0.9061 val acc:83.72% \n",
            "Epoch 38/200 train loss:0.7941 train acc:94.96% \n",
            "Epoch 38/200 val loss:0.8587 val acc:88.40% \n",
            "Epoch 39/200 train loss:0.7956 train acc:94.81% \n",
            "Epoch 39/200 val loss:0.8622 val acc:88.06% \n",
            "Epoch 40/200 train loss:0.7923 train acc:95.20% \n",
            "Epoch 40/200 val loss:0.8484 val acc:89.39% \n",
            "Epoch 41/200 train loss:0.7937 train acc:95.02% \n",
            "Epoch 41/200 val loss:0.8468 val acc:89.56% \n",
            "Epoch 42/200 train loss:0.7937 train acc:95.00% \n",
            "Epoch 42/200 val loss:0.8460 val acc:89.66% \n",
            "Epoch 43/200 train loss:0.7906 train acc:95.33% \n",
            "Epoch 43/200 val loss:0.8663 val acc:87.55% \n",
            "Epoch 44/200 train loss:0.7916 train acc:95.21% \n",
            "Epoch 44/200 val loss:0.8472 val acc:89.50% \n",
            "Epoch 45/200 train loss:0.7925 train acc:95.11% \n",
            "Epoch 45/200 val loss:0.9376 val acc:80.42% \n",
            "Epoch 46/200 train loss:0.7894 train acc:95.45% \n",
            "Epoch 46/200 val loss:0.8531 val acc:89.01% \n",
            "Epoch 47/200 train loss:0.7907 train acc:95.32% \n",
            "Epoch 47/200 val loss:0.8548 val acc:88.84% \n",
            "Epoch 48/200 train loss:0.7904 train acc:95.33% \n",
            "Epoch 48/200 val loss:0.8680 val acc:87.38% \n",
            "Epoch 49/200 train loss:0.7888 train acc:95.50% \n",
            "Epoch 49/200 val loss:0.8672 val acc:87.57% \n",
            "Epoch 50/200 train loss:0.7894 train acc:95.45% \n",
            "Epoch 50/200 val loss:0.8378 val acc:90.46% \n",
            "Epoch 51/200 train loss:0.7879 train acc:95.60% \n",
            "Epoch 51/200 val loss:0.8643 val acc:87.84% \n",
            "Epoch 52/200 train loss:0.7893 train acc:95.45% \n",
            "Epoch 52/200 val loss:0.8572 val acc:88.61% \n",
            "Epoch 53/200 train loss:0.7871 train acc:95.66% \n",
            "Epoch 53/200 val loss:0.8453 val acc:89.67% \n",
            "Epoch 54/200 train loss:0.7874 train acc:95.60% \n",
            "Epoch 54/200 val loss:0.9923 val acc:74.85% \n",
            "Epoch 55/200 train loss:0.7882 train acc:95.53% \n",
            "Epoch 55/200 val loss:0.8462 val acc:89.78% \n",
            "Epoch 56/200 train loss:0.7863 train acc:95.72% \n",
            "Epoch 56/200 val loss:0.8948 val acc:84.76% \n",
            "Epoch 57/200 train loss:0.7873 train acc:95.61% \n",
            "Epoch 57/200 val loss:0.8378 val acc:90.48% \n",
            "Epoch 58/200 train loss:0.7843 train acc:95.95% \n",
            "Epoch 58/200 val loss:0.9086 val acc:83.28% \n",
            "Epoch 59/200 train loss:0.7836 train acc:96.05% \n",
            "Epoch 59/200 val loss:0.9462 val acc:79.44% \n",
            "Epoch 60/200 train loss:0.7869 train acc:95.66% \n",
            "Epoch 60/200 val loss:0.8373 val acc:90.46% \n",
            "Epoch 61/200 train loss:0.7848 train acc:95.89% \n",
            "Epoch 61/200 val loss:0.8842 val acc:85.87% \n",
            "Epoch 62/200 train loss:0.7863 train acc:95.72% \n",
            "Epoch 62/200 val loss:0.8810 val acc:86.20% \n",
            "Epoch 63/200 train loss:0.7828 train acc:96.10% \n",
            "Epoch 63/200 val loss:0.8509 val acc:89.08% \n",
            "Epoch 64/200 train loss:0.7847 train acc:95.92% \n",
            "Epoch 64/200 val loss:0.8609 val acc:88.29% \n",
            "Epoch 65/200 train loss:0.7868 train acc:95.68% \n",
            "Epoch 65/200 val loss:0.8899 val acc:85.31% \n",
            "Epoch 66/200 train loss:0.7846 train acc:95.92% \n",
            "Epoch 66/200 val loss:0.8507 val acc:89.20% \n",
            "Epoch 67/200 train loss:0.7819 train acc:96.20% \n",
            "Epoch 67/200 val loss:0.8515 val acc:89.11% \n",
            "Epoch 68/200 train loss:0.7820 train acc:96.17% \n",
            "Epoch 68/200 val loss:0.8586 val acc:88.34% \n",
            "Epoch 69/200 train loss:0.7822 train acc:96.18% \n",
            "Epoch 69/200 val loss:0.8477 val acc:89.53% \n",
            "Epoch 70/200 train loss:0.7822 train acc:96.15% \n",
            "Epoch 70/200 val loss:0.8470 val acc:89.61% \n",
            "Epoch 71/200 train loss:0.7816 train acc:96.22% \n",
            "Epoch 71/200 val loss:0.8529 val acc:89.01% \n",
            "Epoch 72/200 train loss:0.7817 train acc:96.20% \n",
            "Epoch 72/200 val loss:0.8500 val acc:89.30% \n",
            "Epoch 73/200 train loss:0.7813 train acc:96.24% \n",
            "Epoch 73/200 val loss:0.8852 val acc:85.64% \n",
            "Epoch 74/200 train loss:0.7810 train acc:96.28% \n",
            "Epoch 74/200 val loss:0.9094 val acc:83.23% \n",
            "Epoch 75/200 train loss:0.7812 train acc:96.24% \n",
            "Epoch 75/200 val loss:0.8414 val acc:90.08% \n",
            "Epoch 76/200 train loss:0.7792 train acc:96.45% \n",
            "Epoch 76/200 val loss:0.8361 val acc:90.66% \n",
            "Epoch 77/200 train loss:0.7795 train acc:96.41% \n",
            "Epoch 77/200 val loss:0.8692 val acc:87.40% \n",
            "Epoch 78/200 train loss:0.7811 train acc:96.25% \n",
            "Epoch 78/200 val loss:0.8439 val acc:89.85% \n",
            "Epoch 79/200 train loss:0.7792 train acc:96.44% \n",
            "Epoch 79/200 val loss:0.8452 val acc:89.78% \n",
            "Epoch 80/200 train loss:0.7810 train acc:96.25% \n",
            "Epoch 80/200 val loss:0.8821 val acc:85.95% \n",
            "Epoch 81/200 train loss:0.7811 train acc:96.27% \n",
            "Epoch 81/200 val loss:0.8502 val acc:89.27% \n",
            "Epoch 82/200 train loss:0.7788 train acc:96.49% \n",
            "Epoch 82/200 val loss:0.8486 val acc:89.31% \n",
            "Epoch 83/200 train loss:0.7796 train acc:96.40% \n",
            "Epoch 83/200 val loss:0.8372 val acc:90.52% \n",
            "Epoch 84/200 train loss:0.7778 train acc:96.60% \n",
            "Epoch 84/200 val loss:0.8523 val acc:89.01% \n",
            "Epoch 85/200 train loss:0.7783 train acc:96.52% \n",
            "Epoch 85/200 val loss:0.8463 val acc:89.58% \n",
            "Epoch 86/200 train loss:0.7795 train acc:96.42% \n",
            "Epoch 86/200 val loss:0.8651 val acc:87.66% \n",
            "Epoch 87/200 train loss:0.7785 train acc:96.50% \n",
            "Epoch 87/200 val loss:0.8349 val acc:90.77% \n",
            "Epoch 88/200 train loss:0.7772 train acc:96.65% \n",
            "Epoch 88/200 val loss:0.8455 val acc:89.75% \n",
            "Epoch 89/200 train loss:0.7784 train acc:96.52% \n",
            "Epoch 89/200 val loss:0.8466 val acc:89.66% \n",
            "Epoch 90/200 train loss:0.7769 train acc:96.67% \n",
            "Epoch 90/200 val loss:0.8400 val acc:90.33% \n",
            "Epoch 91/200 train loss:0.7753 train acc:96.84% \n",
            "Epoch 91/200 val loss:0.8592 val acc:88.20% \n",
            "Epoch 92/200 train loss:0.7776 train acc:96.57% \n",
            "Epoch 92/200 val loss:0.8464 val acc:89.60% \n",
            "Epoch 93/200 train loss:0.7758 train acc:96.81% \n",
            "Epoch 93/200 val loss:0.8465 val acc:89.60% \n",
            "Epoch 94/200 train loss:0.7771 train acc:96.64% \n",
            "Epoch 94/200 val loss:0.8441 val acc:89.86% \n",
            "Epoch 95/200 train loss:0.7760 train acc:96.74% \n",
            "Epoch 95/200 val loss:0.9288 val acc:81.08% \n",
            "Epoch 96/200 train loss:0.7787 train acc:96.51% \n",
            "Epoch 96/200 val loss:0.8500 val acc:89.31% \n",
            "Epoch 97/200 train loss:0.7767 train acc:96.70% \n",
            "Epoch 97/200 val loss:0.8453 val acc:89.85% \n",
            "Epoch 98/200 train loss:0.7780 train acc:96.54% \n",
            "Epoch 98/200 val loss:1.0982 val acc:64.04% \n",
            "Epoch 99/200 train loss:0.7754 train acc:96.81% \n",
            "Epoch 99/200 val loss:0.8364 val acc:90.57% \n",
            "Epoch 100/200 train loss:0.7761 train acc:96.78% \n",
            "Epoch 100/200 val loss:0.8379 val acc:90.49% \n",
            "Epoch 101/200 train loss:0.7753 train acc:96.84% \n",
            "Epoch 101/200 val loss:0.8405 val acc:90.29% \n",
            "Epoch 102/200 train loss:0.7745 train acc:96.93% \n",
            "Epoch 102/200 val loss:0.8370 val acc:90.63% \n",
            "Epoch 103/200 train loss:0.7753 train acc:96.81% \n",
            "Epoch 103/200 val loss:0.8485 val acc:89.49% \n",
            "Epoch 104/200 train loss:0.7736 train acc:97.00% \n",
            "Epoch 104/200 val loss:0.8416 val acc:90.11% \n",
            "Epoch 105/200 train loss:0.7737 train acc:97.02% \n",
            "Epoch 105/200 val loss:0.8423 val acc:90.11% \n",
            "Epoch 106/200 train loss:0.7752 train acc:96.85% \n",
            "Epoch 106/200 val loss:0.8812 val acc:86.08% \n",
            "Epoch 107/200 train loss:0.7770 train acc:96.68% \n",
            "Epoch 107/200 val loss:0.8585 val acc:88.42% \n",
            "Epoch 108/200 train loss:0.7754 train acc:96.81% \n",
            "Epoch 108/200 val loss:0.8552 val acc:88.68% \n",
            "Epoch 109/200 train loss:0.7760 train acc:96.75% \n",
            "Epoch 109/200 val loss:0.8396 val acc:90.26% \n",
            "Epoch 110/200 train loss:0.7746 train acc:96.90% \n",
            "Epoch 110/200 val loss:0.8346 val acc:90.82% \n",
            "Epoch 111/200 train loss:0.7726 train acc:97.10% \n",
            "Epoch 111/200 val loss:0.8397 val acc:90.30% \n",
            "Epoch 112/200 train loss:0.7753 train acc:96.83% \n",
            "Epoch 112/200 val loss:0.8417 val acc:90.05% \n",
            "Epoch 113/200 train loss:0.7725 train acc:97.14% \n",
            "Epoch 113/200 val loss:0.8344 val acc:90.93% \n",
            "Epoch 114/200 train loss:0.7720 train acc:97.18% \n",
            "Epoch 114/200 val loss:0.8601 val acc:88.23% \n",
            "Epoch 115/200 train loss:0.7728 train acc:97.08% \n",
            "Epoch 115/200 val loss:0.8377 val acc:90.52% \n",
            "Epoch 116/200 train loss:0.7731 train acc:97.05% \n",
            "Epoch 116/200 val loss:0.8358 val acc:90.62% \n",
            "Epoch 117/200 train loss:0.7729 train acc:97.08% \n",
            "Epoch 117/200 val loss:0.8495 val acc:89.36% \n",
            "Epoch 118/200 train loss:0.7749 train acc:96.86% \n",
            "Epoch 118/200 val loss:0.8459 val acc:89.74% \n",
            "Epoch 119/200 train loss:0.7740 train acc:96.96% \n",
            "Epoch 119/200 val loss:0.8355 val acc:90.70% \n",
            "Epoch 120/200 train loss:0.7736 train acc:97.00% \n",
            "Epoch 120/200 val loss:0.8610 val acc:88.04% \n",
            "Epoch 121/200 train loss:0.7738 train acc:97.01% \n",
            "Epoch 121/200 val loss:0.8367 val acc:90.66% \n",
            "Epoch 122/200 train loss:0.7717 train acc:97.19% \n",
            "Epoch 122/200 val loss:0.8313 val acc:91.15% \n",
            "Epoch 123/200 train loss:0.7713 train acc:97.23% \n",
            "Epoch 123/200 val loss:0.8518 val acc:88.92% \n",
            "Epoch 124/200 train loss:0.7714 train acc:97.23% \n",
            "Epoch 124/200 val loss:0.8609 val acc:88.20% \n",
            "Epoch 125/200 train loss:0.7718 train acc:97.21% \n",
            "Epoch 125/200 val loss:0.8441 val acc:89.96% \n",
            "Epoch 126/200 train loss:0.7703 train acc:97.35% \n",
            "Epoch 126/200 val loss:0.8379 val acc:90.44% \n",
            "Epoch 127/200 train loss:0.7707 train acc:97.32% \n",
            "Epoch 127/200 val loss:0.8412 val acc:90.16% \n",
            "Epoch 128/200 train loss:0.7718 train acc:97.20% \n",
            "Epoch 128/200 val loss:0.8416 val acc:90.05% \n",
            "Epoch 129/200 train loss:0.7737 train acc:96.98% \n",
            "Epoch 129/200 val loss:0.8949 val acc:84.71% \n",
            "Epoch 130/200 train loss:0.7725 train acc:97.12% \n",
            "Epoch 130/200 val loss:0.9009 val acc:84.14% \n",
            "Epoch 131/200 train loss:0.7717 train acc:97.19% \n",
            "Epoch 131/200 val loss:0.8399 val acc:90.41% \n",
            "Epoch 132/200 train loss:0.7704 train acc:97.33% \n",
            "Epoch 132/200 val loss:0.8673 val acc:87.52% \n",
            "Epoch 133/200 train loss:0.7725 train acc:97.10% \n",
            "Epoch 133/200 val loss:0.9922 val acc:74.71% \n",
            "Epoch 134/200 train loss:0.7711 train acc:97.26% \n",
            "Epoch 134/200 val loss:0.8365 val acc:90.65% \n",
            "Epoch 135/200 train loss:0.7698 train acc:97.39% \n",
            "Epoch 135/200 val loss:0.8395 val acc:90.32% \n",
            "Epoch 136/200 train loss:0.7706 train acc:97.32% \n",
            "Epoch 136/200 val loss:0.8370 val acc:90.55% \n",
            "Epoch 137/200 train loss:0.7712 train acc:97.25% \n",
            "Epoch 137/200 val loss:0.9025 val acc:83.89% \n",
            "Epoch 138/200 train loss:0.7697 train acc:97.41% \n",
            "Epoch 138/200 val loss:0.8335 val acc:90.90% \n",
            "Epoch 139/200 train loss:0.7695 train acc:97.38% \n",
            "Epoch 139/200 val loss:0.8545 val acc:88.84% \n",
            "Epoch 140/200 train loss:0.7700 train acc:97.36% \n",
            "Epoch 140/200 val loss:0.8449 val acc:89.75% \n",
            "Epoch 141/200 train loss:0.7696 train acc:97.40% \n",
            "Epoch 141/200 val loss:0.8430 val acc:89.93% \n",
            "Epoch 142/200 train loss:0.7684 train acc:97.52% \n",
            "Epoch 142/200 val loss:0.8360 val acc:90.65% \n",
            "Epoch 143/200 train loss:0.7683 train acc:97.54% \n",
            "Epoch 143/200 val loss:1.0154 val acc:72.54% \n",
            "Epoch 144/200 train loss:0.7706 train acc:97.30% \n",
            "Epoch 144/200 val loss:0.8374 val acc:90.54% \n",
            "Epoch 145/200 train loss:0.7705 train acc:97.31% \n",
            "Epoch 145/200 val loss:0.8809 val acc:85.98% \n",
            "Epoch 146/200 train loss:0.7700 train acc:97.38% \n",
            "Epoch 146/200 val loss:0.8518 val acc:89.08% \n",
            "Epoch 147/200 train loss:0.7714 train acc:97.21% \n",
            "Epoch 147/200 val loss:0.8347 val acc:90.84% \n",
            "Epoch 148/200 train loss:0.7699 train acc:97.38% \n",
            "Epoch 148/200 val loss:0.8337 val acc:90.92% \n",
            "Epoch 149/200 train loss:0.7694 train acc:97.42% \n",
            "Epoch 149/200 val loss:0.8342 val acc:90.87% \n",
            "Epoch 150/200 train loss:0.7696 train acc:97.42% \n",
            "Epoch 150/200 val loss:0.8486 val acc:89.38% \n",
            "Epoch 151/200 train loss:0.7683 train acc:97.54% \n",
            "Epoch 151/200 val loss:0.8508 val acc:89.22% \n",
            "Epoch 152/200 train loss:0.7672 train acc:97.67% \n",
            "Epoch 152/200 val loss:0.8418 val acc:90.07% \n",
            "Epoch 153/200 train loss:0.7715 train acc:97.22% \n",
            "Epoch 153/200 val loss:0.8484 val acc:89.42% \n",
            "Epoch 154/200 train loss:0.7695 train acc:97.42% \n",
            "Epoch 154/200 val loss:0.8357 val acc:90.66% \n",
            "Epoch 155/200 train loss:0.7686 train acc:97.51% \n",
            "Epoch 155/200 val loss:0.8302 val acc:91.37% \n",
            "Epoch 156/200 train loss:0.7682 train acc:97.56% \n",
            "Epoch 156/200 val loss:0.8541 val acc:88.75% \n",
            "Epoch 157/200 train loss:0.7687 train acc:97.50% \n",
            "Epoch 157/200 val loss:0.8269 val acc:91.62% \n",
            "Epoch 158/200 train loss:0.7682 train acc:97.56% \n",
            "Epoch 158/200 val loss:0.8364 val acc:90.59% \n",
            "Epoch 159/200 train loss:0.7683 train acc:97.52% \n",
            "Epoch 159/200 val loss:0.8285 val acc:91.39% \n",
            "Epoch 160/200 train loss:0.7706 train acc:97.29% \n",
            "Epoch 160/200 val loss:0.8368 val acc:90.57% \n",
            "Epoch 161/200 train loss:0.7687 train acc:97.49% \n",
            "Epoch 161/200 val loss:0.8408 val acc:90.21% \n",
            "Epoch 162/200 train loss:0.7676 train acc:97.61% \n",
            "Epoch 162/200 val loss:0.8290 val acc:91.43% \n",
            "Epoch 163/200 train loss:0.7675 train acc:97.62% \n",
            "Epoch 163/200 val loss:0.8245 val acc:91.87% \n",
            "Epoch 164/200 train loss:0.7680 train acc:97.58% \n",
            "Epoch 164/200 val loss:0.8353 val acc:90.79% \n",
            "Epoch 165/200 train loss:0.7685 train acc:97.52% \n",
            "Epoch 165/200 val loss:0.8392 val acc:90.33% \n",
            "Epoch 166/200 train loss:0.7677 train acc:97.58% \n",
            "Epoch 166/200 val loss:0.8336 val acc:90.92% \n",
            "Epoch 167/200 train loss:0.7689 train acc:97.47% \n",
            "Epoch 167/200 val loss:0.8816 val acc:86.19% \n",
            "Epoch 168/200 train loss:0.7680 train acc:97.58% \n",
            "Epoch 168/200 val loss:0.8264 val acc:91.64% \n",
            "Epoch 169/200 train loss:0.7677 train acc:97.61% \n",
            "Epoch 169/200 val loss:0.8368 val acc:90.62% \n",
            "Epoch 170/200 train loss:0.7669 train acc:97.68% \n",
            "Epoch 170/200 val loss:0.9737 val acc:76.83% \n",
            "Epoch 171/200 train loss:0.7694 train acc:97.41% \n",
            "Epoch 171/200 val loss:0.8413 val acc:90.16% \n",
            "Epoch 172/200 train loss:0.7667 train acc:97.68% \n",
            "Epoch 172/200 val loss:0.8304 val acc:91.26% \n",
            "Epoch 173/200 train loss:0.7668 train acc:97.70% \n",
            "Epoch 173/200 val loss:0.8572 val acc:88.43% \n",
            "Epoch 174/200 train loss:0.7685 train acc:97.52% \n",
            "Epoch 174/200 val loss:0.8916 val acc:85.07% \n",
            "Epoch 175/200 train loss:0.7669 train acc:97.69% \n",
            "Epoch 175/200 val loss:0.8388 val acc:90.33% \n",
            "Epoch 176/200 train loss:0.7656 train acc:97.82% \n",
            "Epoch 176/200 val loss:0.8344 val acc:90.84% \n",
            "Epoch 177/200 train loss:0.7657 train acc:97.80% \n",
            "Epoch 177/200 val loss:0.8368 val acc:90.55% \n",
            "Epoch 178/200 train loss:0.7695 train acc:97.43% \n",
            "Epoch 178/200 val loss:0.8321 val acc:91.03% \n",
            "Epoch 179/200 train loss:0.7676 train acc:97.61% \n",
            "Epoch 179/200 val loss:0.8282 val acc:91.47% \n",
            "Epoch 180/200 train loss:0.7667 train acc:97.72% \n",
            "Epoch 180/200 val loss:0.8643 val acc:87.71% \n",
            "Epoch 181/200 train loss:0.7673 train acc:97.63% \n",
            "Epoch 181/200 val loss:0.8368 val acc:90.57% \n",
            "Epoch 182/200 train loss:0.7685 train acc:97.50% \n",
            "Epoch 182/200 val loss:0.8569 val acc:88.50% \n",
            "Epoch 183/200 train loss:0.7685 train acc:97.50% \n",
            "Epoch 183/200 val loss:0.9029 val acc:83.81% \n",
            "Epoch 184/200 train loss:0.7677 train acc:97.60% \n",
            "Epoch 184/200 val loss:0.8271 val acc:91.67% \n",
            "Epoch 185/200 train loss:0.7650 train acc:97.87% \n",
            "Epoch 185/200 val loss:0.8253 val acc:91.75% \n",
            "Epoch 186/200 train loss:0.7651 train acc:97.85% \n",
            "Epoch 186/200 val loss:0.8378 val acc:90.52% \n",
            "Epoch 187/200 train loss:0.7656 train acc:97.79% \n",
            "Epoch 187/200 val loss:0.8965 val acc:84.54% \n",
            "Epoch 188/200 train loss:0.7667 train acc:97.69% \n",
            "Epoch 188/200 val loss:0.8632 val acc:87.99% \n",
            "Epoch 189/200 train loss:0.7667 train acc:97.69% \n",
            "Epoch 189/200 val loss:0.8347 val acc:90.74% \n",
            "Epoch 190/200 train loss:0.7665 train acc:97.71% \n",
            "Epoch 190/200 val loss:0.8268 val acc:91.54% \n",
            "Epoch 191/200 train loss:0.7657 train acc:97.79% \n",
            "Epoch 191/200 val loss:0.8259 val acc:91.73% \n",
            "Epoch 192/200 train loss:0.7652 train acc:97.84% \n",
            "Epoch 192/200 val loss:0.8213 val acc:92.19% \n",
            "Epoch 193/200 train loss:0.7676 train acc:97.60% \n",
            "Epoch 193/200 val loss:0.8603 val acc:88.23% \n",
            "Epoch 194/200 train loss:0.7690 train acc:97.46% \n",
            "Epoch 194/200 val loss:0.8516 val acc:88.97% \n",
            "Epoch 195/200 train loss:0.7667 train acc:97.70% \n",
            "Epoch 195/200 val loss:0.8308 val acc:91.10% \n",
            "Epoch 196/200 train loss:0.7648 train acc:97.89% \n",
            "Epoch 196/200 val loss:0.8268 val acc:91.62% \n",
            "Epoch 197/200 train loss:0.7658 train acc:97.77% \n",
            "Epoch 197/200 val loss:0.8396 val acc:90.51% \n",
            "Epoch 198/200 train loss:0.7652 train acc:97.83% \n",
            "Epoch 198/200 val loss:0.8366 val acc:90.62% \n",
            "Epoch 199/200 train loss:0.7671 train acc:97.65% \n",
            "Epoch 199/200 val loss:0.8394 val acc:90.40% \n",
            "Epoch 200/200 train loss:0.7658 train acc:97.78% \n",
            "Epoch 200/200 val loss:0.8389 val acc:90.37% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mobile import MobileNetV3_Large\n",
        "\n",
        "bs = 100\n",
        "train_split = 0.8\n",
        "lr = 1e-3\n",
        "epochs = 200\n",
        "n_samples = len(mydataset)\n",
        "model = MobileNetV3_Large().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = int(train_split * n_samples)\n",
        "num_val = n_samples - num_train\n",
        "\n",
        "train_set, val_set = torch.utils.data.random_split(mydataset, [num_train, num_val])\n",
        "\n",
        "assert len(train_set) == num_train, \"Same\"\n",
        "assert len(val_set) == num_val, \"Same\"\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True)\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, criterion = nn.CrossEntropyLoss())\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "\n",
        "  if loss_val < best_loss:\n",
        "    best_loss = loss_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"bl-mobile.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model, os.path.join(base_path, 'audios', \"bv-mobile.pth\"))\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPonh4SL1_M9",
        "outputId": "c686f3c5-efeb-4f61-9346-6dcd969391f1"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/200 train loss:0.5775 train acc:79.35% val loss:0.5985 val acc:79.21%\n",
            "2/200 train loss:0.3572 train acc:87.87% val loss:0.3697 val acc:86.89%\n",
            "3/200 train loss:0.2895 train acc:89.89% val loss:0.3728 val acc:87.68%\n",
            "4/200 train loss:0.2644 train acc:90.48% val loss:0.3346 val acc:88.31%\n",
            "5/200 train loss:0.2458 train acc:91.26% val loss:0.5997 val acc:73.06%\n",
            "6/200 train loss:0.2228 train acc:91.91% val loss:0.4331 val acc:88.59%\n",
            "7/200 train loss:0.2042 train acc:92.81% val loss:0.2907 val acc:89.39%\n",
            "8/200 train loss:0.1795 train acc:93.65% val loss:0.6188 val acc:78.47%\n",
            "9/200 train loss:0.1554 train acc:94.29% val loss:0.2454 val acc:91.56%\n",
            "10/200 train loss:0.1468 train acc:94.63% val loss:0.5624 val acc:80.92%\n",
            "11/200 train loss:0.1265 train acc:95.43% val loss:0.3431 val acc:89.66%\n",
            "12/200 train loss:0.1140 train acc:95.92% val loss:0.4471 val acc:88.10%\n",
            "13/200 train loss:0.1017 train acc:96.37% val loss:0.3115 val acc:91.62%\n",
            "14/200 train loss:0.0938 train acc:96.56% val loss:0.3609 val acc:89.55%\n",
            "15/200 train loss:0.0892 train acc:96.89% val loss:0.4583 val acc:87.60%\n",
            "16/200 train loss:0.0742 train acc:97.27% val loss:0.3772 val acc:89.75%\n",
            "17/200 train loss:0.0689 train acc:97.53% val loss:0.4517 val acc:84.72%\n",
            "18/200 train loss:0.0684 train acc:97.60% val loss:0.2592 val acc:91.65%\n",
            "19/200 train loss:0.0611 train acc:97.82% val loss:0.4175 val acc:89.83%\n",
            "20/200 train loss:0.0544 train acc:98.07% val loss:0.2997 val acc:90.98%\n",
            "21/200 train loss:0.0514 train acc:98.08% val loss:0.2955 val acc:91.83%\n",
            "22/200 train loss:0.0430 train acc:98.43% val loss:0.3018 val acc:92.30%\n",
            "23/200 train loss:0.0441 train acc:98.32% val loss:0.2883 val acc:92.46%\n",
            "24/200 train loss:0.0397 train acc:98.64% val loss:0.5684 val acc:84.68%\n",
            "25/200 train loss:0.0415 train acc:98.52% val loss:0.3164 val acc:92.33%\n",
            "26/200 train loss:0.0478 train acc:98.38% val loss:0.2758 val acc:91.84%\n",
            "27/200 train loss:0.0386 train acc:98.66% val loss:0.6551 val acc:86.61%\n",
            "28/200 train loss:0.0324 train acc:98.87% val loss:0.3178 val acc:91.48%\n",
            "29/200 train loss:0.0365 train acc:98.79% val loss:0.3633 val acc:90.40%\n",
            "30/200 train loss:0.0350 train acc:98.80% val loss:0.3538 val acc:90.15%\n",
            "31/200 train loss:0.0308 train acc:98.96% val loss:0.5222 val acc:88.37%\n",
            "32/200 train loss:0.0387 train acc:98.65% val loss:0.2714 val acc:92.03%\n",
            "33/200 train loss:0.0312 train acc:98.90% val loss:0.3591 val acc:91.62%\n",
            "34/200 train loss:0.0347 train acc:98.79% val loss:0.4019 val acc:91.26%\n",
            "35/200 train loss:0.0298 train acc:98.99% val loss:0.4210 val acc:91.64%\n",
            "36/200 train loss:0.0310 train acc:99.04% val loss:0.2825 val acc:93.86%\n",
            "37/200 train loss:0.0350 train acc:98.81% val loss:0.3438 val acc:92.36%\n",
            "38/200 train loss:0.0243 train acc:99.12% val loss:0.6150 val acc:89.91%\n",
            "39/200 train loss:0.0208 train acc:99.29% val loss:0.2779 val acc:93.37%\n",
            "40/200 train loss:0.0307 train acc:99.01% val loss:0.2294 val acc:93.92%\n",
            "41/200 train loss:0.0232 train acc:99.19% val loss:0.2789 val acc:93.10%\n",
            "42/200 train loss:0.0298 train acc:99.02% val loss:0.2499 val acc:93.51%\n",
            "43/200 train loss:0.0231 train acc:99.19% val loss:0.2469 val acc:94.03%\n",
            "44/200 train loss:0.0403 train acc:98.66% val loss:0.4367 val acc:91.70%\n",
            "45/200 train loss:0.0264 train acc:99.09% val loss:0.3652 val acc:89.83%\n",
            "46/200 train loss:0.0197 train acc:99.36% val loss:0.3089 val acc:91.51%\n",
            "47/200 train loss:0.0178 train acc:99.39% val loss:0.2455 val acc:93.81%\n",
            "48/200 train loss:0.0238 train acc:99.17% val loss:0.2965 val acc:92.99%\n",
            "49/200 train loss:0.0285 train acc:99.12% val loss:0.2769 val acc:93.23%\n",
            "50/200 train loss:0.0239 train acc:99.14% val loss:0.2322 val acc:94.56%\n",
            "51/200 train loss:0.0171 train acc:99.41% val loss:0.2272 val acc:94.92%\n",
            "52/200 train loss:0.0181 train acc:99.37% val loss:1.0437 val acc:77.37%\n",
            "53/200 train loss:0.0255 train acc:99.13% val loss:0.2448 val acc:94.48%\n",
            "54/200 train loss:0.0253 train acc:99.10% val loss:0.2403 val acc:93.82%\n",
            "55/200 train loss:0.0176 train acc:99.45% val loss:0.3003 val acc:93.42%\n",
            "56/200 train loss:0.0187 train acc:99.38% val loss:0.2772 val acc:94.11%\n",
            "57/200 train loss:0.0181 train acc:99.37% val loss:0.2650 val acc:93.40%\n",
            "58/200 train loss:0.0199 train acc:99.32% val loss:0.4116 val acc:90.96%\n",
            "59/200 train loss:0.0222 train acc:99.28% val loss:0.2709 val acc:93.98%\n",
            "60/200 train loss:0.0190 train acc:99.37% val loss:0.2379 val acc:94.89%\n",
            "61/200 train loss:0.0219 train acc:99.27% val loss:0.2398 val acc:93.87%\n",
            "62/200 train loss:0.0157 train acc:99.47% val loss:0.2284 val acc:95.16%\n",
            "63/200 train loss:0.0144 train acc:99.53% val loss:0.5360 val acc:90.52%\n",
            "64/200 train loss:0.0173 train acc:99.44% val loss:0.2653 val acc:93.48%\n",
            "65/200 train loss:0.0201 train acc:99.36% val loss:0.2694 val acc:93.68%\n",
            "66/200 train loss:0.0246 train acc:99.16% val loss:0.2235 val acc:94.31%\n",
            "67/200 train loss:0.0148 train acc:99.56% val loss:0.2176 val acc:95.41%\n",
            "68/200 train loss:0.0120 train acc:99.60% val loss:0.3185 val acc:94.14%\n",
            "69/200 train loss:0.0179 train acc:99.49% val loss:0.3083 val acc:93.38%\n",
            "70/200 train loss:0.0214 train acc:99.33% val loss:0.2914 val acc:94.03%\n",
            "71/200 train loss:0.0133 train acc:99.56% val loss:0.3163 val acc:93.21%\n",
            "72/200 train loss:0.0210 train acc:99.32% val loss:0.1915 val acc:95.16%\n",
            "73/200 train loss:0.0131 train acc:99.54% val loss:0.2856 val acc:93.67%\n",
            "74/200 train loss:0.0180 train acc:99.42% val loss:0.2475 val acc:94.69%\n",
            "75/200 train loss:0.0158 train acc:99.46% val loss:0.2917 val acc:93.38%\n",
            "76/200 train loss:0.0143 train acc:99.54% val loss:0.3382 val acc:93.48%\n",
            "77/200 train loss:0.0138 train acc:99.56% val loss:0.2649 val acc:94.47%\n",
            "78/200 train loss:0.0111 train acc:99.64% val loss:0.5711 val acc:89.94%\n",
            "79/200 train loss:0.0179 train acc:99.41% val loss:0.3225 val acc:92.13%\n",
            "80/200 train loss:0.0144 train acc:99.52% val loss:0.2604 val acc:93.82%\n",
            "81/200 train loss:0.0133 train acc:99.65% val loss:0.2429 val acc:94.88%\n",
            "82/200 train loss:0.0134 train acc:99.54% val loss:0.4548 val acc:92.93%\n",
            "83/200 train loss:0.0176 train acc:99.38% val loss:0.3554 val acc:89.96%\n",
            "84/200 train loss:0.0154 train acc:99.55% val loss:0.2632 val acc:93.68%\n",
            "85/200 train loss:0.0137 train acc:99.54% val loss:0.2350 val acc:94.55%\n",
            "86/200 train loss:0.0146 train acc:99.54% val loss:0.2901 val acc:94.19%\n",
            "87/200 train loss:0.0192 train acc:99.30% val loss:0.2598 val acc:93.10%\n",
            "88/200 train loss:0.0106 train acc:99.68% val loss:0.3471 val acc:92.96%\n",
            "89/200 train loss:0.0089 train acc:99.71% val loss:0.2740 val acc:93.86%\n",
            "90/200 train loss:0.0155 train acc:99.50% val loss:0.3772 val acc:90.79%\n",
            "91/200 train loss:0.0124 train acc:99.59% val loss:0.2805 val acc:93.75%\n",
            "92/200 train loss:0.0127 train acc:99.61% val loss:0.2308 val acc:94.58%\n",
            "93/200 train loss:0.0112 train acc:99.60% val loss:0.2739 val acc:94.17%\n",
            "94/200 train loss:0.0174 train acc:99.43% val loss:0.2136 val acc:95.10%\n",
            "95/200 train loss:0.0115 train acc:99.65% val loss:0.2027 val acc:95.35%\n",
            "96/200 train loss:0.0092 train acc:99.69% val loss:0.2078 val acc:95.38%\n",
            "97/200 train loss:0.0134 train acc:99.59% val loss:0.2264 val acc:94.94%\n",
            "98/200 train loss:0.0108 train acc:99.66% val loss:0.6957 val acc:84.57%\n",
            "99/200 train loss:0.0146 train acc:99.52% val loss:0.4127 val acc:91.42%\n",
            "100/200 train loss:0.0136 train acc:99.56% val loss:0.2106 val acc:94.78%\n",
            "101/200 train loss:0.0155 train acc:99.47% val loss:0.3544 val acc:91.78%\n",
            "102/200 train loss:0.0153 train acc:99.43% val loss:0.3698 val acc:93.26%\n",
            "103/200 train loss:0.0087 train acc:99.71% val loss:0.3259 val acc:93.32%\n",
            "104/200 train loss:0.0092 train acc:99.67% val loss:0.2786 val acc:94.19%\n",
            "105/200 train loss:0.0125 train acc:99.58% val loss:0.2382 val acc:94.64%\n",
            "106/200 train loss:0.0088 train acc:99.71% val loss:0.2443 val acc:94.94%\n",
            "107/200 train loss:0.0103 train acc:99.65% val loss:0.2687 val acc:94.81%\n",
            "108/200 train loss:0.0149 train acc:99.56% val loss:0.2416 val acc:94.74%\n",
            "109/200 train loss:0.0087 train acc:99.73% val loss:0.2256 val acc:95.41%\n",
            "110/200 train loss:0.0137 train acc:99.57% val loss:0.2531 val acc:94.66%\n",
            "111/200 train loss:0.0119 train acc:99.56% val loss:0.2312 val acc:95.05%\n",
            "112/200 train loss:0.0127 train acc:99.57% val loss:0.3020 val acc:93.26%\n",
            "113/200 train loss:0.0086 train acc:99.71% val loss:0.2358 val acc:95.16%\n",
            "114/200 train loss:0.0098 train acc:99.67% val loss:0.2695 val acc:93.81%\n",
            "115/200 train loss:0.0134 train acc:99.56% val loss:0.3655 val acc:92.19%\n",
            "116/200 train loss:0.0112 train acc:99.63% val loss:0.5550 val acc:91.76%\n",
            "117/200 train loss:0.0081 train acc:99.77% val loss:0.2669 val acc:93.84%\n",
            "118/200 train loss:0.0100 train acc:99.68% val loss:0.2577 val acc:94.86%\n",
            "119/200 train loss:0.0093 train acc:99.68% val loss:0.3864 val acc:91.20%\n",
            "120/200 train loss:0.0138 train acc:99.54% val loss:0.2975 val acc:94.06%\n",
            "121/200 train loss:0.0091 train acc:99.72% val loss:0.4174 val acc:91.73%\n",
            "122/200 train loss:0.0102 train acc:99.71% val loss:0.3208 val acc:93.84%\n",
            "123/200 train loss:0.0276 train acc:99.09% val loss:0.2678 val acc:94.81%\n",
            "124/200 train loss:0.0126 train acc:99.54% val loss:0.3209 val acc:93.31%\n",
            "125/200 train loss:0.0068 train acc:99.80% val loss:0.2452 val acc:94.85%\n",
            "126/200 train loss:0.0057 train acc:99.82% val loss:0.3176 val acc:94.59%\n",
            "127/200 train loss:0.0071 train acc:99.80% val loss:0.3067 val acc:94.41%\n",
            "128/200 train loss:0.0139 train acc:99.57% val loss:0.3065 val acc:94.00%\n",
            "129/200 train loss:0.0092 train acc:99.67% val loss:0.2361 val acc:95.30%\n",
            "130/200 train loss:0.0101 train acc:99.67% val loss:0.2508 val acc:94.72%\n",
            "131/200 train loss:0.0116 train acc:99.63% val loss:0.5624 val acc:89.36%\n",
            "132/200 train loss:0.0093 train acc:99.72% val loss:0.3998 val acc:93.04%\n",
            "133/200 train loss:0.0099 train acc:99.69% val loss:0.2870 val acc:94.78%\n",
            "134/200 train loss:0.0074 train acc:99.77% val loss:0.2369 val acc:95.38%\n",
            "135/200 train loss:0.0088 train acc:99.71% val loss:0.2733 val acc:94.56%\n",
            "136/200 train loss:0.0091 train acc:99.66% val loss:0.4408 val acc:92.08%\n",
            "137/200 train loss:0.0099 train acc:99.65% val loss:0.2336 val acc:95.16%\n",
            "138/200 train loss:0.0135 train acc:99.54% val loss:0.2719 val acc:94.92%\n",
            "139/200 train loss:0.0111 train acc:99.63% val loss:0.2011 val acc:95.69%\n",
            "140/200 train loss:0.0067 train acc:99.76% val loss:0.2214 val acc:95.79%\n",
            "141/200 train loss:0.0059 train acc:99.81% val loss:0.3569 val acc:93.07%\n",
            "142/200 train loss:0.0136 train acc:99.54% val loss:0.2362 val acc:94.89%\n",
            "143/200 train loss:0.0061 train acc:99.77% val loss:0.2964 val acc:94.69%\n",
            "144/200 train loss:0.0117 train acc:99.59% val loss:0.5341 val acc:86.89%\n",
            "145/200 train loss:0.0122 train acc:99.57% val loss:0.2566 val acc:94.12%\n",
            "146/200 train loss:0.0061 train acc:99.81% val loss:0.3185 val acc:94.64%\n",
            "147/200 train loss:0.0113 train acc:99.66% val loss:0.2691 val acc:94.63%\n",
            "148/200 train loss:0.0036 train acc:99.89% val loss:0.2530 val acc:95.55%\n",
            "149/200 train loss:0.0078 train acc:99.76% val loss:0.2800 val acc:93.71%\n",
            "150/200 train loss:0.0073 train acc:99.74% val loss:0.2721 val acc:94.69%\n",
            "151/200 train loss:0.0092 train acc:99.66% val loss:0.2368 val acc:95.14%\n",
            "152/200 train loss:0.0140 train acc:99.54% val loss:0.2292 val acc:94.92%\n",
            "153/200 train loss:0.0102 train acc:99.63% val loss:0.2492 val acc:94.69%\n",
            "154/200 train loss:0.0042 train acc:99.87% val loss:0.4118 val acc:93.04%\n",
            "155/200 train loss:0.0077 train acc:99.70% val loss:0.2394 val acc:94.75%\n",
            "156/200 train loss:0.0144 train acc:99.49% val loss:0.2552 val acc:94.92%\n",
            "157/200 train loss:0.0094 train acc:99.70% val loss:0.2574 val acc:94.14%\n",
            "158/200 train loss:0.0061 train acc:99.80% val loss:0.1973 val acc:96.31%\n",
            "159/200 train loss:0.0094 train acc:99.72% val loss:0.2338 val acc:95.13%\n",
            "160/200 train loss:0.0078 train acc:99.76% val loss:0.2910 val acc:94.11%\n",
            "161/200 train loss:0.0100 train acc:99.70% val loss:0.2471 val acc:94.86%\n",
            "162/200 train loss:0.0107 train acc:99.70% val loss:0.2020 val acc:95.76%\n",
            "163/200 train loss:0.0062 train acc:99.83% val loss:0.2410 val acc:95.29%\n",
            "164/200 train loss:0.0045 train acc:99.84% val loss:0.2173 val acc:95.87%\n",
            "165/200 train loss:0.0057 train acc:99.82% val loss:0.2877 val acc:93.92%\n",
            "166/200 train loss:0.0150 train acc:99.47% val loss:0.3225 val acc:92.13%\n",
            "167/200 train loss:0.0176 train acc:99.46% val loss:0.3484 val acc:93.92%\n",
            "168/200 train loss:0.0093 train acc:99.72% val loss:0.2582 val acc:95.38%\n",
            "169/200 train loss:0.0078 train acc:99.77% val loss:0.2317 val acc:95.38%\n",
            "170/200 train loss:0.0071 train acc:99.77% val loss:0.2382 val acc:95.13%\n",
            "171/200 train loss:0.0094 train acc:99.71% val loss:0.2422 val acc:94.64%\n",
            "172/200 train loss:0.0054 train acc:99.83% val loss:0.2422 val acc:95.51%\n",
            "173/200 train loss:0.0093 train acc:99.67% val loss:0.3035 val acc:94.37%\n",
            "174/200 train loss:0.0073 train acc:99.74% val loss:0.2112 val acc:95.91%\n",
            "175/200 train loss:0.0071 train acc:99.77% val loss:0.2260 val acc:95.33%\n",
            "176/200 train loss:0.0041 train acc:99.87% val loss:0.2160 val acc:96.02%\n",
            "177/200 train loss:0.0054 train acc:99.84% val loss:0.2876 val acc:94.59%\n",
            "178/200 train loss:0.0080 train acc:99.73% val loss:0.3012 val acc:93.87%\n",
            "179/200 train loss:0.0115 train acc:99.62% val loss:0.3200 val acc:92.35%\n",
            "180/200 train loss:0.0086 train acc:99.72% val loss:0.2879 val acc:93.97%\n",
            "181/200 train loss:0.0085 train acc:99.72% val loss:0.2327 val acc:95.16%\n",
            "182/200 train loss:0.0106 train acc:99.64% val loss:0.2163 val acc:95.58%\n",
            "183/200 train loss:0.0079 train acc:99.71% val loss:0.3225 val acc:94.47%\n",
            "184/200 train loss:0.0073 train acc:99.76% val loss:0.2316 val acc:95.76%\n",
            "185/200 train loss:0.0057 train acc:99.82% val loss:0.3127 val acc:94.17%\n",
            "186/200 train loss:0.0126 train acc:99.58% val loss:0.2705 val acc:94.91%\n",
            "187/200 train loss:0.0072 train acc:99.79% val loss:0.1868 val acc:95.95%\n",
            "188/200 train loss:0.0030 train acc:99.93% val loss:0.2295 val acc:95.93%\n",
            "189/200 train loss:0.0083 train acc:99.72% val loss:0.2336 val acc:95.47%\n",
            "190/200 train loss:0.0075 train acc:99.74% val loss:0.3197 val acc:93.34%\n",
            "191/200 train loss:0.0056 train acc:99.83% val loss:0.2351 val acc:95.30%\n",
            "192/200 train loss:0.0072 train acc:99.76% val loss:0.3483 val acc:94.11%\n",
            "193/200 train loss:0.0097 train acc:99.70% val loss:0.2406 val acc:94.70%\n",
            "194/200 train loss:0.0100 train acc:99.69% val loss:0.2827 val acc:94.70%\n",
            "195/200 train loss:0.0040 train acc:99.89% val loss:0.2139 val acc:95.63%\n",
            "196/200 train loss:0.0053 train acc:99.82% val loss:0.2404 val acc:94.81%\n",
            "197/200 train loss:0.0105 train acc:99.66% val loss:0.3483 val acc:93.64%\n",
            "198/200 train loss:0.0098 train acc:99.65% val loss:0.2214 val acc:95.46%\n",
            "199/200 train loss:0.0075 train acc:99.78% val loss:0.2074 val acc:95.99%\n",
            "200/200 train loss:0.0074 train acc:99.82% val loss:0.2289 val acc:95.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_pretrained = torch.load(os.path.join(base_path, 'audios', \"bv-mobile.pth\"))\n",
        "model_pretrained.to(device)\n",
        "model_pretrained.eval()\n",
        "\n",
        "voting_dir = '/content/drive/MyDrive/COSRMAL_CHALLENGE/audios'\n",
        "\n",
        "mfcc_MAX_VALUE=194.19187653405487\n",
        "mfcc_MIN_VALUE=-313.07119549054045\n",
        "\n",
        "t2_MAX_VALUE = 57.464638\n",
        "t2_MIN_VALUE = -1.1948369\n",
        "save_size=64\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "audio_paths = [os.path.join(audio_folder, path) for path in sorted(os.listdir(audio_folder))]\n",
        "save_data = {}\n",
        "data_num = 0\n",
        "for i, path in enumerate(audio_paths):\n",
        "  count_pred = [0] * 4\n",
        "  pred_list = []\n",
        "  sample_rate, signal = scipy.io.wavfile.read(path)\n",
        "  ap = AudioProcessing(sample_rate,signal)\n",
        "  mfcc = ap.calc_MFCC()\n",
        "  mfcc_length=mfcc.shape[0]\n",
        "  f_step=int(mfcc.shape[1]*0.25)\n",
        "  f_length=mfcc.shape[1]\n",
        "  save_mfcc_num=int(np.ceil(float(np.abs(mfcc_length - save_size)) /f_step))\n",
        "  for i in range(save_mfcc_num):\n",
        "      tmp_mfcc = mfcc[i*f_step:save_size+i*f_step,: ,:]\n",
        "      tmp_mfcc= (tmp_mfcc-mfcc_MIN_VALUE)/(mfcc_MAX_VALUE-mfcc_MIN_VALUE)\n",
        "      tmp_mfcc=tmp_mfcc.transpose(2,0,1)\n",
        "      audio=torch.from_numpy(tmp_mfcc.astype(np.float32))\n",
        "      audio=torch.unsqueeze(audio, 0)\n",
        "      audio = audio.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        pred_T2 = model_pretrained.forward(audio)\n",
        "        _,pred_T2=torch.max(pred_T2,1)\n",
        "        count_pred[pred_T2.item()]+=1\n",
        "        pred_list.append(pred_T2.item())\n",
        "  if  count_pred[1]>5 or count_pred[2]>5 or count_pred[3]>5:\n",
        "    final_pred_T2=count_pred[1:4].index(max(count_pred[1:4]))+1\n",
        "  else:\n",
        "    final_pred_T2=0\n",
        "  \n",
        "  file_name = path.split(os.path.sep)[-1].replace('.wav', '')\n",
        "  #print(\"sequence:{}, frequency:{}\".format(file_name, count_pred))\n",
        "  to_save_data = {\"data_num\":data_num,\n",
        "                  \"file\":file_name,\n",
        "                  \"count_pred\":count_pred,\n",
        "                  \"final_pred\":final_pred_T2,\n",
        "                  'pred':pred_list}\n",
        "  save_data[\"{}\".format(file_name)] = to_save_data\n",
        "  data_num+=1\n",
        "\n",
        "with open (os.path.join(voting_dir, \"voting.json\"), 'w') as f:\n",
        "  json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
        "  elapsed_time = time.time() - start\n",
        "  print(\"elapsed_time:{}\".format(elapsed_time) + \"sec\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IRmiIp0hrkk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(os.path.join(voting_dir, \"voting.json\"))\n",
        "vote_js = json.load(f)\n",
        "\n",
        "vote = pd.DataFrame(vote_js).T\n",
        "vote.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "nv4DQKgjsHjV",
        "outputId": "cbe01d09-1d3b-4028-f234-460557dc8782"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0e627883-9ca2-4b44-8dc7-9829f548bcef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data_num</th>\n",
              "      <th>file</th>\n",
              "      <th>count_pred</th>\n",
              "      <th>final_pred</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>000000</th>\n",
              "      <td>0</td>\n",
              "      <td>000000</td>\n",
              "      <td>[19, 0, 19, 0]</td>\n",
              "      <td>2</td>\n",
              "      <td>[0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000001</th>\n",
              "      <td>1</td>\n",
              "      <td>000001</td>\n",
              "      <td>[13, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000002</th>\n",
              "      <td>2</td>\n",
              "      <td>000002</td>\n",
              "      <td>[56, 0, 0, 21]</td>\n",
              "      <td>3</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000003</th>\n",
              "      <td>3</td>\n",
              "      <td>000003</td>\n",
              "      <td>[16, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>000004</th>\n",
              "      <td>4</td>\n",
              "      <td>000004</td>\n",
              "      <td>[7, 9, 0, 0]</td>\n",
              "      <td>1</td>\n",
              "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e627883-9ca2-4b44-8dc7-9829f548bcef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e627883-9ca2-4b44-8dc7-9829f548bcef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e627883-9ca2-4b44-8dc7-9829f548bcef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       data_num  ...                                               pred\n",
              "000000        0  ...  [0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...\n",
              "000001        1  ...            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "000002        2  ...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "000003        3  ...   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "000004        4  ...   [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gt = pd.read_csv('/content/drive/MyDrive/COSRMAL_CHALLENGE/train.csv')\n",
        "acc = np.sum(gt['filling_type'].to_numpy() == vote['final_pred'].to_numpy()) / len(gt['filling_type'])\n",
        "print('Acc: {:.2f}%'.format(100 * acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG3xQQqJoQpv",
        "outputId": "4c29b1c8-5005-4e50-8883-9b5659c4d5a3"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMx6OPYmtWHy",
        "outputId": "6ec66e25-2403-4e96-fcd9-f52ed0bea719"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(684,)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    }
  ]
}