{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RGBD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "xTCST85Jf6sA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4nVowpzcsSE"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.io.wavfile\n",
        "import time\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/rgb.zip /content/rgb.zip\n",
        "!cp /content/drive/MyDrive/depth.zip /content/depth.zip\n",
        "!cp /content/drive/MyDrive/labels.zip /content/labels.zip"
      ],
      "metadata": {
        "id": "6NK5jmrxc4qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/rgb.zip\n",
        "!unzip /content/drive/MyDrive/depth.zip\n",
        "!unzip /content/drive/MyDrive/labels.zip"
      ],
      "metadata": {
        "id": "fxUgopCVeRMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing 2"
      ],
      "metadata": {
        "id": "lU7Y4M1egAh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.io.wavfile\n",
        "import time\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "v9ej-BmvgInY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/crops_rgb_train.zip /content/crops_rgb_train.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/crops_rgb_test.zip /content/crops_rgb_test.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/crops_depth_train.zip /content/crops_depth_train.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/crops_depth_test.zip /content/crops_depth_test.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/labels_test.zip /content/labels_test.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/labels_train.zip /content/labels_train.zip\n"
      ],
      "metadata": {
        "id": "BTQA8MKYgN1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/train/crops_rgb_train.zip -d /content/train/\n",
        "!unzip /content/test/crops_rgb_test.zip -d /content/test/\n",
        "!unzip /content/train/crops_depth_train.zip -d /content/train/\n",
        "!unzip /content/test/crops_depth_test.zip -d /content/test/\n",
        "!unzip /content/train/labels_train.zip -d /content/train/\n",
        "!unzip /content/test/labels_test.zip -d /content/test"
      ],
      "metadata": {
        "id": "-GKjFsuRg8wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient"
      ],
      "metadata": {
        "id": "cD3P6O_vc458"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a EfficientNetV2 Model as defined in:\n",
        "Mingxing Tan, Quoc V. Le. (2021). \n",
        "EfficientNetV2: Smaller Models and Faster Training\n",
        "arXiv preprint arXiv:2104.00298.\n",
        "import from https://github.com/d-li14/mobilenetv2.pytorch\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "__all__ = ['effnetv2_s', 'effnetv2_m', 'effnetv2_l', 'effnetv2_xl']\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "# SiLU (Swish) activation function\n",
        "if hasattr(nn, 'SiLU'):\n",
        "    SiLU = nn.SiLU\n",
        "else:\n",
        "    # For compatibility with old PyTorch versions\n",
        "    class SiLU(nn.Module):\n",
        "        def forward(self, x):\n",
        "            return x * torch.sigmoid(x)\n",
        "\n",
        " \n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, inp, oup, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
        "                SiLU(),\n",
        "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "def conv_3x3_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
        "        super(MBConv, self).__init__()\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = round(inp * expand_ratio)\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "        if use_se:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                SELayer(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # fused\n",
        "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.identity:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class EffNetV2(nn.Module):\n",
        "    def __init__(self, cfgs, num_classes=1, width_mult=1.):\n",
        "        super(EffNetV2, self).__init__()\n",
        "        self.cfgs = cfgs\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(24 * width_mult, 8)\n",
        "        layers = [conv_3x3_bn(4, input_channel, 2)]\n",
        "        # building inverted residual blocks\n",
        "        block = MBConv\n",
        "        for t, c, n, s, use_se in self.cfgs:\n",
        "            output_channel = _make_divisible(c * width_mult, 8)\n",
        "            for i in range(n):\n",
        "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n",
        "                input_channel = output_channel\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        # building last several layers\n",
        "        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n",
        "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(output_channel, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    def extract(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        feature = x.view(x.size(0), -1)\n",
        "        x = self.classifier(feature)\n",
        "\n",
        "        return feature, x\n",
        "\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.001)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def effnetv2_s(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-S model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  24,  2, 1, 0],\n",
        "        [4,  48,  4, 2, 0],\n",
        "        [4,  64,  4, 2, 0],\n",
        "        [4, 128,  6, 2, 1],\n",
        "        [6, 160,  9, 1, 1],\n",
        "        [6, 256, 15, 2, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)\n",
        "\n",
        "\n",
        "def effnetv2_m(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-M model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  24,  3, 1, 0],\n",
        "        [4,  48,  5, 2, 0],\n",
        "        [4,  80,  5, 2, 0],\n",
        "        [4, 160,  7, 2, 1],\n",
        "        [6, 176, 14, 1, 1],\n",
        "        [6, 304, 18, 2, 1],\n",
        "        [6, 512,  5, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)\n",
        "\n",
        "\n",
        "def effnetv2_l(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-L model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  32,  4, 1, 0],\n",
        "        [4,  64,  7, 2, 0],\n",
        "        [4,  96,  7, 2, 0],\n",
        "        [4, 192, 10, 2, 1],\n",
        "        [6, 224, 19, 1, 1],\n",
        "        [6, 384, 25, 2, 1],\n",
        "        [6, 640,  7, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)\n",
        "\n",
        "\n",
        "def effnetv2_xl(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-XL model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  32,  4, 1, 0],\n",
        "        [4,  64,  8, 2, 0],\n",
        "        [4,  96,  8, 2, 0],\n",
        "        [4, 192, 16, 2, 1],\n",
        "        [6, 256, 24, 1, 1],\n",
        "        [6, 512, 32, 2, 1],\n",
        "        [6, 640,  8, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)"
      ],
      "metadata": {
        "id": "r4L7Z7n9c3W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mobile"
      ],
      "metadata": {
        "id": "3WUCTNd1o8G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "\n",
        "\n",
        "class hswish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        out = x * F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class hsigmoid(nn.Module):\n",
        "    def forward(self, x):\n",
        "        out = F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class SeModule(nn.Module):\n",
        "    def __init__(self, in_size, reduction=4):\n",
        "        super(SeModule, self).__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_size, in_size // reduction, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(in_size // reduction),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_size // reduction, in_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(in_size),\n",
        "            hsigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.se(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise'''\n",
        "    def __init__(self, kernel_size, in_size, expand_size, out_size, nolinear, semodule, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.se = semodule\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(expand_size)\n",
        "        self.nolinear1 = nolinear\n",
        "        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=expand_size, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(expand_size)\n",
        "        self.nolinear2 = nolinear\n",
        "        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_size)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_size != out_size:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_size),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.nolinear1(self.bn1(self.conv1(x)))\n",
        "        out = self.nolinear2(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        if self.se != None:\n",
        "            out = self.se(out)\n",
        "        out = out + self.shortcut(x) if self.stride==1 else out\n",
        "        return out\n",
        "\n",
        "\n",
        "class MobileNetV3_Large(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(MobileNetV3_Large, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.hs1 = hswish()\n",
        "\n",
        "        self.bneck = nn.Sequential(\n",
        "            Block(3, 16, 16, 16, nn.ReLU(inplace=True), None, 1),\n",
        "            Block(3, 16, 64, 24, nn.ReLU(inplace=True), None, 2),\n",
        "            Block(3, 24, 72, 24, nn.ReLU(inplace=True), None, 1),\n",
        "            Block(5, 24, 72, 40, nn.ReLU(inplace=True), SeModule(40), 2),\n",
        "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
        "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
        "            Block(3, 40, 240, 80, hswish(), None, 2),\n",
        "            Block(3, 80, 200, 80, hswish(), None, 1),\n",
        "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
        "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
        "            Block(3, 80, 480, 112, hswish(), SeModule(112), 1),\n",
        "            Block(3, 112, 672, 112, hswish(), SeModule(112), 1),\n",
        "            Block(5, 112, 672, 160, hswish(), SeModule(160), 1),\n",
        "            Block(5, 160, 672, 160, hswish(), SeModule(160), 2),\n",
        "            Block(5, 160, 960, 160, hswish(), SeModule(160), 1),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.conv2 = nn.Conv2d(160, 960, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(960)\n",
        "        self.hs2 = hswish()\n",
        "        self.linear3 = nn.Linear(960, 1280)\n",
        "        self.bn3 = nn.BatchNorm1d(1280)\n",
        "        self.hs3 = hswish()\n",
        "        self.linear4 = nn.Linear(1280, num_classes)\n",
        "        self.init_params()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.hs3(self.bn3(self.linear3(out)))\n",
        "        out = self.linear4(out)\n",
        "        return out\n",
        "    def extract(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MobileNetV3_Small(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(MobileNetV3_Small, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.hs1 = hswish()\n",
        "\n",
        "        self.bneck = nn.Sequential(\n",
        "            Block(3, 16, 16, 16, nn.ReLU(inplace=True), SeModule(16), 2),\n",
        "            Block(3, 16, 72, 24, nn.ReLU(inplace=True), None, 2),\n",
        "            Block(3, 24, 88, 24, nn.ReLU(inplace=True), None, 1),\n",
        "            Block(5, 24, 96, 40, hswish(), SeModule(40), 2),\n",
        "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
        "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
        "            Block(5, 40, 120, 48, hswish(), SeModule(48), 1),\n",
        "            Block(5, 48, 144, 48, hswish(), SeModule(48), 1),\n",
        "            Block(5, 48, 288, 96, hswish(), SeModule(96), 2),\n",
        "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
        "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(576)\n",
        "        self.hs2 = hswish()\n",
        "        self.linear3 = nn.Linear(576, 1280)\n",
        "        self.bn3 = nn.BatchNorm1d(1280)\n",
        "        self.hs3 = hswish()\n",
        "        self.linear4 = nn.Linear(1280, num_classes)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.hs3(self.bn3(self.linear3(out)))\n",
        "        out = self.linear4(out)\n",
        "        return out\n",
        "        \n",
        "    def extract(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = MobileNetV3_Small()\n",
        "    x = torch.randn(2,3,224,224)\n",
        "    y = net(x)\n",
        "    print(y.size())"
      ],
      "metadata": {
        "id": "K7fa7LEWo6Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "6XvAhx-tdXJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_annotation(id,input,anno_path='/content/labels'):\n",
        "    anno = np.load(os.path.join(anno_path,'{:06d}.npy'.format(id)),allow_pickle=True).item()\n",
        "    return anno.get(input)"
      ],
      "metadata": {
        "id": "K3Na-c5wgyl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "\n",
        "class MiniDataset(Dataset):\n",
        "    def __init__(self, label_f, depth, crops_rgb_f):\n",
        "      self.label_f = label_f #_f = folder\n",
        "      self.depth = depth\n",
        "      self.crops_rgb_f = crops_rgb_f\n",
        "      self.samples = os.listdir(crops_rgb_f)\n",
        "      self.ids = [ int(x.split('.')[0]) for x in self.samples]\n",
        "      self.transform = transforms.Compose([\n",
        "                                             transforms.Resize((320, 320)),\n",
        "                                             transforms.ToTensor(),\n",
        "                                             transforms.ConvertImageDtype(torch.float),\n",
        "                                             ])\n",
        "    def __len__(self):\n",
        "      return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      id_ = self.ids[idx]\n",
        "        \n",
        "      # depth\n",
        "      depth = np.asarray(Image.open(os.path.join(self.depth,'{:06d}.png'.format(id_))))[:,:,np.newaxis]\n",
        "      \n",
        "      # rgb_cropped\n",
        "      crop = np.asarray(Image.open(os.path.join(self.crops_rgb_f,'{:06d}.png'.format(id_))))\n",
        "\n",
        "      h, w, c = crop.shape\n",
        "\n",
        "      resX = 640 - h\n",
        "      resY = 640 - w\n",
        "\n",
        "      up = resX // 2\n",
        "      down = up\n",
        "      if resX % 2 != 0:\n",
        "        down +=1\n",
        "\n",
        "      left = resY // 2\n",
        "      right = left\n",
        "\n",
        "      if resY % 2 != 0:\n",
        "        left += 1\n",
        "\n",
        "      padding = transforms.Pad((left, up, right, down))\n",
        "\n",
        "    \n",
        "      image = Image.fromarray(np.concatenate((crop, depth), axis=2))\n",
        "      image = padding(image)\n",
        "      image = self.transform(image)\n",
        "      \n",
        "      # label\n",
        "      label = get_annotation(id_,'container capacity')\n",
        "      return image, label"
      ],
      "metadata": {
        "id": "Wdvu3wGxdYWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def computeScoreType1(gt, _est):\n",
        "  gt = gt.cpu().data.numpy()\n",
        "  _est = _est.cpu().data.numpy()\n",
        "  est = copy.deepcopy(_est)\n",
        "  assert (len(gt) == len(est))\n",
        "  if all(x == -1 for x in est):\n",
        "    return 0\n",
        "  indicator_f = est > -1\n",
        "  ec = np.exp(-(np.abs(gt - est) / gt)) * indicator_f\n",
        "  score = np.sum(ec) / len(gt)\n",
        "  return score"
      ],
      "metadata": {
        "id": "ajNw56bHrLQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "U2kxapxBc7_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion = nn.L1Loss()):\n",
        "  model.train()\n",
        "  loss_train = 0.0\n",
        "  correct_train = 0.0\n",
        "  num_train = len(train_loader)\n",
        "  for batch_idx, (audio, target) in enumerate(train_loader):\n",
        "    audio = audio.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(audio)[:,0]\n",
        "\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    #nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_train += loss.item() / num_train\n",
        "    correct_train += computeScoreType1(target, outputs) * audio.shape[0]\n",
        "    \n",
        "  \n",
        "  return loss_train, correct_train\n",
        "\n",
        "\n",
        "def evaluate(model, testloader, criterion = nn.L1Loss()):\n",
        "  model.eval()\n",
        "  loss_test = 0\n",
        "  correct_test=0\n",
        "  num_val = len(testloader)\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (audio, target) in enumerate(testloader):\n",
        "      audio = audio.to(device)\n",
        "      target = target.to(device)\n",
        "      outputs = model.forward(audio)[:,0]\n",
        "      loss = criterion(outputs, target)\n",
        "      loss_test += loss.item() / num_val   \n",
        "      correct_test += computeScoreType1(target, outputs) * audio.shape[0]\n",
        "\n",
        "  \n",
        "  return loss_test, correct_test\n"
      ],
      "metadata": {
        "id": "RfA3yrJ8jvmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def myLoss(est, gt):\n",
        "  ec = 1 - torch.exp(-(torch.abs(gt - est) / gt))\n",
        "  score = torch.sum(ec) / len(gt)\n",
        "\n",
        "  return score\n",
        "  "
      ],
      "metadata": {
        "id": "vrJWrpPM9cot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "my_save_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE'\n",
        "bs = 2\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "train_set = MiniDataset('/content/train/labels', \n",
        "                        '/content/train/crops_depth',\n",
        "                        '/content/train/crops_rgb')\n",
        "val_set = MiniDataset('/content/test/labels', \n",
        "                        '/content/test/crops_depth',\n",
        "                        '/content/test/crops_rgb')\n",
        "model = effnetv2_xl().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = len(train_set)\n",
        "num_val = len(val_set)\n",
        "\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True,\n",
        "                            num_workers=1)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer, myLoss)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, myLoss)\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "  \n",
        "\n",
        "  # if loss_val < best_loss:\n",
        "  #   best_loss = loss_val\n",
        "  #   torch.save(model, os.path.join(base_path, 'audios', \"bl-efficient-xl.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(my_save_path, \n",
        "                                              'audios', \n",
        "                                              'RGBD',\n",
        "                                              \"XL-my{:.2f}.pth\".format(100 * correct_val/num_val)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u7ge09lVhZNY",
        "outputId": "3e93966c-72ac-42c5-adef-80f9efcf6072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "1/200 train loss:0.5407 train acc:31.13% val loss:0.6569 val acc:14.11%\n",
            "2/200 train loss:0.4657 train acc:44.30% val loss:0.4740 val acc:52.68%\n",
            "3/200 train loss:0.3280 train acc:67.19% val loss:0.3677 val acc:63.17%\n",
            "4/200 train loss:0.2887 train acc:71.14% val loss:0.3215 val acc:67.62%\n",
            "5/200 train loss:0.2866 train acc:71.33% val loss:0.3242 val acc:67.59%\n",
            "6/200 train loss:0.2863 train acc:71.36% val loss:0.3264 val acc:67.22%\n",
            "7/200 train loss:0.2846 train acc:71.54% val loss:0.3220 val acc:67.83%\n",
            "8/200 train loss:0.2858 train acc:71.42% val loss:0.3382 val acc:66.11%\n",
            "9/200 train loss:0.2832 train acc:71.68% val loss:0.3552 val acc:64.49%\n",
            "10/200 train loss:0.2803 train acc:71.97% val loss:0.3273 val acc:67.15%\n",
            "11/200 train loss:0.2804 train acc:71.96% val loss:0.3370 val acc:66.25%\n",
            "12/200 train loss:0.2749 train acc:72.51% val loss:0.3202 val acc:67.91%\n",
            "13/200 train loss:0.2681 train acc:73.20% val loss:0.3151 val acc:68.49%\n",
            "14/200 train loss:0.2493 train acc:75.06% val loss:0.3171 val acc:68.05%\n",
            "15/200 train loss:0.2347 train acc:76.54% val loss:0.3208 val acc:68.03%\n",
            "16/200 train loss:0.2295 train acc:77.04% val loss:0.3160 val acc:68.15%\n",
            "17/200 train loss:0.2231 train acc:77.69% val loss:0.3032 val acc:69.51%\n",
            "18/200 train loss:0.2223 train acc:77.79% val loss:0.3222 val acc:67.80%\n",
            "19/200 train loss:0.2074 train acc:79.27% val loss:0.3091 val acc:68.85%\n",
            "20/200 train loss:0.1989 train acc:80.11% val loss:0.3280 val acc:66.96%\n",
            "21/200 train loss:0.1941 train acc:80.60% val loss:0.3279 val acc:67.39%\n",
            "22/200 train loss:0.1870 train acc:81.29% val loss:0.3180 val acc:68.05%\n",
            "23/200 train loss:0.1942 train acc:80.57% val loss:0.3320 val acc:66.94%\n",
            "24/200 train loss:0.2704 train acc:72.95% val loss:0.3346 val acc:66.40%\n",
            "25/200 train loss:0.2297 train acc:77.03% val loss:0.3230 val acc:67.65%\n",
            "26/200 train loss:0.2178 train acc:78.21% val loss:0.3272 val acc:67.29%\n",
            "27/200 train loss:0.2166 train acc:78.34% val loss:0.3255 val acc:67.21%\n",
            "28/200 train loss:0.2013 train acc:79.88% val loss:0.3083 val acc:69.34%\n",
            "29/200 train loss:0.1886 train acc:81.14% val loss:0.3078 val acc:69.16%\n",
            "30/200 train loss:0.1791 train acc:82.09% val loss:0.3239 val acc:67.66%\n",
            "31/200 train loss:0.1796 train acc:82.04% val loss:0.3223 val acc:67.61%\n",
            "32/200 train loss:0.1756 train acc:82.44% val loss:0.3254 val acc:67.46%\n",
            "33/200 train loss:0.1636 train acc:83.65% val loss:0.3297 val acc:67.02%\n",
            "34/200 train loss:0.1525 train acc:84.75% val loss:0.3146 val acc:68.35%\n",
            "35/200 train loss:0.1495 train acc:85.05% val loss:0.3154 val acc:68.44%\n",
            "36/200 train loss:0.1467 train acc:85.34% val loss:0.3138 val acc:68.73%\n",
            "37/200 train loss:0.1397 train acc:86.03% val loss:0.3187 val acc:68.21%\n",
            "38/200 train loss:0.1370 train acc:86.30% val loss:0.3237 val acc:67.68%\n",
            "39/200 train loss:0.1369 train acc:86.32% val loss:0.3161 val acc:68.60%\n",
            "40/200 train loss:0.1393 train acc:86.08% val loss:0.3174 val acc:68.20%\n",
            "41/200 train loss:0.1266 train acc:87.35% val loss:0.3332 val acc:66.83%\n",
            "42/200 train loss:0.1253 train acc:87.47% val loss:0.3400 val acc:65.99%\n",
            "43/200 train loss:0.1215 train acc:87.84% val loss:0.3193 val acc:68.11%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b91e7a2c53e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;31m#start_time = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;31m#elapsed_time = time.time() - start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-e46dd8b453ef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-747f4faa3112>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-747f4faa3112>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "my_save_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE'\n",
        "bs = 16\n",
        "train_split = 0.8\n",
        "lr = 1e-3\n",
        "epochs = 200\n",
        "train_set = MiniDataset('/content/train/labels', \n",
        "                        '/content/train/crops_depth',\n",
        "                        '/content/train/crops_rgb')\n",
        "val_set = MiniDataset('/content/test/labels', \n",
        "                        '/content/test/crops_depth',\n",
        "                        '/content/test/crops_rgb')\n",
        "model = MobileNetV3_Large().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = len(train_set)\n",
        "num_val = len(val_set)\n",
        "\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True,\n",
        "                            num_workers=1,\n",
        "                            drop_last=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer, myLoss)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, myLoss)\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "  \n",
        "\n",
        "  # if loss_val < best_loss:\n",
        "  #   best_loss = loss_val\n",
        "  #   torch.save(model, os.path.join(base_path, 'audios', \"bl-efficient-xl.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(my_save_path, \n",
        "                                              'audios', \n",
        "                                              'RGBD',\n",
        "                                              \"mobile{:.2f}.pth\".format(100 * correct_val/num_val)))\n",
        "\n"
      ],
      "metadata": {
        "id": "FB5uZuFlmZgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de15d77-9c7f-42aa-ea8e-7372eddd4663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "1/200 train loss:0.5707 train acc:23.02% val loss:0.6795 val acc:0.00%\n",
            "2/200 train loss:0.4007 train acc:51.81% val loss:0.3701 val acc:60.14%\n",
            "3/200 train loss:0.3136 train acc:66.64% val loss:0.3333 val acc:66.40%\n",
            "4/200 train loss:0.2954 train acc:70.12% val loss:0.3278 val acc:67.31%\n",
            "5/200 train loss:0.2704 train acc:72.62% val loss:0.5672 val acc:43.54%\n",
            "6/200 train loss:0.2753 train acc:71.85% val loss:0.3424 val acc:65.86%\n",
            "7/200 train loss:0.2713 train acc:72.38% val loss:0.3411 val acc:65.61%\n",
            "8/200 train loss:0.2711 train acc:72.43% val loss:0.3562 val acc:64.06%\n",
            "9/200 train loss:0.2651 train acc:73.17% val loss:0.3464 val acc:65.62%\n",
            "10/200 train loss:0.2651 train acc:73.14% val loss:0.3332 val acc:66.83%\n",
            "11/200 train loss:0.2535 train acc:74.31% val loss:0.3367 val acc:66.66%\n",
            "12/200 train loss:0.2454 train acc:75.12% val loss:0.3380 val acc:66.41%\n",
            "13/200 train loss:0.2245 train acc:77.21% val loss:0.3539 val acc:64.85%\n",
            "14/200 train loss:0.2157 train acc:78.09% val loss:0.3518 val acc:64.79%\n",
            "15/200 train loss:0.2414 train acc:75.34% val loss:0.3296 val acc:66.95%\n",
            "16/200 train loss:0.2124 train acc:78.38% val loss:0.3240 val acc:67.42%\n",
            "17/200 train loss:0.1978 train acc:79.87% val loss:0.3337 val acc:66.19%\n",
            "18/200 train loss:0.1994 train acc:79.22% val loss:0.3921 val acc:60.90%\n",
            "19/200 train loss:0.2134 train acc:77.89% val loss:0.3328 val acc:66.86%\n",
            "20/200 train loss:0.2279 train acc:76.52% val loss:0.3335 val acc:66.54%\n",
            "21/200 train loss:0.2206 train acc:77.11% val loss:0.3193 val acc:67.98%\n",
            "22/200 train loss:0.2044 train acc:78.70% val loss:0.3423 val acc:65.93%\n",
            "23/200 train loss:0.2007 train acc:79.21% val loss:0.4172 val acc:58.06%\n",
            "24/200 train loss:0.2020 train acc:79.15% val loss:0.3292 val acc:67.22%\n",
            "25/200 train loss:0.2108 train acc:78.03% val loss:0.3715 val acc:62.98%\n",
            "26/200 train loss:0.1992 train acc:79.33% val loss:0.3391 val acc:66.09%\n",
            "27/200 train loss:0.2003 train acc:79.16% val loss:0.3764 val acc:62.18%\n",
            "28/200 train loss:0.1980 train acc:79.40% val loss:0.3520 val acc:64.40%\n",
            "29/200 train loss:0.1893 train acc:80.34% val loss:0.3706 val acc:62.70%\n",
            "30/200 train loss:0.2077 train acc:78.58% val loss:0.3533 val acc:64.72%\n",
            "31/200 train loss:0.2060 train acc:78.85% val loss:0.3565 val acc:64.75%\n",
            "32/200 train loss:0.2130 train acc:78.18% val loss:0.3430 val acc:65.50%\n",
            "33/200 train loss:0.1949 train acc:79.88% val loss:0.3715 val acc:62.92%\n",
            "34/200 train loss:0.1981 train acc:79.55% val loss:0.3498 val acc:64.90%\n",
            "35/200 train loss:0.1891 train acc:80.34% val loss:0.3444 val acc:65.51%\n",
            "36/200 train loss:0.1906 train acc:80.20% val loss:0.4057 val acc:59.47%\n",
            "37/200 train loss:0.1932 train acc:80.03% val loss:0.3361 val acc:66.23%\n",
            "38/200 train loss:0.1991 train acc:79.53% val loss:0.3424 val acc:65.39%\n",
            "39/200 train loss:0.2196 train acc:77.43% val loss:0.3558 val acc:64.08%\n",
            "40/200 train loss:0.2118 train acc:78.07% val loss:0.3874 val acc:61.35%\n",
            "41/200 train loss:0.2261 train acc:76.53% val loss:0.3435 val acc:65.45%\n",
            "42/200 train loss:0.2091 train acc:78.50% val loss:0.3348 val acc:66.34%\n",
            "43/200 train loss:0.1962 train acc:79.83% val loss:0.3476 val acc:64.93%\n",
            "44/200 train loss:0.2144 train acc:77.95% val loss:0.6270 val acc:37.32%\n",
            "45/200 train loss:0.3070 train acc:68.89% val loss:0.3722 val acc:62.58%\n",
            "46/200 train loss:0.2212 train acc:77.54% val loss:0.3268 val acc:67.54%\n",
            "47/200 train loss:0.1893 train acc:80.72% val loss:0.3778 val acc:62.17%\n",
            "48/200 train loss:0.1733 train acc:82.31% val loss:0.3676 val acc:63.04%\n",
            "49/200 train loss:0.1652 train acc:83.12% val loss:0.3578 val acc:64.54%\n",
            "50/200 train loss:0.1752 train acc:82.12% val loss:0.3665 val acc:63.60%\n",
            "51/200 train loss:0.1667 train acc:82.96% val loss:0.3429 val acc:65.23%\n",
            "52/200 train loss:0.1577 train acc:83.86% val loss:0.3379 val acc:66.52%\n",
            "53/200 train loss:0.1540 train acc:84.23% val loss:0.3543 val acc:64.73%\n",
            "54/200 train loss:0.1519 train acc:84.44% val loss:0.3656 val acc:64.32%\n",
            "55/200 train loss:0.1492 train acc:84.71% val loss:0.3891 val acc:61.08%\n",
            "56/200 train loss:0.1507 train acc:84.56% val loss:0.3776 val acc:62.35%\n",
            "57/200 train loss:0.1468 train acc:84.95% val loss:0.3573 val acc:64.47%\n",
            "58/200 train loss:0.1429 train acc:85.33% val loss:0.3559 val acc:64.43%\n",
            "59/200 train loss:0.1454 train acc:85.09% val loss:0.3414 val acc:66.07%\n",
            "60/200 train loss:0.1428 train acc:85.35% val loss:0.3492 val acc:65.30%\n",
            "61/200 train loss:0.1471 train acc:84.92% val loss:0.3411 val acc:65.70%\n",
            "62/200 train loss:0.1365 train acc:85.97% val loss:0.3499 val acc:65.20%\n",
            "63/200 train loss:0.1388 train acc:85.74% val loss:0.4018 val acc:59.76%\n",
            "64/200 train loss:0.1405 train acc:85.57% val loss:0.3694 val acc:63.38%\n",
            "65/200 train loss:0.1365 train acc:85.98% val loss:0.3553 val acc:64.16%\n",
            "66/200 train loss:0.1442 train acc:85.21% val loss:0.3487 val acc:65.47%\n",
            "67/200 train loss:0.1377 train acc:85.85% val loss:0.3536 val acc:64.82%\n",
            "68/200 train loss:0.1334 train acc:86.28% val loss:0.3589 val acc:63.97%\n",
            "69/200 train loss:0.1311 train acc:86.51% val loss:0.3432 val acc:65.79%\n",
            "70/200 train loss:0.1343 train acc:86.20% val loss:0.3416 val acc:65.85%\n",
            "71/200 train loss:0.1323 train acc:86.39% val loss:0.3402 val acc:65.69%\n",
            "72/200 train loss:0.1346 train acc:86.16% val loss:0.3554 val acc:64.74%\n",
            "73/200 train loss:0.1349 train acc:86.13% val loss:0.3872 val acc:61.91%\n",
            "74/200 train loss:0.1269 train acc:86.93% val loss:0.3448 val acc:65.22%\n",
            "75/200 train loss:0.1311 train acc:86.51% val loss:0.3598 val acc:63.70%\n",
            "76/200 train loss:0.1292 train acc:86.70% val loss:0.3569 val acc:64.50%\n",
            "77/200 train loss:0.1257 train acc:87.05% val loss:0.3333 val acc:66.98%\n",
            "78/200 train loss:0.1316 train acc:86.46% val loss:0.3476 val acc:65.10%\n",
            "79/200 train loss:0.1309 train acc:86.53% val loss:0.3622 val acc:63.53%\n",
            "80/200 train loss:0.1233 train acc:87.29% val loss:0.3455 val acc:65.62%\n",
            "81/200 train loss:0.1259 train acc:87.03% val loss:0.3767 val acc:62.33%\n",
            "82/200 train loss:0.1182 train acc:87.79% val loss:0.3522 val acc:65.05%\n",
            "83/200 train loss:0.1228 train acc:87.34% val loss:0.3662 val acc:63.89%\n",
            "84/200 train loss:0.1258 train acc:87.04% val loss:0.3650 val acc:63.37%\n",
            "85/200 train loss:0.1244 train acc:87.18% val loss:0.3458 val acc:65.47%\n",
            "86/200 train loss:0.1195 train acc:87.66% val loss:0.3668 val acc:63.34%\n",
            "87/200 train loss:0.1229 train acc:87.33% val loss:0.3515 val acc:64.83%\n",
            "88/200 train loss:0.1204 train acc:87.58% val loss:0.3567 val acc:64.35%\n",
            "89/200 train loss:0.1217 train acc:87.45% val loss:0.3374 val acc:66.20%\n",
            "90/200 train loss:0.1136 train acc:88.25% val loss:0.3401 val acc:65.81%\n",
            "91/200 train loss:0.1115 train acc:88.47% val loss:0.3600 val acc:63.88%\n",
            "92/200 train loss:0.1137 train acc:88.24% val loss:0.3706 val acc:62.56%\n",
            "93/200 train loss:0.1192 train acc:87.70% val loss:0.3624 val acc:64.05%\n",
            "94/200 train loss:0.1181 train acc:87.81% val loss:0.3672 val acc:63.50%\n",
            "95/200 train loss:0.1131 train acc:88.30% val loss:0.3491 val acc:65.17%\n",
            "96/200 train loss:0.1142 train acc:88.20% val loss:0.3570 val acc:64.28%\n",
            "97/200 train loss:0.1189 train acc:87.73% val loss:0.3574 val acc:64.44%\n",
            "98/200 train loss:0.1102 train acc:88.59% val loss:0.3610 val acc:63.46%\n",
            "99/200 train loss:0.1079 train acc:88.82% val loss:0.3847 val acc:61.87%\n",
            "100/200 train loss:0.1102 train acc:88.60% val loss:0.3580 val acc:64.17%\n",
            "101/200 train loss:0.1123 train acc:88.38% val loss:0.3622 val acc:63.72%\n",
            "102/200 train loss:0.1138 train acc:88.23% val loss:0.3505 val acc:65.09%\n",
            "103/200 train loss:0.1096 train acc:88.66% val loss:0.3793 val acc:62.19%\n",
            "104/200 train loss:0.1096 train acc:88.66% val loss:0.3519 val acc:64.40%\n",
            "105/200 train loss:0.1171 train acc:87.91% val loss:0.3704 val acc:63.12%\n",
            "106/200 train loss:0.1039 train acc:89.22% val loss:0.3502 val acc:64.67%\n",
            "107/200 train loss:0.1028 train acc:89.33% val loss:0.3587 val acc:64.14%\n",
            "108/200 train loss:0.1053 train acc:89.08% val loss:0.3568 val acc:64.46%\n",
            "109/200 train loss:0.1012 train acc:89.49% val loss:0.3581 val acc:64.08%\n",
            "110/200 train loss:0.1054 train acc:89.07% val loss:0.3642 val acc:63.57%\n",
            "111/200 train loss:0.1025 train acc:89.36% val loss:0.3624 val acc:64.32%\n",
            "112/200 train loss:0.1004 train acc:89.57% val loss:0.3743 val acc:63.55%\n",
            "113/200 train loss:0.1026 train acc:89.35% val loss:0.3617 val acc:64.42%\n",
            "114/200 train loss:0.1023 train acc:89.38% val loss:0.3654 val acc:63.24%\n",
            "115/200 train loss:0.1016 train acc:89.45% val loss:0.3676 val acc:63.63%\n",
            "116/200 train loss:0.1089 train acc:88.72% val loss:0.3491 val acc:65.22%\n",
            "117/200 train loss:0.0998 train acc:89.63% val loss:0.3579 val acc:64.71%\n",
            "118/200 train loss:0.0960 train acc:90.00% val loss:0.3513 val acc:64.62%\n",
            "119/200 train loss:0.1031 train acc:89.30% val loss:0.3623 val acc:63.75%\n",
            "120/200 train loss:0.0984 train acc:89.77% val loss:0.3551 val acc:64.32%\n",
            "121/200 train loss:0.1026 train acc:89.35% val loss:0.3589 val acc:64.38%\n",
            "122/200 train loss:0.0956 train acc:90.05% val loss:0.3473 val acc:65.26%\n",
            "123/200 train loss:0.1015 train acc:89.46% val loss:0.3587 val acc:64.29%\n",
            "124/200 train loss:0.0989 train acc:89.72% val loss:0.3811 val acc:62.45%\n",
            "125/200 train loss:0.0952 train acc:90.09% val loss:0.3471 val acc:65.40%\n",
            "126/200 train loss:0.0973 train acc:89.88% val loss:0.3599 val acc:63.91%\n",
            "127/200 train loss:0.1020 train acc:89.41% val loss:0.3455 val acc:65.31%\n",
            "128/200 train loss:0.0912 train acc:90.48% val loss:0.3598 val acc:64.18%\n",
            "129/200 train loss:0.0948 train acc:90.13% val loss:0.3515 val acc:65.00%\n",
            "130/200 train loss:0.0965 train acc:89.96% val loss:0.3508 val acc:64.87%\n",
            "131/200 train loss:0.0990 train acc:89.70% val loss:0.3656 val acc:63.47%\n",
            "132/200 train loss:0.1017 train acc:89.44% val loss:0.3390 val acc:65.98%\n",
            "133/200 train loss:0.0988 train acc:89.73% val loss:0.3518 val acc:65.31%\n",
            "134/200 train loss:0.0904 train acc:90.56% val loss:0.3550 val acc:64.35%\n",
            "135/200 train loss:0.0967 train acc:89.94% val loss:0.3838 val acc:61.62%\n",
            "136/200 train loss:0.0994 train acc:89.66% val loss:0.3587 val acc:64.27%\n",
            "137/200 train loss:0.0908 train acc:90.53% val loss:0.3435 val acc:65.60%\n",
            "138/200 train loss:0.0932 train acc:90.29% val loss:0.3458 val acc:65.22%\n",
            "139/200 train loss:0.0943 train acc:90.17% val loss:0.3594 val acc:64.64%\n",
            "140/200 train loss:0.0882 train acc:90.78% val loss:0.3707 val acc:63.11%\n",
            "141/200 train loss:0.0923 train acc:90.38% val loss:0.3440 val acc:65.12%\n",
            "142/200 train loss:0.0924 train acc:90.37% val loss:0.3582 val acc:64.27%\n",
            "143/200 train loss:0.0898 train acc:90.63% val loss:0.3765 val acc:62.51%\n",
            "144/200 train loss:0.0893 train acc:90.68% val loss:0.3661 val acc:63.58%\n",
            "145/200 train loss:0.0926 train acc:90.34% val loss:0.3718 val acc:62.54%\n",
            "146/200 train loss:0.0897 train acc:90.64% val loss:0.3559 val acc:63.92%\n",
            "147/200 train loss:0.0918 train acc:90.43% val loss:0.3561 val acc:64.41%\n",
            "148/200 train loss:0.0907 train acc:90.53% val loss:0.3660 val acc:63.35%\n",
            "149/200 train loss:0.0983 train acc:89.78% val loss:0.3744 val acc:62.72%\n",
            "150/200 train loss:0.0919 train acc:90.41% val loss:0.3664 val acc:62.94%\n",
            "151/200 train loss:0.0884 train acc:90.76% val loss:0.3607 val acc:63.71%\n",
            "152/200 train loss:0.0908 train acc:90.52% val loss:0.3499 val acc:64.51%\n",
            "153/200 train loss:0.0860 train acc:91.00% val loss:0.3546 val acc:64.63%\n",
            "154/200 train loss:0.0851 train acc:91.09% val loss:0.3604 val acc:64.00%\n",
            "155/200 train loss:0.0834 train acc:91.26% val loss:0.3630 val acc:63.92%\n",
            "156/200 train loss:0.0789 train acc:91.71% val loss:0.3643 val acc:63.80%\n",
            "157/200 train loss:0.0906 train acc:90.54% val loss:0.3518 val acc:64.93%\n",
            "158/200 train loss:0.0848 train acc:91.12% val loss:0.3603 val acc:63.76%\n",
            "159/200 train loss:0.0806 train acc:91.54% val loss:0.3777 val acc:62.78%\n",
            "160/200 train loss:0.0833 train acc:91.27% val loss:0.3492 val acc:64.63%\n",
            "161/200 train loss:0.0878 train acc:90.82% val loss:0.3534 val acc:64.48%\n",
            "162/200 train loss:0.0875 train acc:90.85% val loss:0.3605 val acc:64.24%\n",
            "163/200 train loss:0.0888 train acc:90.72% val loss:0.3396 val acc:65.65%\n",
            "164/200 train loss:0.0894 train acc:90.66% val loss:0.3424 val acc:65.63%\n",
            "165/200 train loss:0.0851 train acc:91.09% val loss:0.3475 val acc:65.33%\n",
            "166/200 train loss:0.0846 train acc:91.14% val loss:0.3364 val acc:66.18%\n",
            "167/200 train loss:0.0804 train acc:91.56% val loss:0.3464 val acc:65.80%\n",
            "168/200 train loss:0.0843 train acc:91.17% val loss:0.3616 val acc:63.70%\n",
            "169/200 train loss:0.0853 train acc:91.07% val loss:0.3462 val acc:65.45%\n",
            "170/200 train loss:0.0817 train acc:91.44% val loss:0.3560 val acc:64.47%\n",
            "171/200 train loss:0.0832 train acc:91.28% val loss:0.3556 val acc:64.49%\n",
            "172/200 train loss:0.0794 train acc:91.65% val loss:0.3516 val acc:64.68%\n",
            "173/200 train loss:0.0790 train acc:91.70% val loss:0.3366 val acc:66.10%\n",
            "174/200 train loss:0.0790 train acc:91.70% val loss:0.3598 val acc:64.45%\n",
            "175/200 train loss:0.0829 train acc:91.31% val loss:0.3677 val acc:63.44%\n",
            "176/200 train loss:0.0826 train acc:91.34% val loss:0.3588 val acc:63.73%\n",
            "177/200 train loss:0.0824 train acc:91.36% val loss:0.3425 val acc:65.69%\n",
            "178/200 train loss:0.0794 train acc:91.66% val loss:0.3561 val acc:64.61%\n",
            "179/200 train loss:0.0875 train acc:90.85% val loss:0.3654 val acc:63.63%\n",
            "180/200 train loss:0.0817 train acc:91.43% val loss:0.3611 val acc:64.29%\n",
            "181/200 train loss:0.0825 train acc:91.35% val loss:0.3456 val acc:65.30%\n",
            "182/200 train loss:0.0819 train acc:91.41% val loss:0.3596 val acc:64.67%\n",
            "183/200 train loss:0.0778 train acc:91.81% val loss:0.3561 val acc:64.43%\n",
            "184/200 train loss:0.0781 train acc:91.79% val loss:0.3695 val acc:63.64%\n",
            "185/200 train loss:0.0780 train acc:91.80% val loss:0.3616 val acc:63.61%\n",
            "186/200 train loss:0.0715 train acc:92.45% val loss:0.3442 val acc:65.69%\n",
            "187/200 train loss:0.0701 train acc:92.59% val loss:0.3413 val acc:65.60%\n",
            "188/200 train loss:0.0741 train acc:92.19% val loss:0.3675 val acc:63.22%\n",
            "189/200 train loss:0.0774 train acc:91.86% val loss:0.3542 val acc:64.10%\n",
            "190/200 train loss:0.0790 train acc:91.70% val loss:0.3489 val acc:64.76%\n",
            "191/200 train loss:0.0807 train acc:91.53% val loss:0.3567 val acc:64.70%\n",
            "192/200 train loss:0.0751 train acc:92.09% val loss:0.3778 val acc:62.78%\n",
            "193/200 train loss:0.0824 train acc:91.36% val loss:0.3638 val acc:63.62%\n",
            "194/200 train loss:0.0810 train acc:91.50% val loss:0.3716 val acc:62.85%\n",
            "195/200 train loss:0.0780 train acc:91.80% val loss:0.3739 val acc:62.58%\n",
            "196/200 train loss:0.0790 train acc:91.70% val loss:0.3589 val acc:64.34%\n",
            "197/200 train loss:0.0727 train acc:92.33% val loss:0.3640 val acc:64.14%\n",
            "198/200 train loss:0.0716 train acc:92.43% val loss:0.3626 val acc:63.87%\n",
            "199/200 train loss:0.0751 train acc:92.09% val loss:0.3721 val acc:62.92%\n",
            "200/200 train loss:0.0764 train acc:91.96% val loss:0.3414 val acc:65.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9tywM_Rcpk0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}