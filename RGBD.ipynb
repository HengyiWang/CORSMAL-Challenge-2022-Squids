{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RGBD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "xTCST85Jf6sA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S4nVowpzcsSE"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.io.wavfile\n",
        "import time\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/rgb.zip /content/rgb.zip\n",
        "!cp /content/drive/MyDrive/depth.zip /content/depth.zip\n",
        "!cp /content/drive/MyDrive/labels.zip /content/labels.zip"
      ],
      "metadata": {
        "id": "6NK5jmrxc4qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/rgb.zip\n",
        "!unzip /content/drive/MyDrive/depth.zip\n",
        "!unzip /content/drive/MyDrive/labels.zip"
      ],
      "metadata": {
        "id": "fxUgopCVeRMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing 2"
      ],
      "metadata": {
        "id": "lU7Y4M1egAh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import scipy.io.wavfile\n",
        "import time\n",
        "import IPython\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import Subset\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "v9ej-BmvgInY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/crops_rgb_train.zip /content/crops_rgb_train.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/crops_rgb_test.zip /content/crops_rgb_test.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/crops_depth_train.zip /content/crops_depth_train.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/crops_depth_test.zip /content/crops_depth_test.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/labels_test.zip /content/labels_test.zip\n",
        "!cp /content/drive/MyDrive/COSRMAL_CHALLENGE/RGBD/labels_train.zip /content/labels_train.zip\n"
      ],
      "metadata": {
        "id": "BTQA8MKYgN1r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/train',exist_ok=True)\n",
        "os.makedirs('/content/test',exist_ok=True)\n",
        "!unzip /content/crops_rgb_train.zip -d /content/train/\n",
        "!unzip /content/crops_rgb_test.zip -d /content/test/\n",
        "!unzip /content/crops_depth_train.zip -d /content/train/\n",
        "!unzip /content/crops_depth_test.zip -d /content/test/\n",
        "!unzip /content/labels_train.zip -d /content/train/\n",
        "!unzip /content/labels_test.zip -d /content/test"
      ],
      "metadata": {
        "id": "-GKjFsuRg8wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient"
      ],
      "metadata": {
        "id": "cD3P6O_vc458"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a EfficientNetV2 Model as defined in:\n",
        "Mingxing Tan, Quoc V. Le. (2021). \n",
        "EfficientNetV2: Smaller Models and Faster Training\n",
        "arXiv preprint arXiv:2104.00298.\n",
        "import from https://github.com/d-li14/mobilenetv2.pytorch\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "__all__ = ['effnetv2_s', 'effnetv2_m', 'effnetv2_l', 'effnetv2_xl']\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "\n",
        "# SiLU (Swish) activation function\n",
        "if hasattr(nn, 'SiLU'):\n",
        "    SiLU = nn.SiLU\n",
        "else:\n",
        "    # For compatibility with old PyTorch versions\n",
        "    class SiLU(nn.Module):\n",
        "        def forward(self, x):\n",
        "            return x * torch.sigmoid(x)\n",
        "\n",
        " \n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, inp, oup, reduction=4):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "                nn.Linear(oup, _make_divisible(inp // reduction, 8)),\n",
        "                SiLU(),\n",
        "                nn.Linear(_make_divisible(inp // reduction, 8), oup),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "def conv_3x3_bn(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        SiLU()\n",
        "    )\n",
        "\n",
        "\n",
        "class MBConv(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio, use_se):\n",
        "        super(MBConv, self).__init__()\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = round(inp * expand_ratio)\n",
        "        self.identity = stride == 1 and inp == oup\n",
        "        if use_se:\n",
        "            self.conv = nn.Sequential(\n",
        "                # pw\n",
        "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # dw\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                SELayer(inp, hidden_dim),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "        else:\n",
        "            self.conv = nn.Sequential(\n",
        "                # fused\n",
        "                nn.Conv2d(inp, hidden_dim, 3, stride, 1, bias=False),\n",
        "                nn.BatchNorm2d(hidden_dim),\n",
        "                SiLU(),\n",
        "                # pw-linear\n",
        "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(oup),\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.identity:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class EffNetV2(nn.Module):\n",
        "    def __init__(self, cfgs, num_classes=1, width_mult=1.):\n",
        "        super(EffNetV2, self).__init__()\n",
        "        self.cfgs = cfgs\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(24 * width_mult, 8)\n",
        "        layers = [conv_3x3_bn(4, input_channel, 2)]\n",
        "        # building inverted residual blocks\n",
        "        block = MBConv\n",
        "        for t, c, n, s, use_se in self.cfgs:\n",
        "            output_channel = _make_divisible(c * width_mult, 8)\n",
        "            for i in range(n):\n",
        "                layers.append(block(input_channel, output_channel, s if i == 0 else 1, t, use_se))\n",
        "                input_channel = output_channel\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        # building last several layers\n",
        "        output_channel = _make_divisible(1792 * width_mult, 8) if width_mult > 1.0 else 1792\n",
        "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(output_channel, num_classes)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "    \n",
        "    def extract(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.avgpool(x)\n",
        "        feature = x.view(x.size(0), -1)\n",
        "        x = self.classifier(feature)\n",
        "\n",
        "        return feature, x\n",
        "\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.001)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def effnetv2_s(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-S model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  24,  2, 1, 0],\n",
        "        [4,  48,  4, 2, 0],\n",
        "        [4,  64,  4, 2, 0],\n",
        "        [4, 128,  6, 2, 1],\n",
        "        [6, 160,  9, 1, 1],\n",
        "        [6, 256, 15, 2, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)\n",
        "\n",
        "\n",
        "def effnetv2_m(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-M model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  24,  3, 1, 0],\n",
        "        [4,  48,  5, 2, 0],\n",
        "        [4,  80,  5, 2, 0],\n",
        "        [4, 160,  7, 2, 1],\n",
        "        [6, 176, 14, 1, 1],\n",
        "        [6, 304, 18, 2, 1],\n",
        "        [6, 512,  5, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)\n",
        "\n",
        "\n",
        "def effnetv2_l(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-L model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  32,  4, 1, 0],\n",
        "        [4,  64,  7, 2, 0],\n",
        "        [4,  96,  7, 2, 0],\n",
        "        [4, 192, 10, 2, 1],\n",
        "        [6, 224, 19, 1, 1],\n",
        "        [6, 384, 25, 2, 1],\n",
        "        [6, 640,  7, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)\n",
        "\n",
        "\n",
        "def effnetv2_xl(**kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a EfficientNetV2-XL model\n",
        "    \"\"\"\n",
        "    cfgs = [\n",
        "        # t, c, n, s, SE\n",
        "        [1,  32,  4, 1, 0],\n",
        "        [4,  64,  8, 2, 0],\n",
        "        [4,  96,  8, 2, 0],\n",
        "        [4, 192, 16, 2, 1],\n",
        "        [6, 256, 24, 1, 1],\n",
        "        [6, 512, 32, 2, 1],\n",
        "        [6, 640,  8, 1, 1],\n",
        "    ]\n",
        "    return EffNetV2(cfgs, **kwargs)"
      ],
      "metadata": {
        "id": "r4L7Z7n9c3W-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mobile"
      ],
      "metadata": {
        "id": "3WUCTNd1o8G4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "\n",
        "\n",
        "\n",
        "class hswish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        out = x * F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class hsigmoid(nn.Module):\n",
        "    def forward(self, x):\n",
        "        out = F.relu6(x + 3, inplace=True) / 6\n",
        "        return out\n",
        "\n",
        "\n",
        "class SeModule(nn.Module):\n",
        "    def __init__(self, in_size, reduction=4):\n",
        "        super(SeModule, self).__init__()\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(in_size, in_size // reduction, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(in_size // reduction),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_size // reduction, in_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(in_size),\n",
        "            hsigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.se(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    '''expand + depthwise + pointwise'''\n",
        "    def __init__(self, kernel_size, in_size, expand_size, out_size, nolinear, semodule, stride):\n",
        "        super(Block, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.se = semodule\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_size, expand_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(expand_size)\n",
        "        self.nolinear1 = nolinear\n",
        "        self.conv2 = nn.Conv2d(expand_size, expand_size, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=expand_size, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(expand_size)\n",
        "        self.nolinear2 = nolinear\n",
        "        self.conv3 = nn.Conv2d(expand_size, out_size, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_size)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride == 1 and in_size != out_size:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_size, out_size, kernel_size=1, stride=1, padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_size),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.nolinear1(self.bn1(self.conv1(x)))\n",
        "        out = self.nolinear2(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        if self.se != None:\n",
        "            out = self.se(out)\n",
        "        out = out + self.shortcut(x) if self.stride==1 else out\n",
        "        return out\n",
        "\n",
        "\n",
        "class MobileNetV3_Large(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(MobileNetV3_Large, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.hs1 = hswish()\n",
        "\n",
        "        self.bneck = nn.Sequential(\n",
        "            Block(3, 16, 16, 16, nn.ReLU(inplace=True), None, 1),\n",
        "            Block(3, 16, 64, 24, nn.ReLU(inplace=True), None, 2),\n",
        "            Block(3, 24, 72, 24, nn.ReLU(inplace=True), None, 1),\n",
        "            Block(5, 24, 72, 40, nn.ReLU(inplace=True), SeModule(40), 2),\n",
        "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
        "            Block(5, 40, 120, 40, nn.ReLU(inplace=True), SeModule(40), 1),\n",
        "            Block(3, 40, 240, 80, hswish(), None, 2),\n",
        "            Block(3, 80, 200, 80, hswish(), None, 1),\n",
        "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
        "            Block(3, 80, 184, 80, hswish(), None, 1),\n",
        "            Block(3, 80, 480, 112, hswish(), SeModule(112), 1),\n",
        "            Block(3, 112, 672, 112, hswish(), SeModule(112), 1),\n",
        "            Block(5, 112, 672, 160, hswish(), SeModule(160), 1),\n",
        "            Block(5, 160, 672, 160, hswish(), SeModule(160), 2),\n",
        "            Block(5, 160, 960, 160, hswish(), SeModule(160), 1),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.conv2 = nn.Conv2d(160, 960, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(960)\n",
        "        self.hs2 = hswish()\n",
        "        self.linear3 = nn.Linear(960, 1280)\n",
        "        self.bn3 = nn.BatchNorm1d(1280)\n",
        "        self.hs3 = hswish()\n",
        "        self.linear4 = nn.Linear(1280, num_classes)\n",
        "        self.init_params()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.hs3(self.bn3(self.linear3(out)))\n",
        "        out = self.linear4(out)\n",
        "        return out\n",
        "    def extract(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MobileNetV3_Small(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(MobileNetV3_Small, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.hs1 = hswish()\n",
        "\n",
        "        self.bneck = nn.Sequential(\n",
        "            Block(3, 16, 16, 16, nn.ReLU(inplace=True), SeModule(16), 2),\n",
        "            Block(3, 16, 72, 24, nn.ReLU(inplace=True), None, 2),\n",
        "            Block(3, 24, 88, 24, nn.ReLU(inplace=True), None, 1),\n",
        "            Block(5, 24, 96, 40, hswish(), SeModule(40), 2),\n",
        "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
        "            Block(5, 40, 240, 40, hswish(), SeModule(40), 1),\n",
        "            Block(5, 40, 120, 48, hswish(), SeModule(48), 1),\n",
        "            Block(5, 48, 144, 48, hswish(), SeModule(48), 1),\n",
        "            Block(5, 48, 288, 96, hswish(), SeModule(96), 2),\n",
        "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
        "            Block(5, 96, 576, 96, hswish(), SeModule(96), 1),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.conv2 = nn.Conv2d(96, 576, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(576)\n",
        "        self.hs2 = hswish()\n",
        "        self.linear3 = nn.Linear(576, 1280)\n",
        "        self.bn3 = nn.BatchNorm1d(1280)\n",
        "        self.hs3 = hswish()\n",
        "        self.linear4 = nn.Linear(1280, num_classes)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                init.constant_(m.weight, 1)\n",
        "                init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.normal_(m.weight, std=0.001)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.hs3(self.bn3(self.linear3(out)))\n",
        "        out = self.linear4(out)\n",
        "        return out\n",
        "        \n",
        "    def extract(self, x):\n",
        "        out = self.hs1(self.bn1(self.conv1(x)))\n",
        "        out = self.bneck(out)\n",
        "        out = self.hs2(self.bn2(self.conv2(out)))\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = MobileNetV3_Small()\n",
        "    x = torch.randn(2,3,224,224)\n",
        "    y = net(x)\n",
        "    print(y.size())"
      ],
      "metadata": {
        "id": "K7fa7LEWo6Nq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "6XvAhx-tdXJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DjQlCSq1sTd",
        "outputId": "6df56fab-34fc-4c22-e60c-9431371e42b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_annotation(id,input,anno_path='/content/labels'):\n",
        "    anno = np.load(os.path.join(anno_path,'{:06d}.npy'.format(id)),allow_pickle=True).item()\n",
        "    return anno.get(input)"
      ],
      "metadata": {
        "id": "K3Na-c5wgyl4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "\n",
        "class MiniDataset(Dataset):\n",
        "    def __init__(self, base_p, label_f, depth, crops_rgb_f, label_name=['container capacity']):\n",
        "      self.label_f = label_f #_f = folder\n",
        "      self.depth = depth\n",
        "      self.base = base_p\n",
        "      self.label_name = label_name\n",
        "      self.crops_rgb_f = crops_rgb_f\n",
        "      self.samples = os.listdir(crops_rgb_f)\n",
        "      self.ids = [ int(x.split('.')[0]) for x in self.samples]\n",
        "      self.transform = transforms.Compose([\n",
        "                                             transforms.Resize((320, 320)),\n",
        "                                             transforms.ToTensor(),\n",
        "                                             transforms.ConvertImageDtype(torch.float),\n",
        "                                             ])\n",
        "    def __len__(self):\n",
        "      return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      id_ = self.ids[idx]\n",
        "        \n",
        "      # depth\n",
        "      depth = np.asarray(Image.open(os.path.join(self.depth,'{:06d}.png'.format(id_))))[:,:,np.newaxis]\n",
        "      \n",
        "      # rgb_cropped\n",
        "      crop = np.asarray(Image.open(os.path.join(self.crops_rgb_f,'{:06d}.png'.format(id_))))\n",
        "\n",
        "      h, w, c = crop.shape\n",
        "\n",
        "      resX = 640 - h\n",
        "      resY = 640 - w\n",
        "\n",
        "      up = resX // 2\n",
        "      down = up\n",
        "      if resX % 2 != 0:\n",
        "        down +=1\n",
        "\n",
        "      left = resY // 2\n",
        "      right = left\n",
        "\n",
        "      if resY % 2 != 0:\n",
        "        left += 1\n",
        "\n",
        "      padding = transforms.Pad((left, up, right, down))\n",
        "\n",
        "    \n",
        "      image = Image.fromarray(np.concatenate((crop, depth), axis=2))\n",
        "      image = padding(image)\n",
        "      image = self.transform(image)\n",
        "      # label\n",
        "      label = np.array([get_annotation(id_,name,os.path.join(self.base, 'labels')) for name in self.label_name])\n",
        "\n",
        "      return image, label"
      ],
      "metadata": {
        "id": "Wdvu3wGxdYWq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def computeScoreType1(gt, _est):\n",
        "  gt = gt.cpu().data.numpy()\n",
        "  _est = _est.cpu().data.numpy()\n",
        "  est = copy.deepcopy(_est)\n",
        "  assert (len(gt) == len(est))\n",
        "  if all(x == -1 for x in est):\n",
        "    return 0\n",
        "  indicator_f = est > -1\n",
        "  ec = np.exp(-(np.abs(gt - est) / gt)) * indicator_f\n",
        "  score = np.sum(ec) / len(gt)\n",
        "  return score"
      ],
      "metadata": {
        "id": "ajNw56bHrLQy"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "U2kxapxBc7_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def myLoss(est, gt):\n",
        "  ec = 1 - torch.exp(-(torch.abs(gt - est) / gt))\n",
        "  score = torch.sum(ec) / len(gt)\n",
        "\n",
        "  return score\n",
        "  "
      ],
      "metadata": {
        "id": "vrJWrpPM9cot"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "my_save_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE'\n",
        "bs = 2\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 50\n",
        "train_set = MiniDataset('/content/train/',\n",
        "                        '/content/train/labels', \n",
        "                        '/content/train/crops_depth',\n",
        "                        '/content/train/crops_rgb')\n",
        "val_set = MiniDataset('/content/test/',\n",
        "                        '/content/test/labels', \n",
        "                        '/content/test/crops_depth',\n",
        "                        '/content/test/crops_rgb')\n",
        "model = effnetv2_xl(num_classes=1).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = len(train_set)\n",
        "num_val = len(val_set)\n",
        "\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True,\n",
        "                            num_workers=1)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer, myLoss)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, myLoss)\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "  \n",
        "\n",
        "  # if loss_val < best_loss:\n",
        "  #   best_loss = loss_val\n",
        "  #   torch.save(model, os.path.join(base_path, 'audios', \"bl-efficient-xl.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(my_save_path, \n",
        "                                              'audios', \n",
        "                                              'RGBD',\n",
        "                                              \"XL-my{:.2f}.pth\".format(100 * correct_val/num_val)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "u7ge09lVhZNY",
        "outputId": "1f480ede-4ce8-4286-d159-c224e5cfb6d3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "0.7357405696268342\n",
            "0.7358148734526937\n",
            "0.7359936721591558\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-be13ddabecbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;31m#start_time = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m#elapsed_time = time.time() - start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-b5f502d9a935>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-747f4faa3112>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-747f4faa3112>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2283\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2284\u001b[0m     )\n\u001b[1;32m   2285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion = nn.L1Loss()):\n",
        "  model.train()\n",
        "  loss_train = 0.0\n",
        "  correct_train = 0.0\n",
        "  num_train = len(train_loader)\n",
        "  for batch_idx, (audio, target) in enumerate(train_loader):\n",
        "    audio = audio.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(audio)\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    #nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_train += loss.item() / (num_train * audio.shape[0] )\n",
        "\n",
        "    correct_train += computeScoreType1(target, outputs) * audio.shape[0]\n",
        "\n",
        "    \n",
        "    \n",
        "  \n",
        "  return loss_train, correct_train\n",
        "\n",
        "\n",
        "def evaluate(model, testloader, criterion = nn.L1Loss()):\n",
        "  model.eval()\n",
        "  loss_test = 0\n",
        "  correct_test=0\n",
        "  num_val = len(testloader)\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (audio, target) in enumerate(testloader):\n",
        "      audio = audio.to(device)\n",
        "      target = target.to(device)\n",
        "      outputs = model.forward(audio)\n",
        "      loss = criterion(outputs, target)\n",
        "      loss_test += loss.item() / (num_val * audio.shape[0] )\n",
        "      correct_test += computeScoreType1(target, outputs) * audio.shape[0]\n",
        "\n",
        "\n",
        "  \n",
        "  return loss_test, correct_test\n"
      ],
      "metadata": {
        "id": "RfA3yrJ8jvmQ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "my_save_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE'\n",
        "bs = 30\n",
        "train_split = 0.8\n",
        "lr = 1e-3\n",
        "epochs = 100\n",
        "train_set = MiniDataset('/content/train/',\n",
        "                        '/content/train/labels', \n",
        "                        '/content/train/crops_depth',\n",
        "                        '/content/train/crops_rgb')\n",
        "val_set = MiniDataset('/content/test/',\n",
        "                        '/content/test/labels', \n",
        "                        '/content/test/crops_depth',\n",
        "                        '/content/test/crops_rgb')\n",
        "model = MobileNetV3_Large(num_classes=1).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = len(train_set)\n",
        "num_val = len(val_set)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True,\n",
        "                            num_workers=1,\n",
        "                            drop_last=True)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer, myLoss)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, myLoss)\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "  \n",
        "\n",
        "  # if loss_val < best_loss:\n",
        "  #   best_loss = loss_val\n",
        "  #   torch.save(model, os.path.join(base_path, 'audios', \"bl-efficient-xl.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(my_save_path, \n",
        "                                              'audios', \n",
        "                                              'RGBD',\n",
        "                                              \"mobile{:.2f}.pth\".format(100 * correct_val/num_val)))\n",
        "\n"
      ],
      "metadata": {
        "id": "FB5uZuFlmZgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ee1412b-17e6-4d73-8112-460ea22ebe74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "1/100 train loss:0.0199 train acc:24.11% val loss:0.0521 val acc:21.90%\n",
            "2/100 train loss:0.0156 train acc:39.75% val loss:0.0206 val acc:44.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9tywM_Rcpk0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimension"
      ],
      "metadata": {
        "id": "Gqxb5W1dvuzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion = nn.L1Loss()):\n",
        "  model.train()\n",
        "  loss_train = 0.0\n",
        "  correct_train = 0.0\n",
        "  num_train = len(train_loader)\n",
        "  for batch_idx, (audio, target) in enumerate(train_loader):\n",
        "    audio = audio.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model.forward(audio)\n",
        "\n",
        "    loss = criterion(outputs, target)\n",
        "    loss.backward()\n",
        "    #nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_train += loss.item() / num_train\n",
        "\n",
        "    for i in range(outputs.shape[1]):\n",
        "      correct_train += computeScoreType1(target[:, i], outputs[:, i]) * \\\n",
        "      audio.shape[0] / audio.shape[1]\n",
        "    \n",
        "  \n",
        "  return loss_train, correct_train\n",
        "\n",
        "\n",
        "def evaluate(model, testloader, criterion = nn.L1Loss()):\n",
        "  model.eval()\n",
        "  loss_test = 0\n",
        "  correct_test=0\n",
        "  num_val = len(testloader)\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (audio, target) in enumerate(testloader):\n",
        "      audio = audio.to(device)\n",
        "      target = target.to(device)\n",
        "      outputs = model.forward(audio)\n",
        "      loss = criterion(outputs, target)\n",
        "      loss_test += loss.item() / num_val   \n",
        "      for i in range(outputs.shape[1]):\n",
        "        correct_test += computeScoreType1(target[:, i], outputs[:, i]) * \\\n",
        "        audio.shape[0] /  audio.shape[1]\n",
        "\n",
        "  \n",
        "  return loss_test ,correct_test\n"
      ],
      "metadata": {
        "id": "hTsQDoHKzDUE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([os.path.join('/content/train/', 'labels') for name in ['width at the top', 'width at the bottom', 'height']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVkOiuLyyPPL",
        "outputId": "5366c598-f867-4f82-a56b-335256ade72d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/train/labels', '/content/train/labels', '/content/train/labels']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(est, gt):\n",
        "  score = 0\n",
        "  for i in range(est.shape[1]):\n",
        "    ec = 1 - torch.exp(-(torch.abs(gt[:, i] - est[:, i]) / gt[:, i]))\n",
        "    score += torch.sum(ec) / len(gt[:, i])\n",
        "  \n",
        "  score /= est.shape[1]\n",
        "  \n",
        "  return score"
      ],
      "metadata": {
        "id": "oJE3Px_B0xqa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "my_save_path = '/content/drive/MyDrive/COSRMAL_CHALLENGE'\n",
        "bs = 2\n",
        "train_split = 0.8\n",
        "lr = 1e-4\n",
        "epochs = 200\n",
        "train_set = MiniDataset('/content/train/',\n",
        "                        '/content/train/labels', \n",
        "                        '/content/train/crops_depth',\n",
        "                        '/content/train/crops_rgb',\n",
        "                        ['width at the top', 'width at the bottom', 'height'])\n",
        "val_set = MiniDataset('/content/test', \n",
        "                      '/content/test/labels',\n",
        "                      '/content/test/crops_depth',\n",
        "                      '/content/test/crops_rgb',\n",
        "                      ['width at the top', 'width at the bottom', 'height'])\n",
        "model = effnetv2_xl().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,  weight_decay=1e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "best_acc = 0\n",
        "\n",
        "num_train = len(train_set)\n",
        "num_val = len(val_set)\n",
        "\n",
        "\n",
        "\n",
        "train_loader   = DataLoader(train_set,\n",
        "                            batch_size=bs,\n",
        "                            shuffle=True,\n",
        "                            num_workers=1)\n",
        "val_loader   = DataLoader(val_set,\n",
        "                          batch_size=bs,\n",
        "                          shuffle=True,\n",
        "                          num_workers=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  #start_time = time.time()\n",
        "  loss_train, correct_train = train(model, train_loader, optimizer, loss_func)\n",
        "  loss_val, correct_val = evaluate(model, val_loader, loss_func)\n",
        "  #elapsed_time = time.time() - start_time\n",
        "  print(\"{}/{} train loss:{:.4f} train acc:{:.2f}% val loss:{:.4f} val acc:{:.2f}%\".format(\n",
        "      epoch+1,epochs, loss_train, 100 * correct_train/num_train,\n",
        "      loss_val, 100 * correct_val/num_val))\n",
        "  \n",
        "\n",
        "  # if loss_val < best_loss:\n",
        "  #   best_loss = loss_val\n",
        "  #   torch.save(model, os.path.join(base_path, 'audios', \"bl-efficient-xl.pth\"))\n",
        "  \n",
        "  if correct_val > best_acc:\n",
        "    best_acc = correct_val\n",
        "    torch.save(model.state_dict(), os.path.join(my_save_path, \n",
        "                                              'audios', \n",
        "                                              'RGBD',\n",
        "                                              \"Dim{:.2f}.pth\".format(100 * correct_val/num_val)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "dErrVjUdvwPr",
        "outputId": "21dc254f-c20f-4b64-bac6-6bfa29359183"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "1/200 train loss:0.4439 train acc:35.71% val loss:0.2819 val acc:53.85%\n",
            "2/200 train loss:0.1748 train acc:61.89% val loss:0.2176 val acc:58.65%\n",
            "3/200 train loss:0.1584 train acc:63.12% val loss:0.2012 val acc:59.87%\n",
            "4/200 train loss:0.1566 train acc:63.25% val loss:0.2433 val acc:56.87%\n",
            "5/200 train loss:0.1513 train acc:63.65% val loss:0.1739 val acc:61.97%\n",
            "6/200 train loss:0.1450 train acc:64.12% val loss:0.1613 val acc:62.85%\n",
            "7/200 train loss:0.1377 train acc:64.68% val loss:0.1695 val acc:62.27%\n",
            "8/200 train loss:0.1334 train acc:64.99% val loss:0.1617 val acc:62.89%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-675cab9b593f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;31m#start_time = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;31m#elapsed_time = time.time() - start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-b2a28be08fcb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-62ff79f7ab8d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-62ff79f7ab8d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "RXoG_PAxwUDt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}